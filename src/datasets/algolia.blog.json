[
  {
    "id": "9109-0",
    "title": "Onward! Announcing Algolia’s $110 Million in New Funding",
    "author": "Nicolas Dessaigne",
    "text": "Today, we announced our latest round of funding of $110 million with our lead investor Accel and new investor Salesforce Ventures along with many others. \nIn addition to being incredibly grateful at reaching this milestone and proud of what our team has accomplished in such a short time, I’m energized by what lies ahead. \nOn any journey, it’s always helpful to take a step back (and a breath) to appreciate how far you’ve come and appreciate how far you still have left to go to achieve your vision.\n\nIt all started with a vision and mission.\nWhen Julien and I started Algolia in 2012, it was clear that the world would only be more digitally immersed as more websites, apps and digital content were being produced at an increasing rate. Helping users search, discover and navigate through this digital world would be critical for the success of businesses of all sizes.\nOur vision at Algolia from day 1 has been for every business to be able to deliver the best experience possible to its users — a search and discovery experience that is lightning fast, personalized, and highly relevant — one that helps people find what they want across this digital universe AND how they want via any type of device at home, at work, and on-the-go. \nIn pursuing this vision, our mission has been to enable every developer and product team to build consumer-grade search that is easy to implement and easy to tailor and tune to deliver engaging digital experiences to their users that maximize growth.\n\nIt has been an amazing journey already. \nIn just seven years, we’ve already come so far with an amazing team, a strong culture and unshaken conviction in what we are doing to deliver on our vision and mission.\nSince 2012, Algolia’s platform has processed over 1 trillion search queries, and we are privileged to now have over 8,000 customers and are supporting 70 billion searches per month. \nOur employee base now consists of 350 Algolians across 6 offices worldwide — and we are… hiring!\nAnd, just in the past several months, I’m humbled by some of the milestones, partnerships and accolades we’ve been able to achieve together including: \n\nAugust 2019: Algolia was honored to be named #448 on the INC 5000 of the nation’s fastest-growing private companies \nJuly 2019: Algolia’s Summer 2019 product launch made major technology advancements in personalization, mobile and voice search (see the webinar replay here)\nJune 2019: Algolia on Microsoft Azure was made available to support large-scale, global, fast search for global enterprises \nMay 2019: Algolia expanded into Japan and APAC\nApril 2019: Algolia was named as one of the top 50 highest-rated private cloud computing companies to work for \nMarch 2019: A study by Forrester Research showed that Algolia customers achieve $73 million in incremental revenue along with a low TCO and 3 - 10X ROI through Algolia-powered search and discovery.\n\n\nBig things ahead. This is just the beginning.\nWith the new funding, Algolia will be able to propel towards our shared vision and accelerate innovation efforts on behalf of millions of companies that must ensure users are quickly getting what they want through a delightful experience that boosts engagement, conversion and growth. As we look forward for the rest of this year and beyond, we will:\n\nContinue to break down the barriers that get in the way of building great search, so that any company can deliver the best search and discovery experience to its users — while making it incredibly easy for IT teams, developers and business owners to implement, manage and optimize these Algolia-powered experiences. \nDouble down on applying machine learning to help businesses deliver even more relevant, personalized experiences to their users. This includes analyzing user queries to infer, recommend and create an even better user experience. We've just scratched the surface there, and, by processing over 70 billion searches per month through our… search-as-a-service platform and suite of APIs, no solution provider and partner is in a better position to deliver on this than Algolia.\nEnrich our conversational and voice search capabilities across web, mobile and voice-first channels to meet increasing consumer demand to search any way, any where, any time. \nExpand geographically across the Americas, Europe, Asia, Japan and new, emerging markets. This includes growing our global infrastructure of 16 data centers across 70 regions so that we can provide relevant, reliable, lightning fast performance anywhere in the world.\nInvest in our culture and employees through training and development programs, as well as through ongoing hiring of skilled professionals to expand our reach and industry expertise.\n\n“For too long, organizations have had to choose between building resource-intensive search tools by themselves and using lower cost, but inflexible, set-it-and-forget-it search solutions. Algolia solves for both by removing the cost and complexity of building search while empowering businesses to create tailored experiences that engage and delight customers. Our rapid growth underscores both the market’s need to deliver amazing consumer experiences, and Algolia’s ability to help them achieve these experiences.”\n\nMany thanks.\nBuilding a company that is “built to last” and transforms how businesses engage with their users is no small task and requires the right investors, customers, and team. \n\nThank you to all of our participating investors including Accel, Salesforce Ventures, Alven, DAG Ventures, Founders Circle, Owl Rock Capital, SaaStr Fund and World Innovation Lab, and welcome Accel’s Nate Niparko to Algolia’s board of directors.\nThank you to our 8,000+ customers whose trust in Algolia is appreciated and product feedback is invaluable.\nTo our 350 employees, thank you for your grit, candor, humility, trust and care for one another, as well as your passion and efforts to make Algolia what it has… become today and what it will become in the future.\n\nWhile the journey has already been exhilarating every step of the way, I’m energized by what lies ahead and can’t wait to take big leaps forward with all of you.\nOnward!\n_____________________________________________________________________\nFor more information about Algolia’s funding announcement:\n\nSalesforce Ventures: Improving the Search Box: Our Investment in Algolia\nWiL (World Innovation Lab): Algolia: Setting the Bar High for Digital Search & Discovery\nAlven: Algolia raises $110m to support its tremendous growth\nLes Echos Executives: Algolia, Le Roi Du « Search » Pour Entreprise, Lève 110 Millions D'euros"
  },
  {
    "id": "9472-0",
    "title": "What is Federated Search?",
    "author": "Louise Vollaire",
    "text": "If you run a site or business, you know how quickly new databases, storage locations, and product catalogs can build up. With this constant accumulation, how can you ensure your customers always find what they’re looking for on your site? How can you also ensure this solution is simple for your business?\nFederated search is one way to address both of these needs. By allowing users to search many data sources, and therefore multiple types of content, at the same time, federated search improves user experience and engagement. At the same time, it can make it easier for your business to manage data and search tools.\n \nWhat is federated search?\nFederated search is a technique used to search multiple data sources at once. With federated search, you can retrieve information from many different content locations with just one query and one search interface.\nYou may have encountered federated search without realizing it. When you type a search query into MacOS Spotlight or Windows Search, the search engine returns results of all kinds such as apps, webpages, contacts, and documents which are drawn from different sources.  The search engine uses different layouts to best present each type of content returned.. This is federated search in action.\nFederated search can be used in many contexts. For example, if your company maintains separate documentation databases for different products in addition to your product catalog, a federated search tool would allow your customers to search from a single location and obtain results from all of the documentation and the product catalog simultaneously. This type of enhancement goes a long way to improving the customer experience on your site.\n \nThe Importance of Federated Search\nAlthough users may not access your content through search alone, implementing federated search on your website or application has several key benefits:\n \nImproves Customer Experience \nFederated search is an efficient option for mid-to-low funnel users who… know exactly what they need. They can search through a large body of data from one location with one query, thus reaching their goal with fewer clicks. Even users who are still new to the topic benefit from federated search—by searching for one keyword or phrase, they can get a wide range of content from your product pages, documentation, multimedia assets, and more.\nThe fewer clicks or pageviews required to find a product or service, the more likely you are to convert the user. When users find key information faster as a result of federated search, you see improved click-through and conversion rates. \n \nMakes Website Expansion Manageable\nWhenever you add new content or data locations, you can easily integrate them into your existing federated search tool, instead of having to set up an independent search tool along with each new type of content. \n \nSupports Browsability\nWith a centralized, federated search solution, you can modify, add, or restructure data easily while still keeping that data searchable.\nYou can adapt each part of your federated search user interface to perfectly showcase the content it returns, to help users interpret and navigate different categories of information more easily. This improves the browsing experience on your site, promotes discoverability, and increases user engagement. \n\n \nImproves Reliability and Security\nFederated search requires you to manage only one search engine, which simplifies reliability and security. Monitoring, maintaining and securing a single search engine is easier to manage and troubleshoot than doing so for different search tools for each data set.\n \nIncreases Relevance of Search Results\nFederated search allows you to optimize relevance for each type of content you surface to your users. You can take into account different parameters to rank your different types of content, rather than a one-size-fits-all ranking approach.\nWith a larger volume of information to draw from, search results are often more… accurate and more relevant. And when users interact with these improved results, they generate valuable user intent data. Once you understand what users are searching for (and how), you can guide users to more helpful content and improve the searcher’s experience. \n \nApproaches to federated search\nAll federated search solutions rely on two fundamental components:\n\nIndex: a compilation of the data that you want to search, structured in a way that facilitates efficient searches. \nSearch function: the part that parses the index to find relevant information within it in response to a given query.\n\nThe index and the search components can interact in different ways to achieve a federated search. The main approaches include search-time merging, index-time merging and using a federated search interface. \n \nSearch-time merging\nWith this approach, your federated search solution runs separate searches on each data location that you want to include in search results, using multiple indices. Then, it aggregates the results from each of these searches into a final list, which is presented to the user. \n\n \nMerging the results of the various data sources at search time is typically the simplest type of federated search to implement because it does not require you to aggregate all data into a single index. However, it does require you to run and maintain specific search tools for each data source that you want to include in your searches or to use a single tool that can handle all types of content, but ingest each data source in a different index. Search results may also be slower to arrive because the central search engine has to wait until all of the local search engines have responded before it can deliver the final results. Finally, fine-tuning the relevance for the aggregated results list can be very challenging, as the search engine may have trouble ranking highly different forms of data.\n \nIndex-time merging\nIndex-time merging involves building a central index of… all of the data that you want to include in search results, then searching that index to perform a federated search.\n\n \nThis approach requires only one search engine and one index, and it is compatible with data sources that don’t have local search tools available to support them. It also typically generates results more quickly because there is no need to wait on local search tools to respond to a query.\nOn the other hand, index-time merging is more complicated to set up and maintain. You must find a way to aggregate data from multiple locations into a single index, which is particularly difficult if not all data sources exist in the same format (for example, some might be PDF files, while others are HTML pages). Once data is fed into a single index, index-time merging still requires you to decide on a unique relevance strategy for all your different types of content, which is a very complex, if not impossible problem to solve.\n \nThe Federated Search Interface\nThis approach to federated search is an extension of the search-time merging method. However, instead of aggregating the results in one combined result list, it presents one result list for each type of content the search is performed on in a unified interface.\n\n \nThis method requires a robust search solution equipped with the ability to index different types of content in different indices and create the unified federated search interface. It also requires site owners to give forethought to the final experience they want for users so content can be indexed and delivered in the most friendly way.\nA federated search interface allows you to fine-tune the relevance for each type of content independently, thus providing a superior experience to your users.\n \nFederated Search Examples\nImplementing federated search well delivers real results for businesses across a range of industries by improving user experience, increasing user engagement and boosting conversion rates. \n \nRetail\nA successful e-commerce… site relies on advanced site search design to direct visitors to what they need. A typical site might list thousands of products, each sorted into different categories, accompanied by their guides and reviews. If a visitor doesn’t know which category a certain product falls within, they may become overwhelmed and leave the site. \nTake the simple case of a customer searching for a bath mat. Where should they start? Should the customer look in the bath section, the linen section, or look for education material to buy the best one for them? The categorization could easily differ from site to site.\nWith federated search, finding the right place to perform the search doesn’t matter. A simple query for “bath mat” in the search bar will search all product categories, leading the customer quickly to the item and its related content thus maximizing the likelihood of a successful sale.\n \nEnterprise\nLarge corporations often have numerous websites serving various purposes and stakeholders. Separate sites may be maintained for investors relations, hiring, brand awareness, and corporate social responsibility, to name a few.\nWithout federated search, visitors may land on the main corporate website and have trouble targeted resource or information on the relevant site. It can represent a large array of lost opportunities for the company.\nWith federated search, however, the visitor can simply enter a keyword and search through all contents at once. This approach is likely to lead the visitor to what they need faster, providing a more positive experience and increasing the chances that the visitor will keep interacting with the company.\n \nSoftware vendors\nWhen customers are purchasing software, they already have many crucial factors to consider. Customers expect elegant, fast solutions, and a clunky search experience can turn them away. That’s why integrating a seamless federated search experience into websites is critical for software vendors.\nConsider this federated… search feature. Not only does the tool allow users to search the entire website from products to blogs to documentation with a single query, but search results also appear in real time, as users type. Results are categorized into different groups in order to make them easy to interpret. Pictures and short descriptions to illustrate the different types of content are added, when applicable, to make the content discovery and navigation more accessible to users. All of these features help enable a positive end-user experience.\n\n \nGetting Started with Federated Search\nFrom a technical standpoint, implementing federated search can be complicated if you attempt to build a search engine yourself—especially if you want to maximize performance by using an index-time merging architecture. \nWith Algolia, integrating federated search into your website or mobile app is fast and seamless. Algolia delivers lightning fast search results, no matter how many data sources you use, giving your customers what they want faster than ever. \nWatch our demo to see how Algolia’s federated search enabled solution is helping businesses around the world satisfy users."
  },
  {
    "id": "8972-0",
    "title": "Tech Startup Dilemmas: Resilient Deployment vs. Exhaustive Tests",
    "author": "Xavier Grand",
    "text": "As a startup building software, we must innovate to stay attractive and competitive. The market is continually changing, new needs emerge, so it’s crucial we keep in touch with it.\nKeeping up with such a pace means, on the software side, that we must shorten the specifications and development phase as much as possible. Yet, it’s also important to have resilient software. At Algolia, one of our primary goals is to provide excellent service. We have high profile customers for whom search has a huge business impact, especially on events like Black Friday: they can’t have downtime.\nTherefore, we need to find the right balance between resilience and innovation. Both can be contradictory: to be resilient, we must test everything, which consumes time that we don’t spend innovating. A good trade-off is to test in production.\nWhy test in production?\nTesting in production is the process of deploying new code in production, and “testing” it using the production traffic directly instead of running an exhaustive test suite. This is a risky process. You usually want to avoid it, but as your application grows, it becomes impossible to test everything.\nLet’s look at the Algolia engine. We currently have more than twenty query parameters. If they were all boolean flags, testing all cases would require a million tests: twenty parameters, with two possible values per each, that’s 2^20 possibilities.\nWhen it comes to the time that testing takes, there are three things to take into account:\n\nTime to write the tests\nTime to maintain the tests \nTime to run the tests\n\nWriting a million tests is already a long process, but once they’re there, these tests become part of the project. As such, they require maintenance like the rest of the source code, so each software iteration now requires more work.\nNow let’s imagine that your team is large enough to write and maintain those tests, you still need to run them. If each test only takes ten milliseconds, it adds up to 2h45m… to run. Any update in the code now takes 2h45m to validate.\nOur customers buy our product for the existing set of features, but also the upcoming services. They expect us to release new features regularly so we can help them grow, to be innovative and resilient. Therefore, we need to be efficient.\nWhen we implement a new feature, we only write tests to validate the implementation and to detect obvious corner cases. To thoroughly verify the functionality, we decided to put a deployment setup in place that allows us to test in production with minimal impact for our customers in case of a bug. This way, we can deliver new features on time.\nReal-life example\nLet’s take a concrete example: we recently rewrote the highlighting feature of Algolia. If we wanted to test it exhaustively, this would mean combining the one million tests needed for all parameters with all the possible Unicode characters (over a million). This adds up to over a billion tests. If each test only took a millisecond to run, this test suite itself would take 11 days to complete.\nWe had to find a better solution. Therefore, instead of maintaining billions of tests and significantly slowing down our release process, we decided to test in production.\nOur main concern was to avoid impacting customers, so we defined what we needed to put such a setup in place:\n\nA progressive deployment process\nA way to retry on a healthy infrastructure\nProactive issue detection\n\nAnyone who subscribes to Algolia from our website gets access to a three-node cluster. Each node owns 100% of the data and can serve queries independently, which provides a resilient setup. When you consider an average availability of 95% with bare metal servers, this setup allows us to provide 99.987% availability. For your Algolia service to be completely down, each server would need to be down. The probability is, therefore, 5% of 5% of 5%, or 0.0125% of potential downtime.\nNow, even with this setup, a software bug could cause a service… outage. For that reason, we have put in place a progressive deployment process of the search engine which spans across three days. This partial deploy gives us enough time to detect issues.\nIf there’s a bug in the new version, our API clients take over with their retry strategy. If they target a broken node, they transparently retry with another one until they get a successful response. From the end user’s perspective, the issue is invisible.\nWhen we deployed the rewrite of the highlighting feature, we detected a normalization issue. The goal was for it to convert text in a canonical form to simplify comparison between different inputs. Normalized forms are usually shorter or as long as the original form, and that’s what the highlighting code was expecting. However, there are some characters for which the normalized form is longer: the ß (German sharp S) is normalized into ss. During the rewrite, we added runtime pre-conditions checks to ensure the normalized size was shorter or equal, and the code behaved as expected. This is what revealed the bug.\nWhen we deployed the new version on the first node, it immediately stopped responding to queries that were generating a longer normalized form. However, thanks to our API client’s retry strategy, none of our customers noticed it. On our end, our monitoring system triggered an alert, and we quickly reverted the change to stabilize the engine. It left us all the necessary time to properly understand the issue, write a proper fix, and the corresponding tests.\nA suitable setup\nThere are three crucial elements that you need when you want to test in production:\n\nA replicated infrastructure\nA resilient software\nA safe deployment strategy\n\nReplicated infrastructure\nNowadays, configuring a replicated infrastructure is straightforward: all cloud providers offer load balancers in front of multiple VMs. In our case, we handle the replication of the search engine data at the cluster level, with each node owning 100% of the… data. This way, nodes can reply to queries independently.\nResilient software\nThis part highly depends on the software you’re building. In the Algolia engine, we have plenty of health checks within the code to verify the pre-conditions of a function and expected state. When we come to an unexpected state, the engine stops processing to avoid returning corrupted data. It forces the API clients to retry on a different node transparently.\nSafe deployment strategy\nLast and not least, the deployment process’s main goal here is to release new versions while controlling risks progressively.\nAt Algolia, we’ve split our infrastructure into four environments: testing, staging, production, and safe. Each environment has a different SLA:\n\nTesting contains clusters that we use internally. Breaking it only impacts the company.\nStaging contains search clusters of public-facing projects that are maintained by Algolians.\nProduction contains the clusters of our customers.\nSafe contains the clusters of our premium SLA customers.\n\nIn our deployment strategy, we leverage those different environments to mitigate risk. This means we first deploy on environments with the least impact for our customers.\nIn addition to the different environments, we also leveraged the replication factor of three, and created a deployment process in twelve steps:\n\nWe deploy on all three nodes of the testing clusters.\nWe deploy on the first node of the staging clusters, then on the first node of the production clusters, then on the first node of safe clusters.\nWe wait one day.\nWe deploy on the second node of the staging clusters, then on the second node of the production clusters, then on the second node of safe clusters.\nWe wait one day.\nWe deploy on the third node of the staging clusters, then on the third node of the production clusters, then on the third node of safe clusters.\n\nThe first step allows us to detect issues in the code that handles the distributed system and the interactions between nodes… within a cluster.\nThen, deploying on each node one by one lets us assess whether a cluster can support two different versions at the same time and if the code is stable.\nThe rationale behind waiting for a whole day before deploying on the next environment is that it leaves enough time to detect performance issues, data corruption, or time-based issues. Once we reach this step, we’ve detected most of the problems. The rest of the progressive deployment is here to help us identify uncaught breaking changes.\nWhenever we detect a bug, we immediately revert the new version. This way, we can set the service back into a stable state, and this gives us enough time to fix the issue and add the corresponding tests correctly.\nWith this approach, our test suite is driven by customer usage. This makes it particularly efficient and allows us to release a new version of the engine every week to sustain the needs of our customers, even with such a large codebase.\nGood is better than perfect\nThe startup ecosystem is challenging. Small teams need to find efficient strategies to create better products than big companies with large teams.\nRegularly releasing new features is as important as having a stable product with a great user experience that answers the demand of customers. This need for efficiency is the reason why we must find a middle ground between writing enough tests to have good feature coverage, and being able to release often.\nYou should always be careful with adding more tests because they are time-consuming: it takes time to write them, maintain them, and run them. So how do you know when to write tests? The ninety-ninety rule applies: it’s usually straightforward to test 90% of a feature, and the last 10% takes as much time. That’s why it’s crucial to handle the remaining 10% based on customer usage, instead of pushing for exhaustive coverage.\nTo mitigate the risk, make sure you take the time to design a piece of software and an infrastructure that supports… tests in production, with the least possible impact on customers."
  },
  {
    "id": "9104-0",
    "title": "Site Search and SEO: How Functionality Creates Value",
    "author": "Louise Vollaire",
    "text": "Today’s customers have higher expectations than ever before. People expect frictionless, dynamic experiences from nearly every site they visit and every app they use. And if they can’t get the right experience on your site, they’ll simply try a competitor’s site instead. \nWhen your site is easy to search, customers are more likely to stay on the site, consume your content, make a purchase, and even become repeat buyers. Plus, the more people searching on your site, the more data you can collect on their behavior and needs to inform business, marketing, and SEO strategy. \n \nWhat is On-Site Search? \nOn-site search (also known as site search or internal search) is the functionality that allows users to retrieve results from a website by typing queries into a search bar. There are different approaches to site search, but they are all designed to serve website content based on a user’s input in the search bar. \nA great site search experience can improve almost any website. The better the site search, the more success you will have in two key areas: \n\nHigher conversions: Visitors who search on your site are at least 200% more likely to convert, making site search a very powerful tool for eCommerce. \nGreater brand loyalty: 12% of website visitors who are unsatisfied with your search results will turn to a competitor instead. Companies who provide an excellent search experience drive more brand loyalty, and lose fewer customers to the competition. \n\nSite search is particularly important for some websites, like eCommerce sites, that depend on customers finding certain products quickly and easily. But any website with a collection of content can reap the benefits of relevant, fast, and easy-to-use site search: better click-through rates, more user engagement, and a better understanding of customer needs. \n \nThe Connection Between SEO & Site Search\nThere’s an important connection between search engine optimization (SEO), which drives users to your site in… the first place, and site search, which helps them find what they need once they’ve arrived. \nModern SEO is based on creating high-quality, relevant, and timely web content that addresses the needs of your audience. However, the more content or products that are added to the site, the harder they are to find. \nAlthough a robust navigational menu can start visitors on a journey to discovering content, each product or piece of content cannot be captured with its own submenu. (And, even if they could, it would be quite a hassle for a user to dig through!) A better long-term solution is to invest in great site search, which helps users find what they need every time without continuously updating menus and submenus. \nPlus, site search analytics can help you uncover what exactly your visitors want, which can refine and enhance your SEO strategy. With each search, visitors are telling the business in their own words what they need. You can use this data to find out what products or content are most popular, and if there are any gaps in your content marketing, sales, or product development strategies. \nFor example, if hundreds of users are searching for something on your site, but you don’t have any corresponding products or resources, that raises a red flag. This data sparks a larger conversation with internal stakeholders (SEO, marketing, product, etc.) about how to fill that gap. You may decide to accelerate something on your product roadmap due to high demand, or set up a remarketing campaign that targets visitors who perform a site search but do not convert right away. \nSo at the end of the day, why does site search matter so much? It helps you understand your customers better, drive more conversions, and increase customer loyalty/repeat business. Up to 30% of eCommerce site visitors use the search function, and site searchers account for up to 14% of eCommerce website revenue. With a captive audience searching for your products here and now, a great site… search experience is a must. \n \nHow On-Site Search Drives SEO\nThere are numerous metrics that show how effective on-site search is. A robust site search solution should have analytics capabilities that allow you to measure these metrics, which can help you uncover user behavior and monitor your search performance. Using this information you can make to drive iterative improvements to your site and help you adapt for your users. Some notable metrics include:\n \nQueries\nUncovering what exactly your user needs is a cornerstone of SEO. Analyzing queries, or the information users type into the search bar, is an underutilized way to uncover that information. Queries provide the most direct way to understand what visitors are seeking, and help you determine which topics, content, or products are most popular. Using this information, you can make a number of strategic actions such as highlighting popular results on the homepage, bringing popular items to the top of the search results, or expanding content areas that are lacking.\n \nKeywords\nKeywords reveal some of the major topics that are important to customers. These are insightful because they illustrate the different ways customers are searching for similar things. Your site visitors may also be using these same terms in major search engines. Analyzing internal search data can help organizations find new keywords to prioritize in PPC or organic SEO campaigns.\n \nConversions\nThough the definition of a successful conversion may vary from business to business, it is still a highly relevant metric to track. Whether a conversion consists of an item added to a cart, a sale, time spent on a piece of content, or a subscription, tracking conversion can help you understand which queries perform better than others. It can also help you determine which products are most valuable to your customer.  For instance, if you locate queries with a low click position but a high conversions, you can take actionable steps to highlight the… related items for customers. Overall, items and content that convert will tend to be most relevant to your users and can fuel your campaigns, product strategy, SEO strategy, and even offline merchandising. The direct link between conversions and revenue make it an imperative metric to track.\n \nClick-Through Rate\nOn an individual site, click-through rate (CTR), or impressions resulting in a click, is an important measure of the relevancy of your search engine and how engaged users are on your site. Generally, the higher the click through rate, the more satisfied users are.  If the results showing up for users on your site have a low CTR, your site is likely failing to address user needs. It may be worth looking into redefining the relevance of your on-site search. \n \nClick Position \nExamining the click position for different content assets or products on your site can help you see how accessible they are to visitors. For example, if popular or new products have a low click position, visitors will have a lot of trouble finding that content. This might suggest some queries should be worked on (ranked higher for instance). Using this information, you can make necessary improvements to the relevancy of your search engine. Analyzing click position can also provide granular insights on users’ interests, which can be invaluable in driving SEO and online and offline campaigns.\n \nIndexing Site Search Pages for SEO \nPopular search engines like Google can index internal site search results pages with their crawlers. This was initially considered undesirable because organizations thought viewers didn’t want to see the results of an on-site search engine when they were searching with a general search engine like Google. Thus, it was standard practice to prevent bots from linking to on-site search pages. \nMore recently, some large eCommerce sites have found success allowing Google crawlers to index internal site search pages. This allows visitors to get a range of… topical results, right from Google or Bing itself, rather than just a single product or content page. \nHowever, this tactic can introduce a few risks. \nThe main risk is that, if left unmonitored, allowing bots to crawl on-site search pages can result in indexable pages for each keyword. The outcome is a mass of user-generated spam pages which greatly reduce the effectiveness—and meaning—of search results. Letting bots index any keyword can contribute to some rich, high-quality pages. Still, it has a tendency to create many low-quality pages that don’t fit user needs.\nIn addition, unrestrained bots crawling too many site search result pages may affect crawl budget. A large number of what Google calls “low-value-add” URLs can negatively affect the crawlability and indexability of the site. This is typically only an issue for large, high-traffic sites (1,000+ URLs), but it can negatively impact the discoverability of your content. \nNonetheless, as long as you investigate and minimize the risks of indexing site search pages, you can develop a site search strategy that incorporates SEO and users’ needs. \n \nSite Search Usability and SEO\nWebsites with good performance and a good user experience tend to perform better with search engines like Google. The usability of search functionality has a direct effect on the quality of visitor’s interactions on sites and the likelihood of engagement.\nTo delight visitors and profit from site search, businesses need to understand how to optimize it for user engagement. They can do so by focusing on:\n\nVisuals: Including images in the search results makes on-site search more appealing and user-friendly.\nSearch Result Relevance:  An unsatisfying search experience can quickly push visitors to other sites. Therefore, relevant results, tailored to textual and business ranking rules, are imperative. \nPage Load Speed: Although a few milliseconds of delay may seem small on paper, users are sensitive to small lags, which… build up over the course of the search experience. Search functionality should be built on robust infrastructure to prevent frustrating delays.\n\nThese are just some of the basic aspects of on-site search that can improve the site’s usability. If they’re properly implemented, organizations can create a better user experience that delivers more customer satisfaction, while driving SEO goals.\n \nOptimize Your On-Site Search with Algolia\nOn-site search is an invaluable asset for any business today. It enables organizations to increase their conversion rates, deliver the information they want when they need it, and strategically plan for the site’s continued success.\nAlgolia enables consistently frictionless experiences for all of our clients. Watch the Algolia demo to see our search in action!"
  },
  {
    "id": "7927-0",
    "title": "Beyond the search bar: advanced e-commerce search experiences",
    "author": "Ivana Ivanovic",
    "text": "A simplistic, linear approach to search experience—a customer looking for a product in a search box or bar, then either adding to the cart or bouncing—has two downsides. For one thing, it feels restrictive to modern users who want holistic, personalized experiences. Secondly, it is costly. Businesses unable to offer inspiration through methods like browsing, navigation and advanced faceting miss out on conversion and retention opportunities.\nTop retailers use search as a strategic tool, embracing, as Forrester puts it, “...a design thinking approach focused not on technology but on ease, effectiveness, and emotion.” This results in improved KPIs like conversion rates, stickiness, engagement, and increased basket size.\nWe'll look at innovative ways in which technology can power creative search experiences and discovery for your users, and better business results for you.\nWhy advance site search: a quick look at the numbers\nRespondents to the Internet Retailer KPIs & Site Search Survey, which asked 103 e-commerce companies about how site search impacts their KPIs, indicated that they are increasingly seeing the link between their site search and their top-line and bottom-line metrics.\nWe already know that site searchers are over 200% more likely to convert than regular visitors, but take a look at these statistics:\n\nForrester’s recent report titled “Revamp Site Search To Jump-Start AI, Chat, And Personalization” confirms that “digital business leaders in healthcare, retail, financial services, and other industries must master the market forces and options for site search—or jeopardize the projected ROI of their investments.”\nThese are just some of the reasons why 88% of retailers large and small told Internet Retailer that advancing their search strategy is important. Let’s take a look at how to go about it.\nMastering the basics of search experience\nLooking at the progression along the search maturity model, we see that 36% of retailers have… already moved past the simplistic, transactional model of search in the box.\n\nBut before moving forward with advanced capabilities, search technology must first satisfy a few basics. Consumers “spoiled” by Google expect first and foremost an ultra-fast search bar that provides most relevant results.\nThere are three aspects of relevance that users have come to count on:\n1. Textual relevance\nThe name of a product, its brand, keywords in the description—what we call attributes—make up textual relevance. Textual relevance means reading correctly into the user’s intent even if they misspelled a word or used a stop word (\"the\", \"and\", \"at\", \"with\"…). It also means giving them a synonym result (offering a parka when they typed in “jacket”), and making sure that plurals are accounted for (searching for “feet warmers” should display foot warmers).\n2. Business relevance\nOn the occasions when you want to promote certain products for, let’s say, a recent release or a holiday sale, you can influence the results a user sees using business relevance. You can leverage your own business metrics ensuring that the user sees content they are most likely to act on.\nYou could, for example, be tracking conversion rates on your products, and wanting to display first the result with the highest conversion rate.\n3. Personalization\nPersonalized search experience is all about taking signals from your user: the conscious signals they are sending you by interacting with your user interface, and the more subconscious, latent signals coming from their past preference indications.\nMultiple paths to conversion through advanced user experiences\nHaving covered the basics, let’s look at how progressive retailers provide advanced customer journeys past the simple: search bar > click > add to cart.\nOffering multiple content types as search results\nCratejoy is an e-commerce website that makes it easy for users to subscribe to boxes of products they’ll love.\nIf a user types… “planner” in Cratejoy’s search box, they can instantly choose a subscription, a category, a collection related to planners—or, if none of that works, dive into a blog post. This offers the user multiple paths into different types of content, which translates into multiple paths to conversion, based on user’s personal preferences and ways of processing information.\n\nFacets: filters with special intelligence\nL’Occitane En Provence is an international retailer of body, face, fragrances and home products. Similarly to Cratejoy, their search is an as-you-type experience, but they have another interesting feature that showcases advanced search possibilities. Let’s say a user types in “baby” in the search box: on the left, facets will appear.\n\nWhat exactly are facets? We are all familiar with filters as a way to narrow down our search result options. A facet is a filter that is intertwined with the result sets: the only facets that appear are the ones that match result sets. This prevents a “no result” type of screen experience, which almost universally causes users to bounce. Looking at our “baby” facets, we already know the number of items available in each category, together with the pricing.\nIf a user refines her search query to, let’s say, a “baby bath”, we see that the categories available to her have decreased, showing only applicable facets.\n\nThis means that in a few keystrokes and clicks, the user finds the very item they want. Compare this with a long list of items that comes with a standard search bar experience — by far superior.\nFrom beyond the search bar to no search bar\nArtsper, an arts buying and selling website, takes to heart the idea that search is not just about the search bar. Search technology rather powers the entire browse experience.\nIf a customer selects a “painting” category on the home page, products and facets are showing up—even though no search query is entered.\nArtsper also includes recommended… categories; a customer can easily dive into abstract paintings, for example, and just as easily remove that filter.\n\nWhy is this so interesting? No search bar query by the user is involved, even though the search technology behind the scenes takes care of the engaging and natural customer experience, including ranking and personalized recommendations.\nImagining possibilities for e-commerce search experience\nThere are many other aspects of state-of-the-art search technologies that can advance the user experience and improve your business numbers: from recommendations and dynamic faceting to letting the users define the interface.\nLearn more in our eBook: Search Beyond the Box: Innovative User Experiences for Improving E-commerce Conversion Rates\nWe hope to have inspired you to join top retailers in advancing user experiences through search capabilities that go beyond the search bar. Your users should enjoy powerful discovery through browsing, faceting, innovative UX, and much more, shortening their path to the shopping cart while driving your business results."
  },
  {
    "id": "9084-0",
    "title": "The Benefits and Challenges of Federated Search",
    "author": "Pauline Lafontaine",
    "text": "When users visit your site, they want to find relevant content as quickly as possible. And after pouring resources into perfecting your site, you want to ensure visitors can find what they need with as little effort (and frustration) as possible. \nFederated search is a powerful tool to improve your site search and benefit your overall business. But with great power comes unique challenges. With enough forethought and a clear plan, your site can reap the benefits and minimize the pain points associated with federated search. \n \nWhat is Federated Search?\nFederated search is a search technique that involves indexing multiple data sources at once, then presenting this variety of results to users in one unified interface .\nMost traditional search methods only indexes a single data source at a time. For example, a user might only be able to search through all of the HTML pages on a site or only the files stored inside a database. But with federated search, you improve the likelihood of users finding what they need on the first attempt by including more data sources. \nThere are multiple possible approaches to building a federated search, and the underlying architecture can vary. However, all federated search solutions share the same core feature of allowing users to search many data sources using one search process and interface.\n\nBenefits of Federated Search\nThanks to the ability to consolidate and centralize the search process, federated search offers a range of benefits for both the site and the site visitors.\n\nFederated search offers a number of benefits for your website:\nImproves Customer Engagement\nBy delivering a simpler, more efficient user experience, federated search increases customer engagement and satisfaction rates. Federated search also improves discoverability, by providing the most relevant information in an intuitive format and even, at times, surfacing content a user didn’t even know they needed. \nIncreases Conversions\nWith a federated search… interface, visitors reach their desired destination with fewer clicks overall. The result? Users stay on your site longer, rather than leaving due to dissatisfaction. This ultimately increases the click through rate and the chances of a successful conversion.\n\nIncreases Search Coverage and Scope\n With federated search, you can make all of your content easily searchable using a single tool. It eliminates the need to set up and manage multiple search tools for different types of content. Your single tool can be much more robust and comprehensive than each individual tool.\nEnhances Security\nLess is more when it comes to security. In other words, the fewer applications and databases that need management, the fewer opportunities there are for your systems to be compromised. Because federated search means you only have to maintain one search engine, there are fewer security risks to worry about.\nBetter Search Relevance \nThe federated search interface allows you to decide how each distinct type of content appears in the search results. With several search engines, defining the relative importance of each type of content is not possible. But with federated search, you can dynamically establish the importance of different content assets, determining, for example, if products should show up higher than blogs posts in the results.\n \nFederated search also benefits your users in several important ways: \nStreamlines the User Experience \nFor a site visitor, navigating a site map to look through a wide catalog of products can be tedious and ineffective. With federated search, users need only to know where the search box is in order to look for anything, no matter where it is on your website. You provide a single unified experience across any online properties, such as blog posts, FAQs, user-generated content, and other relevant data. \nOffers Easy-to-Interpret Results\nFederated search engines can be configured to use methods like highlighting to help users interpret search… results from different categories. In this way, federated search makes it easier for users to interpret complex information, even if the information includes multiple types of data.\n\n\nOverall, federated search makes it easier and more efficient for users and sites alike to work with the range of data sources that many organizations now maintain. \nChallenges of Federated Search\nFederated search offers so many benefits because it is more sophisticated than traditional, simple search. But with that complexity and sophistication come a few new and different challenges. They can be addressed, but they require some advance planning.\n \nFor your company, those challenges could include:\nImplementation Challenges\nAlthough a federated search solution is easier to maintain in the long run than multiple independent search tools, it often takes more upfront work to implement. Implementing federated search requires an initial investment of time and resources. Using a third-party federated search partner to create and implement the federated search architecture may consume less internal time, but will still require upfront investment.\nData Source Inconsistency\nYou can use federated search with virtually any type of data source. However, if your data sources vary widely in type and structure, the index schema will as well. Thus, the relevance configurations will need to be fine-tuned according to the data source. r\nDuplication\nIf you run the same search across multiple data sources, you increase the risk that some of the data you search will be duplicated. For example, you might have an HTML Web page and a PDF file that contain identical information, and you don’t want to present duplicate search results to your users. Fortunately, there is an easy way to address this challenge: deduplication, or the identification and elimination of redundant data. You can implement deduplication within your search tool.\n \nFor users, common federated search challenges include:\nToo Many… Results\nBecause federated search draws on so many data sources, there is a risk that your users will be presented with so many results that they will struggle to find the most relevant ones. However, you can address that risk by taking steps to optimize the relevance of your search results. Designing a user interface that makes results easier to interpret can help, too.\nLagging Performance\nFederated search may also not perform as fast as search tools that have to work with only one type of data source. That’s why it’s important to conduct search performance testing to ensure that your federated search solution meets users’ expectations.\n \nHow to Choose a Federated Search Solution\nChoosing the right federated search tool requires taking inventory of your company’s needs, your users’ needs, your existing site architecture, and your IT capabilities. Consider the following areas as you evaluate federated search options: \n \nBusiness goals\nWhat specifically is your business hoping to get out of federated search? Is ease of implementation more important than performance or coverage? Or is the usability and performance of the end product more important than the implementation process? If your business depends on search, like eCommerce sites, how important is reliability and uptime? \nThe best federated search site search solutions should provide hassle-free implementation (either through hands-on implementation or expert guidance) and unparalleled performance.   \n \nCustomer needs\nWhich benefits will federated search bring to your customers? Are you trying to simplify their experience on your website, increase engagement, help them understand complex search results more effectively, or do all of this at the same time? What features does the platform offer to help customers navigate results, such as highlighting or categorizing search results? \nMake sure your federated search provider understands your priorities and offers features that address… them.\n \nSearch Engine Features\nConsider the specific features offered by the federated search solution that you choose, and how those features reinforce your goals. For example, your federated search tool should have the ability to show different types of results in the same search box. It should easily index (and structure, if necessary) different types of content for the search, even from various websites or CMS installations. \nBefore you choose a federated search provider, find out what features they offer to help you accomplish these goals, and any other objectives for your site search. \n \nFind the Federated Solution for Your Site\nFor users and companies alike, federated search offers many benefits, ranging from greater simplicity and convenience to tighter security. But it also presents some special challenges that you need to consider in order to implement a federated search solution that best fits the needs of your business and your customers.\nAs you evaluate federated search solutions, consider Algolia. With lightning-fast search results, extensive customizability and support for virtually any type of data source, Algolia makes it easy to build an effective federated search solution regardless of the type or scope of your.\n See for yourself  the significant difference between buying vs. building search, or watch a demo to see Algolia in action."
  },
  {
    "id": "9074-0",
    "title": "The 4 Types of Federated Search",
    "author": "Matthieu Blandineau",
    "text": "All search methods are not created equal. The easier (and faster) it is to search through a website, the more motivated your users are to stay on the site, browse your content, and become a repeat customer. \nFederated search is a great way to improve the usability and performance of your site search. Federated search is a form of site search that pulls information of various types from multiple data sources and presents it in one common interface for users to browse. \nWhen federated search is implemented and designed well, it encourages users to go beyond what they were searching for. They can browse, discover, and consume more information about your business and products than ever before. Plus, the more searches conducted on the site, the more valuable data the company can collect about popular products and incorporate into decisions about the product roadmap. \nBut designing a powerful, intuitive search function requires choosing the right approach to federated search. The right approach depends on your website, how data is currently organized and stored, and the overall goals of your website and business. \n \nDifferent Approaches to Federated Search\nFederated search involves two fundamental processes: indexing and searching. \nFirst, data is indexed. Indexing involves gathering, parsing, and storing data in a way that enables streamlined, efficient search. For the typical site search solution, indexes need to be updated at specific intervals, dependent on how often new data is added and how quickly the new data needs to become searchable. \nNext, the data is searched. The search process involves querying the indexes to return the right information in the right order.\nEvery federated search tool is based on indexing and searching. But there are three different approaches to indexing and searching to choose from, based on your business needs: search time merging, index time merging, and hybrid federated search. \n \n1. Search Time Merging\nSearch time merging… (also sometimes called query time merging) involves maintaining a separate index for each data source that you want to include in your federated search. Then, to perform a search, you search each index separately for a given search term. You may also need to deduplicate data by identifying results drawn from redundant data sources. Finally, the search results are aggregated to produce the list of final results.\nAdvantages and Disadvantages of Search Time Merging\nThe main advantage of search time merging is that it is the simplest federated search method to implement. Because it does not require you to create a central index for all of your data sources, you can set up a search time solution quickly, using the indices you already have within each data source.\nIn addition, search time merging can be simpler to set up because there is no need to standardize your indices. The data structures for one index could be different than those for another, but search time merging will work with both.\nOn the other hand, the performance rate of searches conducted using search time merging tends to be slower than that of other federated search methods. It is less efficient to search multiple indices independently. If one index is particularly slow to respond, the entire search will be delayed. Finally, setting up a satisfying relevance for the aggregated results list can be very challenging, as it comes to comparing apples with oranges \n \n2. Index Time Merging\nAn alternative approach to federated search is index time merging. With this approach, you create a central index for all of your data sources, then parse that index in order to perform a search.\nAdvantages and Disadvantages of Index Time Merging\nBecause you only have to search one index, index time merging typically results in faster searches than search time merging. This is the primary advantage of index time merging. Index time merging also allows you to include data sources that do not have their own search… functionality, and therefore cannot be used with search time merging. \nThe chief disadvantage of index time merging is that it requires more effort to implement. Instead of being able to parse a collection of indices, you must create a central index for all of your data sources, and update that index whenever the data sources change. Plus, if some of your data sources are formatted differently from others, you need to standardize all data to be the same format. Similarly to search-time merging, it still requires you to decide on a unique relevance strategy for all your different types of content, which isn’t optimal.\n \n3. Hybrid Federated Search\nYou can also take a hybrid approach to federated search by combining some of the methods from both search time and index time merging.\nFor a hybrid federated search, you create a central index for as many data sources as possible, just as you would for index time merging. However, if you have data sources that cannot easily be represented to the central index, you maintain separate indices for them. When you execute a search, you search all of the indices—your central index, as well as the additional indices that exist for any other data sources not represented in the central index. The search results based on all indices are aggregated to create a final list, as you would do with search time merging.\nAdvantages and Disadvantages of Hybrid Federated Search\nBy reducing the number of indices that need to be searched, hybrid federated search provides better performance than you would achieve with search time merging. At the same time, however, it does not require you to create a single index for all of your data sources.\nThe chief disadvantage of the hybrid federated search technique is that, because you still have more than one index to search, performance is usually slower than it would be if there were a single index.\n \n4. The Federated Search Interface\nThis method starts similarly to the search-time merging method,… but instead of aggregating the results in one result list, it presents one result list for each type of content the search is performed on, in a unified interface.\nAdvantages and Disadvantages of the Federated Search Interface\nNot only does the federated search interface deliver superior performance, it also allows site owners to independently fine-tune relevance for each type of content. However, achieving these benefits does require a bit of forethought. The design of the final interface should reflect the experience a site owner wants visitors to have, thus some strategic planning is needed. Moreover, all search site tools are not equipped to display a federated search interface. Thus, a site owner would need to ensure their site search solution is capable of both indexing different types of content in different indices and presenting that information in the most user-friendly way. \n \nHow to Choose a Federated Search Approach\nWith four different federated search techniques to choose from, how do you decide which is best suited to your business’s needs? There are several factors to consider.\n \nData Environment\nYou should consider the types of data you have, and which tools are available to you to manipulate, index, and search them. If your data sources comprise widely varying formats, a search time approach will typically make most sense. Search time is also more viable if each of your data sources can be easily searched independently, which is the case if the data is structured consistently. If, on the other hand, all of your data can easily be standardized into a single database, index time merging is a better solution. However if you have a range of different content forms and your search solution supports federated search interfaces, it should be the preferred approach.\n \nDeveloper Needs\nYour developers are an important factor in deciding which federated search approach to use, too. If you have a large development team and the resources necessary to… build a central index, index time merging may be a good fit for you. But for smaller development teams, search time merging may be a more practical option, since it requires less effort to implement. If your developer team does not have much experience in building search applications, a third-party federated search solution might also be an attractive option.\n \nUser Needs and Experience\nAt the end of the day, the main point of search is to connect your content with your users’ intent. User experience requirements should be high priority when deciding which federated search approach to take. If users expect a single list of heterogeneous results (think Google Drive), search-time or index-time merging are good solutions to decide between based on your data environment and resources. If users already know what they are looking for but could benefit from additional content or a structured results layout, a federated search interface is the right solution. \n \nFederated Search and Algolia\nNo matter which approach you take, Algolia can help speed your implementation of federated search. With blazing fast search results, support for virtually any type of data source and the ability to customize search UIs to help guide users, Algolia makes it easy to add federated search functionality to your website. See for yourself by signing up for a free trial. "
  },
  {
    "id": "9069-0",
    "title": "Alternatives to Google Site Search",
    "author": "Matthieu Blandineau",
    "text": "Once upon a time, Google offered an easy, ad-free way for businesses to integrate search functionality into their own websites. For a monthly fee, companies could use Google Site Search to index their websites and allow visitors to search the site’s contents. This allowed companies with limited resources to provide powerful search functionality without building their own system. \nGoogle discontinued Google Site Search in April 2017 in favor of a new solution called Custom Search Engine (CSE). In the wake of this announcement, companies have to decide whether to try out Google’s new site search tool or move to another site search provider. \nRead on to learn more about Google Custom Site Search and the pros and cons of potential alternatives to Google’s site search tools. \n \nWhat is Google Custom Search Engine?\nGoogle Custom Search Engine, the successor to Google Site Search, allows web developers to create custom search engines for their websites and external content.\n The solution offers several benefits:\n\nCost: It’s free to use basic features.\nControl: Developers can choose which pages from their own or other sites the search engine should index.\nSimplicity: It’s easy for programmers to integrate the search service into a website using a JSON API.\n\n \nHowever, Google CSE can also create challenges for many businesses:\n\nAds: Most sites’ search results are subject to including ads placed by Google AdSense. (Nonprofits can apply for an exemption from having to display ads.) Although website admins can prevent certain types of ads from being displayed, they do not have full control.\nExtra Costs: Blocking ads completely costs $5 per 1000 queries. If your site search is used frequently, this cost can increase quickly. \nImpact on Brand Image: Website visitors tend to dislike ads, which can be distracting and raise privacy concerns. Most ordinary website users, who do not understand how CSE works, may wrongly assume that your business is responsible for… displaying the ads, which could hurt your brand’s image.\n\n \nAlternatives to Google Site Search\nFor businesses that want more control over their search results or do not want to display advertisements in their search results, there are a number of alternatives. \nSome run as on-premises platforms, while others are SaaS solutions hosted in the cloud. The solutions vary in regard to capabilities, benefits, and the industries and types of content they are best-suited for. \n \nSaaS site search solutions\nSaaS search solutions are typically the simplest for developers to integrate into their websites. \nMost offer APIs that programmers can use to specify where and how a custom search tool should appear within a website, as well as which types of content it should display. The SaaS site search  solution handles the rest of the work by indexing the site’s content and delivering search results automatically. Handing off the responsibility of hosting and maintenance to a SaaS provider reduces strain on internal developer teams. Those teams can be comprised of fewer people overall and spend more time on business innovations and less time keeping things working smoothly. In addition, operational costs also go down, since infrastructure such as storage and servers are no longer needed. The best SaaS providers should ensure high service reliability and be equipped to scale with your business needs.\nAlthough all SaaS solutions confer measurable benefits over on-premise solution, there are a number of other features that might make your site search partnership even more profitable. Features such as distributed search infrastructure, multi-language optimization, and the ability to power multiple channels such as web, mobile, and voice, may help a SaaS solution to stand out among others.\nThere are a number of providers to choose from. Providers like SiteSearch 360, Swiftype, and, of course, Algolia serve a range of use cases. Then, there are also specialized e-commerce… solutions like Cludo and Klevu. Regardless of the branding, however, you should pay attention to the pricing model (some prices vary by pages indexed and queries), personalization features, analytics capabilities, ease of customization and implementation, and the overall reliability. In addition, some all-purpose solutions have a range of integrations with a number of content management systems and e-commerce platforms. Make sure your chosen tool is willing to partner with you to provide the most tailored experience.\n \nAlternative Solutions from top Cloud Providers \nMajor public cloud providers also offer custom site search solutions. These services are fully hosted in the cloud, but they differ from the SaaS options described above because they are part of larger cloud-computing platforms. These services can be an attractive search solution for companies that already use a major cloud provider for other needs, like virtual server hosting or other general-purpose cloud computing. While it may be simpler for your company to work with one less vendor, these solutions are often nearly impossible to customize to your specific business needs.\nThere are two popular competitors in this space:\nCloudSearch, the custom search service provided by Amazon Web Services (AWS), offers a great deal of configurability and boasts advanced features, such as highlighting and natural language support. However, indexing can be a hassle, as new fields are not searchable until they are indexed, and it can only search one index at a time. Azure Search, which is hosted on the Microsoft Azure cloud, offers AI-powered customized search results based partly on the same engine that drives the company’s Bing search platform. Pricing for both options depends on a variety of factors, such as storage and geographic region, and may be hard to nail down upfront. In addition, neither solution provides UI customization for business users to control the experience or analytics… capabilities\n\n“Default” or “Native” Site Search \nMany content management systems (CMS) or e-commerce platforms come with a native search solution pre-packaged for your use. But you shouldn’t default to default search without making sure it meets all of your needs. Let’s look at these options.\nWordPress and other CMS search solutions\nIf you build your website using WordPress, a massively popular open source Content Management System (CMS), you can use WordPress’ built-in search widget for indexing site content. You can place the widget wherever you want on your site and alter the search bar header. But that’s pretty much it. \nMost other major CMS platforms, such as Joomla and HubSpot, also offer built-in site search functionality. The upside? One less vendor to deal with. The downside? Overall, they don’t offer a robust and personalized search algorithm, and the breadth of features is typically lacking. Luckily, most third-party SaaS search solutions can be integrated into any CMS platform with ease.\nE-Commerce Platform Internal Search Solutions\nSome e-commerce platforms, such as Magento, Shopify and Prestashop, also offer built-in search solutions. While these search solutions are appealing because they don’t require any additional installation or cost, they cannot provide relevant e-commerce results with the same speed and quality as a customizable, standalone site search solution. The recommended path to a robust e-commerce search on those platforms is to extend them using the plugin of a standalone search solution.\n\nOpen Source Solutions\nAs an alternative to the SaaS site search solutions described above, businesses can also create and deploy a search tool on their own server. But be warned, deploying an open source solution requires intensive work, as the search system needs to be built from scratch. In addition, the responsibility for implementation, maintenance, scaling and upgrading the system falls solely on the site or business. And if… something goes wrong, there is no professional support service to turn to--unless you pay for it. Overtime, these costs can add up to make the total cost of ownership higher than a hosted solution.\nHowever, these open source tools are free to download and use. For many of these tools, the open source developer community is highly active, and your company can benefit from features developed in the community. Still, this option is best suited for those with significant resources and knowledge of how to create an effective search algorithm from scratch.  \nTwo popular open source solutions are Solr and Elasticsearch.  While both solutions will work with virtually any site, you will need technical knowledge to implement and maintain them. For most smaller sites (or larger sites with higher priorities), this sort of upkeep could be an unnecessary investment. In addition, if you intend to use these solutions for consumer-grade search, keep in mind that it will require significant investment in both the backend and front-end of the site. \n \nChoosing the Best Google Site Search Alternative\nThanks to the void left by Google site search, companies have a long list of solutions to choose from. Making that decision can certainly be difficult. \nWith some site search solutions tailored to specific verticals—e.g. SearchSpring, EasyAsk, SLI and Nextopia all market to e-commerce sites—it might make sense to choose a search platform designed specifically for your industry. However, many seemingly “general” site search solutions may provide all of the tools you need for success. You should make sure the solution matches your site’s needs, not just your vertical. \nWhen it comes to site search solutions that are ready to use out-of-the-box, there are several factors to weigh when deciding which of the many options available best fits your needs:\n\n\n\nSearch customization: How important is it for you to be able to control the results your users see? Is it critical to avoid… displaying certain types of content, such as politically sensitive material or out-of-stock products? Do you want to be able to set rules that control how the search engine responds to certain types of queries?\nAnalytics: Searches generate a lot of data, which can help you understand what your users want. Search platforms that provide structured tools to help you decipher and act on that data will give you a leg up in a competitive online world.\nSpeed: Not all search tools are equally equipped to deliver fast results, especially when they have to parse large volumes of data. But in a search world dominated by Google, site visitors are primed for instantaneous results. Can your site search deliver the speed necessary to guarantee a positive user experience? \nPersonalization: Even though two users might search for the same query, their ideal search results could be different. That is why adding a personalization layer to search results within your search platform can help to ensure that you meet your users’ expectations.\nImplementation and support: How much expertise do your developers and IT team have in setting up a search tool? If their experience in this domain is limited, it makes sense to choose a tool that requires minimal setup and provides adequate customer support.\nMaintenance: If you set up a server-side search solution on your own hardware, you will have to maintain that hardware indefinitely. A SaaS solution eliminates this need. Do you have the bandwidth and resources to maintain an in-house solution?\nVisual design: Usability and visual design of the search interface is a crucial part of customer experience. Does the tool you’re researching take this into account?\n\n\n\n \n\nIntegrations: A tool that works with the platforms, plugins and extensions your site uses is paramount. Consider the tools you are currently using to support your site, and see if your top choices for site search integrate with those tools (or desirable alternatives). If they… don’t, the road to implementation will be more challenging. \n\n \n \nSite Search with Algolia\nIf you’re seeking a powerful site search solution that is simple to install and configure with little-to-no developer experience, Algolia is the leading choice. As a fully hosted, SaaS search solution, Algolia’s search functionality can be integrated into websites and mobile apps quickly and easily using APIs, advanced front-end libraries, and a crawler if need be. In addition, Algolia personalizes search results based on users’ individual needs while still delivering extremely fast performance.\nTo see Algolia in action on your site, watch our demo, which gives overviews all of Algolia’s premium features and professional customer support."
  },
  {
    "id": "9064-0",
    "title": "How to Analyze Your Internal Site Search Data",
    "author": "Xabier Ormazabal",
    "text": "When was the last time you performed site search analysis for your website? If your answer is somewhere in the ballpark of “never,” you’re not the only one. Few companies dedicate resources to digging through site search data. But even if you’re not alone, you’re definitely missing out. Every time visitors search your site, they generate tons of useful data that just might answer your burning questions.\n \nWhat is Site Search Analysis?\nSite search analysis is the detailed examination of visitor search behavior on your website. Those behaviors include keyword and key-phrase searches, filter and facet selections, and search suggestion clicks. This analysis can be done through a dedicated site search analytics dashboard. \n\n\nWith site search analysis, you’re able to enhance your website weekly or even daily based on what you continually discover about your users. For example: \n\nRetail and E-commerce sites can discover the week’s most popular items and have them show up in more searches.\nMedia websites can uncover popular topics and develop content that resonates better with their users.\nAlmost any kind of website can improve design and functionality based on the latest user insights.\n\nAnalysis is a critical step to realizing these enhancements. But that’s only the beginning of what it can enable you to do.\n \n6 Ways Site Search Analysis Can Make an Impact\nThe advantages of site search analysis are substantial. It improves sales and profitability, bolsters competitiveness, and promotes long-term customer satisfaction. Here are six ways that analyzing your site search can impact your business:\nReveals the quality of your site search \nOnce you start looking at your data, you might be surprised by the percentage of visitors using your site search and by the experience they are having. If visitors are leaving immediately after searching, this is a clear indication their needs aren’t being met. You can and should devote efforts to improving the search… functionality.\nDemystifies your users and their user intent \nEvery time your website’s visitors type something into the search bar or apply a search filter, they’re telling you valuable information about what they want. Understanding the customer is vital to the success of any business or site, and it can be particularly beneficial to marketing efforts such as market segmentation.\nEnhances user experience (UX) \nAs visitors search and navigate your site, they leave a breadcrumb trail indicating how easy (or hard!) it was to find the things they desired. You can use these clues to fine-tune your website, tweaking the design to provide the most customer-friendly UX. You can now ensure that navigating your site is straightforward and intuitive for visitors.\nImproves conversion rates \nThe link between conversion rate and site search is well-established. Website users who run site searches are more likely to convert than those who don’t. Lacoste increased their conversion rate by 37% and their sales from search by 150% after implementing a site search solution. But to achieve these high conversion rates, you need insight into customer intent to make sure your search functionality is perfectly tuned and leading visitors to the right place.\nReduces bounce rates \nKISSmetrics has undiscovered that 12% of a website’s visitors will leave and opt for a competitor’s site after an unsatisfactory search. Site search analysis data can help you optimize search results for accuracy and speed. \nEnhances SEO efforts \nAlthough site search analysis isn’t designed to be an SEO tool, it has an indirect positive impact on SEO. Implementing insights from site search analysis bolsters UX, and since Google favors websites with excellent UX, it’s no coincidence that some of the highest-ranking websites are also easy to search.\n \nHow to Analyze Your Site Search\nBecause every website and customer base is different, there’s not just one “right way” for a business to act… on their site search analysis.\nThere’s a lot to learn, but before you get started, you’ll want to set up tracking analytics in the dashboard of your site search platform. This will allow you to analyze the performance of your site search. \nOnce you’ve got that set up, you’re ready to employ site search tactics that could benefit any site. Here are some tips for content and search configurations that are informed by site search analysis.\nTips to Improve Content Searches\nModern-day marketers understand the importance of high-performance content. But even if your content team has filled your site with the highest-quality articles, infographics, and videos in the industry, those assets are not valuable if they aren’t aligned to customer needs or can’t be found. To bridge the gap between you and your site visitors, you need to examine and act on their search behavior. \nOptimize Content Around Keywords\nThe more you use site search analysis to continue learning about your customers and customer intent, the better you’ll become at crafting content that quickly shows up in their search results. Include the keywords and key phrases your users search for, and make sure you’re anticipating their needs in your content strategy.\nFix “0 Results” Occurrences\nSearches that return “0 results” are a surefire way to drive visitors away from your site. \nThis could indicate that content is improperly or insufficiently tagged or that your visitors are searching for content topics that you’re simply not addressing on your site. When this happens, it’s time to fill the content gaps. \nMaybe you’re missing information about specific product features, demonstration videos, FAQs, or spec sheets. Whatever it may be, once you produce it, you can be certain you’re providing purposeful content since it’s validated by your analysis. \nImprove Content When Users Aren’t Clicking Search Results\nIn your site search analysis, you might uncover occurrences where… your visitors’ searches are returning results, but your visitors aren’t clicking those results. This could be a sign that though your content may be aligned to the keywords, it doesn’t match the user intent behind the query. Again, this could be a simple matter of making sure content is properly tagged, or it could mean revamping your content to address the questions and curiosities your visitors have about those topics.\nMake Content More Accessible with Better Website Design\nOnce you’ve performed search site analysis and discovered your visitors’ most highly searched content topics, it’s smart to make this content easily accessible through intuitive navigation as well as search functionality.\nDon’t bury frequently searched content topics under numerous clicks and layers of pages. Instead, feature sought-after content prominently for every visitor to see. You might even consider adding prominent sidebars displaying the most searched content on your site.\nAlso, it’s a valuable practice to analyze the page a user was visiting just before they searched for a keyword or phrase. Did that page already provide the information a visitor searched for? If so, the user might not have seen the information due to poor UX design. \n \nTips for Your Search Configurations\nThe most advanced site search platforms offer robust tools and configurations to help you give users the easiest and fastest search experience possible. Here are some tips for maximizing these features.\nAdd Filters and Facets to Prevent Search Overload \nIf you search for an “iphone charger” in Amazon, you get over 40,000 results. Having options is almost always a plus, but users can be overwhelmed when too many results are displayed without a sense of prioritization. Hundreds (or even thousands) of search results for a single query can lead to search-result overload, making it easy for visitors to get lost, distracted, and dissatisfied. \nActivate filters and facets within your search platform… to present your visitors with more relevant, narrowed-down search results to lend ease to the searching experience.\n\n \nConfigure Search Settings Based on User Click Depth\nIf site visitors are clicking into the second, third, and fourth pages of our search results, you should take notice. The longer it takes a user to reach their goal, the less satisfying their experience becomes.\nIf your analytics reveal this problem, then click depth improvements are needed. Consider leveraging Query Suggestions, a search function that suggests keywords as the user is typing, as a potential way to reduce this hassle. Query suggestions effectively bring relevant information to the user in the search bar. For example, when a user types “mac,” “MacBook Air” and “MacBook Air 15 Inch” might be displayed to help the user find what they need sooner.\n\n \nConfigure Search Settings Based on Changing Product Demand\nProduct demand often changes with the seasons or the latest viral fads, and it’s easy to be caught off guard. Using site search analysis, you can observe changes in the frequency of particular product searches to gain real-time insight into what’s currently hot and what’s not.\nWith a deeper knowledge of current demand, website owners can configure custom ranking settings to bring the hottest-selling items (or most-read content) to the top of the search results.\nConfigure Search Settings Based on Your Own Daily Testing\nIt’s important to run your own searches on your site, and to do this frequently. Search for prominent keywords and analyze the search results.\n\nAre your searches returning relevant results? If not, you’ll want to configure your search settings based on the problems you’ve found with your searches. This could mean creating search synonyms (such as “soda” results being listed along with “pop” results,) as well as adding filtering, alternative sorting, and other configurations.\n\nIf your site search platform doesn’t allow you to make… adjustments and configurations, consider re-evaluating your platform and testing a tool that does.\n \nTry Out Algolia Site Search Analytics Free \nWith Algolia’s intuitive analytics dashboard, you’re able to track key site search metrics:\n\nSearch queries: The most popular queries and the ones returning no results\nSearch filters: The most commonly used filter attributes and values\nSearch results: The results that are most returned to users across all queries\nClicks: Track users’ click positions and clickthrough rate (CTR) of a given query\nConversion rates: The conversion rates of all search queries\n\n \nThe dashboard explains each metric in an easy-to-understand graphical format and provides all the insight you need to take the right courses of action based on objective data.\nTry out Algolia’s robust site search analytics today with a 14-day free trial."
  },
  {
    "id": "9045-0",
    "title": "What is Site Search?",
    "author": "Xabier Ormazabal",
    "text": "Site search is the functionality that enables users to search a given website’s content or product catalogs with speed and relevance. A great site search function is tailored to the specific website. Not only does a great site search constantly index the site to ensure the latest content is easily accessible, it also guides users as they explore a website’s content, helping them discover content they might not have even known they were interested in.  The best site search products delight users by allowing them to quickly connect with the content they need while capturing valuable data about the content and products visitors are most interested in.  \n \nWhy Is Site Search Important?\nSearch sites like Google and search-centered websites like Amazon and YouTube are an integral part of our experience on the internet. So it’s not surprising that when it comes to searching individual websites for products, content, or information, visitors expect the same level of ease and speed that these sites offer.  \nSearch bars are an essential part of website functionality and design, and for most of us, using a website without a search bar would seem—well, weird. A search bar is a no-brainer when designing a website, but truly helpful site search is so much more than just having a search bar.\nResearch shows that visitors that use search are 3 times as likely to find what they are looking for and convert into customers or leads than users that simply browse.  Search is also the best way for visitors to tell you what they are looking for and to give you the opportunity to tailor the website to the visitor’s specific needs.\nStrong site search can have a powerful effect on your website: \nImprove user experiences\nA site can have awesome content or phenomenal products, but if users can’t find it, they’re likely to be frustrated. A great site search experience combines fast relevant search results with a great post-search discovery experience that allows users to… filter results, browse different categories of content, and even recommend related and popular searches. Ease of discoverability leads to positive and continued interactions between visitors and your content. Thus, you should consider site search just as important to the user experience (UX) as aesthetics and functionality. \n\nReduce bounce rate\nResearch from KISSmetrics shows that 12% of a website’s visitors will leave and opt for a competitor’s site after an unsatisfactory search. Don’t drive your visitors away! If you offer a fantastic search experience, your website can be the one visitors turn to after they leave a competitor’s site empty-handed.\n\nBoost conversions\nWhen you help visitors find what they’re seeking the first time around, it has a dramatic impact on sales and engagement. In fact, according to Forrester research, up to 43% of visitors immediately navigate to the search bar when they visit a site.\nA customer-centric focus on site search has beneficial downstream effects on business performance. A dedicated site search solution allows a business to keep control over the searchable and promoted content on the website and provide users with helpful recommendations. Site search is so integral to the day-to-day health and operation of a site that it should not be treated as an afterthought.\n \nThe Benefits of Site Search for Your Business \nSo just how does this user-centered approach help to buoy the business? You can use robust search capabilities to achieve a healthier bottom line in several ways: \nGive users a better understanding of your products, services, or content \nThe more your visitors discover what’s offered on your site, the more knowledge they gain. This knowledge can be easily translated into action, especially if it’s what the visitor needs. A good site search facilitates an easy discovery process. And natural discovery serves to capture and hold the attention of the user.\nAnalyze and capitalize on valuable data\nEach time… a user searches your site, they generate valuable user intent data. Users are telling you their desires in their own words. With site search analytics capabilities you can make strategic moves like:\n\nOptimizing your results: Prioritizing relevance helps decrease the time a user spends sifting through results. Once you evaluate trends and your own business goals, a robust site search will allow you to fine-tune relevance based on attributes such as popularity or  Twitter likes or retweets to ensure the most relevant results rank the highest for site users.\nFilling in content gaps: Visitors might be searching for content topics you haven’t even considered writing about yet.\nUnderstanding your users better: Who is searching for your products? Where do they live? Which products are most popular in which months? Site search analytics can help with market segmentation. \n\nEnhance your SEO Efforts\nEffective site search improves the overall UX of your website, which is something Google values when ranking sites. Site search also requires a logical, hierarchical organization of content, which helps search engines like Google understand your core topics and products (and how they all fit together). \nAlso, if your visitors frequently search for a word or phrase on your site, they’re probably searching Google for that keyword as well. So, as you discover highly-searched words on your site, you can strategize on how to address the user intent behind those keywords. \nEase growing pains\nA website’s expansion, including the addition of daily content and/or new product lines, can actually harm UX. When a website grows significantly, locating an exact item is like finding a needle in an ever-growing haystack. But with the right site search, the size of the haystack isn’t a burden for visitors. They can easily find the exact thing they need and get suggestions for other related content to continue on their journey. \n \nWhich Types of Sites Should Have Site Search… Capabilities?\nInternal site search can make any website or technology platform easier to use. A positive experience with site search drives more repeat visitors, more subscriptions, a stronger brand image, and fewer user complaints. \nNevertheless, here are a few types of sites that may benefit the most from effective site search:\nEcommerce sites\nWhen shoppers know what they’re looking for, they don’t want to navigate through a jungle of product categories to locate it. And you don’t want them to do this either, since wasted time equals lost customers. Therefore, at all times, but especially during the busiest shopping seasons, it’s vital to give your visitors the best online shopping experience. Help them search, find, and buy with ease, and see an increase in ROI in return.\nMedia Sites\nThe content on media sites is ever-expanding and changing. It’s easy for readers and video watchers to lose their way in such a content-rich environment. Site search can help them find it, directing them to new and relevant topics of interest and prolonging the time they stay on the site.\nSaaS Companies\nSoftware as a Service (SaaS) platforms that allow customers to search through their created data can greatly enhance the user experience. And with every SaaS company now faced with almost 10 competitors on average, offering a better UX than the competition is more important than ever. \nOther types of highly-searched websites that benefit from internal site search include web forums, wikis, and knowledge bases. Businesses in the healthcare and finance industries can also benefit greatly from a smooth site search.\n \nHow To Choose The Right Internal Site Search \nNot all site search platforms are created equal, and price is only one of many factors. Look for a platform that offers all the features you need to provide an advanced search experience—both today and in the future, as your site grows. Here are a few features to look for. \n\nAnalytics: Many site search benefits… can’t be realized without a site search equipped with analytics capabilities. Make sure the platform you choose enables you to understand your visitors and constantly refine your site.\nCustomization: The best site search platforms enable you to include your visitors’ profiles in your ranking strategies. The platform can then personalize the results your visitors see based on their prior searches and website behavior.\nFunctionality: The site search functionality should perform reliably, quickly and easily with features like typo tolerance, suggestions, filters, synonyms, and multilingual options.\nEase for developers: Choose a platform that’s designed to work with your developers, not against them. It should be intuitive to implement and maintain (likely even accessible to teams with little to no coding skills), work with numerous integrations, provide 24/7 support, and have an active developer community. Implementing a site search platform should be an exciting milestone for everyone involved, not a pain.\nSecurity:  Your internal search platform should offer security features like SOC 2 and SOC 3 compliance, API keys, cluster isolation, and multi-tenant architecture.\nReliability:  With site search such an integral part of the customer experience, you can’t afford downtime or unreliable solutions. Look for a trusted solution built on reliable infrastructure with a team dedicated to handling issues before they can ever affect you.\n\n \nSee How Algolia Can Improve Your Site Search \nWorld-renowned brands such as Lacoste, Stripe, Twitch and Birchbox, turn to Algolia as their site search engine of choice. Trusted for its robust search features, analytics, security, and ease of use, it’s an out-of-the-box solution loved by web visitors and developers alike.\n\nTo see what Algolia can do for you, try it out with a 14-day free trial."
  },
  {
    "id": "9032-0",
    "title": "Build better search personalization with control and visibility",
    "author": "Matthieu Blandineau",
    "text": "Today, everybody agrees on the positive impact of personalization, for consumers and businesses alike. From simple demographic based website personalization to comprehensive behavioral based multichannel personalization, every step towards a more personalized experience is a step in the good direction.\n\nHaving said that, how do you do personalization right? Which strategy is the right one for your audience and your business? What is the line between a creepy personalization and a personalization that really enhances the experience — and serves your bottomline?\nThis past July, we announced Algolia Summer ‘19 feature launch, enabling businesses of all sizes to deliver fast, tailored and personalized experiences across any channel.\nToday, as we unveil the General Availability of our Personalization Simulator, we’d like to double down on the importance of  total visibility and control of your personalization strategy.\nCommon personalization pitfalls\nPersonalization done wrong is worse than no personalization at all. Poor implementation has the potential to annul every uplift brought by personalization. Or even worse, damage the experience and drive your users and shoppers away from your brand. Here are key pitfalls to watch out for.\n\nImproper data management. Let’s take data. A personalized experience is only as good as the data it relies on. But with this indispensable data come colossal responsibilities for businesses using it to drive their personalized experiences. Data collection should be transparent and based on consumers opt-in. Data storage and processing should comply with the highest security standards. Data should be used for very specific, clearly stated purposes. Handling data leaves no room for approximation: not only it can quickly be unlawful, but also demolish any trust consumers had in your company. 75% of consumers said that if they don’t trust a company to protect their data they won’t buy from them no matter how great their products… are.\n\nUnfortunately, the complexity doesn’t stop here.\n\nTaking into account the wrong variables. A lot of variables can be taken into account to power a personalization strategy, however not all of them are relevant. Imagine Netflix-like use case. One of the parameters the personalization could use to personalize the content surfaced to users is the number of words in the title of the movies. Would it make sense and enhance the experience? Probably not, no matter what state-of-the-art personalization technology you use.\nMaking your personalization...a little too personal. Going further, when not well configured, personalization can become creepy. Personalization that follows consumers across their online activities, outside of a given brand’s properties, or that is too strong, will deter those consumers.  63% of them say they would stop purchasing products and services from companies that take “creepy” marketing too far. A personalization applied too strongly will not serve the experience, but rather constrain it. It will add friction for users to discover and enjoy the breadth of your offering.\n\nAs Jay Baer puts it, “[...] we cannot ever forget that our audiences—not us—are those that decide what is satisfying and what is creepy. They decide what is clever and what is stupid. They decide what is in bounds and what is out of bounds.”\nVisibility and control are fundamental\nYou won’t be able to quickly assess the behavior of your personalization, nor optimize it, if you have no visibility on the logic behind it. Of course you will probably A/B test a personalized vs. non-personalized experience before rolling out your personalization to all your digital properties and all your traffic. But you’ll still be exposing a potentially broken experience to part of your traffic for several business cycles, and losing those users, as well as wasting this time before iterating.\nGiving the keys of a core element of your user experience to a black box has its… downsides even when it works well. You won’t be able to know why your personalization strategy is performing well, and to apply those learnings to other parts of your experience, or your business altogether. Visibility of the personalization logic is therefore critical.\nVisibility unlocks even more value when you can act upon your learning, and define this personalization logic. Like in any conversation, context matters. Instead of blindly applying a personalization algorithm built for any business, control on the personalization logic allows you to adapt it to your business and your audience. If you know that a visit to a product page is less important than an “add to cart”, or that an interest in a brand is more meaningful than an interest in a color, you should be able to apply it to your personalization.\nPersonalization is key in every part of the user experience, and we believe it is especially true for search and discovery. Search and discovery are where consumers will most clearly state their intent. Answering them with irrelevant, non-personalized content is not an option.\nIntroducing Algolia personalization simulator: tale of algorithms, insights and human expertise\nAt Algolia, we believe that our customers should have full visibility and control over the search and discovery experience they build for their business, and their audience. We enable our customers to combine their knowledge, the data we surface to them, and our algorithms in order to achieve best results. And it is with this mindset that we’ve built our personalization.\nAlgolia Personalization Simulator is the human-facing side of our personalization engine.\n\nControl:\n\nWith this tool business stakeholders can quickly create personalization rules based on ingested customer behavioral data — such as click data and purchase data  — to tailor search results for customers. They can define what events to take into account (such as page views, add-to-carts, etc.) and how important they… should be in the personalization strategy.\nIn parallel, they can define how important the affinity to each attribute of their products or pieces of content in the personalization logic is. So based on their knowledge of their business and their audience, they can for instance define if an affinity to a brand is more important to an affinity to a color.\n\nVisibility:\n\nThe personalization simulator allows business teams to visualize the effect of changes to their personalization strategy in real time. They can quickly see how their configuration changes search results and impacts the end user experience. With the simulator, users can easily toggle across different user profiles to ensure their personalization strategy makes sense and actually enhances the experience for real life users.\n\nTransparency:\n\nA side but nonetheless extremely valuable effect of this personalization approach it that it allows businesses to expose the personalization profiles computed by Algolia to their users. As discussed above, consumers are becoming more and more demanding about their personal data, and are turned off by invasive personalization. Exposing them why they are shown specific products or content is key to gain their trust.\n\nUse case: a large luxury fashion marketplace\nLet’s look at a real life use case of this new tool. A large luxury fashion marketplace in Europe is using Algolia to power their search and discovery experience.\nGiven the breadth of their catalog (~1M items), it’s reasonable to think that all their shoppers aren’t looking for the same items. So,they decided to offer their shoppers a personalized experience — one that made sense for their audience, business and catalog.\nTheir business is unique which is why the ability to control the personalization logic matters hugely. For instance, people clicking on very high-end handbags might just want to have a look at them and not actually buy them. Therefore, the ability to tell Algolia that an add to cart or add… to wishlist is a way more important event to consider than a simple click on an item is critical.\nIn addition, they’ve seen that shoppers tend to buy items from the same brand, while the color for instance isn’t that important. That’s why they decided to optimize the personalization logic to show their shoppers items of the brands they interact with the most in priority. A black box personalization tool that optimized for criteria that didn’t make sense for them wouldn't have been productive.\nGreat user experiences delight users at every step of their journey\nPersonalization is only the cherry on the cake of a great experience. We have more tips and best practices for your multi-channel experiences. Watch our Algolia Summer ‘19 webinar on demand to learn how to create amazing, personalized experiences across platforms.\nAnd if you prefer a personal tour, we’ll gladly connect you to a search specialist. If you’d like to speak to a search specialist. Meanwhile, we’d love to hear your personalization tips. Leave a comment or tweet to us."
  },
  {
    "id": "8996-0",
    "title": "Drive loyalty with better mobile experiences",
    "author": "Matthieu Blandineau",
    "text": "Consumers today want instant, frictionless Google- and Amazon-like experiences when looking for information, media content and products. They demand intuitive experiences that deliver exactly what they are looking for — regardless of the digital interface — and they want it Now. These are “Now Moments”. Today, this is what determines whether a customer engages, subscribes, or buys from a business. Great mobile experience is how you drive loyalty.\n\nThis past July, we announced Algolia Summer ‘19, enabling businesses of all sizes to deliver fast, tailored and personalized experiences across any channel, including Mobile.\nToday, as we unveil the General Availability of our latest mobile UI libraries, we’d like to double down on the importance of a great experience within your mobile apps.\nConsumers are all about apps\nIt is not breaking news that mobile is eating the world. Thanks to the wonderful pieces of technology our smartphones have become, consumers are empowered to do everyday tasks on the go, from communicating to checking news to banking to watching their favorite shows to shopping.\nHowever, one nuance is rarely broken out — mobile web vs. mobile applications. As we will cover below, there is actually no debate here — companies should invest in both. But, it is important to stress how important mobile apps have become in the journeys of consumers.\nStats show that 85% of consumers prefer mobile apps to mobile websites. Moreover, conversion rate on mobile apps is 21%, compared to 6% on mobile websites. Focusing on e-commerce, we see that mobile app users spend 20x more time shopping than website users.\nIt all comes from a combination of which audience is using a specific mobile application and what experiences mobile applications allow businesses to offer to their users.\nMobile apps are the key to loyalty\n\nEach of your digital properties bears a critical and important role in your user’s journey, and the user experience of each one of them… should be designed with its role in this journey in mind.\nTypically your desktop and mobile websites are where you will acquire new users, thanks to well crafted paid acquisition campaigns, SEO strategies, etc. This is where those users will discover your offers — products, services, videos, podcasts, etc. The experience on those properties should be optimized for conversion, in order to create a first meaningful connection with your users.\nIf this user experience was engaging and compelling,chances are your users will want to engage more with your company. This is where a state-of-the-art mobile application is critical:\n \n\nDirect path to your content: When your users will be looking for your specific content, offering them a mobile app allows them to directly immerse themselves in your offers with one tap from their home screen. They won't have to open their mobile web browser, remember your URL and type it, or search for your brand, with the risk of losing them to a competitor's ad.\n\n\nMore immersive experiences: mobile applications give you the opportunity to create smoother flows, use innovative designs, offer new types of interactions such as voice search, and allow your users to switch from one type of content to another in no time. They give you options that would be extremely difficult, if not impossible to achieve on the web.\n\n\nAdvanced personalization: mobile app users will most likely stay logged in their applications, and allow geolocation usage. It allows businesses to provide their mobile app users with deeply personalized, frictionless experiences, which will get them even more engaged. Additionally, local search is growing 50% faster than mobile search overall, which drives even more relevance and conversion.\nNotifications: With mobile apps, users can decide to stay informed of your latest deals, breaking news, new season of their favorite series … via notifications. It is an unmatched channel to keep your users engaged.\n\n \nThe world’s leading… brands already leverage these advantages to attract their users to their mobile apps, and build brand loyalty. Macy’s for instance, made their app a must have for shoppers, by offering app exclusives ranging from app-only deals to early access to enhanced in-store experiences.\nIn order to execute well on the promise of mobile, you need to nail all critical components of your application.\nRetain and delight your mobile app users with great in-app search and discovery\nAs we’ve seen, once a user is delighted with your application, they will be highly engaged with your brand. But getting someone to install your application is far from an easy task — it requires much more effort than just clicking on a link. So you will most likely spend significant resources to drive your audience to your mobile application.\nIf the experience on a mobile app is poor, users will simply uninstall it, and getting them to reinstall your app is more difficult than having them land on your website again. It implies a missed opportunity of course, as well as higher cost of customer acquisition!\nAs we mentioned in our introduction, consumers are used to and expect instant, frictionless experiences, especially when it comes to search and discovery. It is also true on mobile applications. And when they’re disappointed, they won’t hesitate to express their frustration.\n“Awful\nThis is the worst app. It doesn’t work without WiFi and whenever I search for anything it just freezes or tells me that ‘something went wrong’ ☹️ It would be great if these issues could be fixed. They’ve been going on for a long time despite updates.\nRating 1 / 5 - 3 months ago”\n \n“Terrible user interface!\nThe UFC app forced me to join ESPN+ to see the latest content & this app makes it impossible to search, navigate, and access the content I am paying for. Please find a new 3rd party app developer. If not, I'm done with trying to follow the UFC!\nRating 1 / 5 - 1 month ago”\nThe good news is,… users also recognize great experiences, and will be equally vocal about them.\n\"Excellent App!\nMacy's has truly done a superb job not only revamping this app to make it more navigable, efficient, and user-friendly, but also to create a more seamless search-result process when attempting to find specific products. Hands down great app!\nRating 5 / 5 - 1 month ago\"\n\"Great news resource\nI've come to trust CNET for my technology news for some time now. App has a great format & search features on past articles. Very happy with my CNET experience.\nRating 5 / 5 - 4 months ago\"\n \nBut building those great mobile application experiences is hard. So how can you exceed your users expectations?\nIntroducing Algolia’s new InstantSearch Mobile UI libraries\nSeveral years ago, we introduced ground breaking UI components libraries allowing companies to integrate Algolia’s advanced search and discovery capabilities into their mobile apps. While it was a great first step in the right direction, those libraries were not flexible enough to adapt to every use case, or to perfectly fit the look and feel of all ap UIs.\nAnd as we covered above, to make the most out of their potential, mobile applications should be perfect and deliver exceptional user experiences.\nThat’s why we’re announcing today a brand new version of our InstantSearch Android & iOS libraries, that overcomes the limitations of the previous one.\nThis version ships numerous new widgets, lets you fully customize the UI, and is easier to integrate into your projects.\nWe provide you various components by default, while you keep total control of the library thanks to its decoupled architecture.\nThis new version of InstantSearch focuses on solving the business challenges of search:\n\nFilter management and network resources optimization\nAutomatic updating of UI\nHandling of the interface's business logic\n\nThis allows you to focus on the look and feel of your application to achieve the user experience that will delight your… users, as InstantSearch takes care of the cumbersome tasks and of anything Algolia-specific.\n\nThanks to this new architecture, you can greatly reduce the amount of code necessary to create your search interfaces. To learn more about the technical capabilities of our new InstantSearch libraries, visit our widget showcase.\nGreat user experiences meet users where they are\nYou should now be equipped to build best-in-class mobile applications that delight your users and increase their loyalty to your brand. But again, mobile applications are only one part of the journey. If you want to stay ahead of the competition, you have no choice but to carefully design frictionless experiences for every channel.\nWe have more tips and best practices for your multichannel experiences. Watch our Algolia Summer ‘19 webinar to learn how to create amazing experiences across platforms. If you’d like to speak to a search specialist, click here. Meanwhile, we’d love to hear your mobile UX tips. Leave a comment or tweet to us."
  },
  {
    "id": "8990-0",
    "title": "10 tips to prepare your Magento storefront for the holiday season",
    "author": "Matthieu Blandineau",
    "text": "Consumers restlessly wait for it, merchants both await and dread it. The holiday season is approaching fast. It accounts for half of all annual sales, but also represents a huge traffic spike for online stores, and Magento stores are no exception. While the holiday season presents an enormous opportunity to reach consumers online, the smallest mistake -- including an outage or lag time -- can harm a retailer’s bottom line.\nTo make the most out of the season, merchants must consider all the stages a product (‘the gift’) passes through before landing with its recipient, including Search and Discovery.\nTo learn how to optimize the other parts of the journey, check out Magento’s 2019 Holiday Guide, “The Journey of a Gift” and discover all the opportunities to create delightful experiences for your customers—and ensure your eCommerce site stays firmly on the ‘Nice List’.\n1. Ready all the components of your Magento store to handle the charge.\nJust like any other day, shoppers will expect a Google or Amazon-like experience on your store. But more than any other day, the cost of failing that expectation is gigantic.\nYour competitors are spending just as much money and effort in acquiring customers as you. According to McKinsey & Company, a user will abandon bad sites for competitors after 2 to 3 seconds. This means that every additional delay on your website will lead visitors to your competitors and cost you revenue.\nTypical culprits that cause friction are page loading time, payment system availability–but did you know that your Magento search can also be a major cause of downtime? To give you a sense of the craziness during Black Friday 2018, some of our customers experienced 3,000 search queries per second for hours, if not days, with peaks over 10,000 queries per second, compared to 700 search queries per second on a normal day.\nWhether you handle your own Magento search infrastructure or trust a search provider, you should make sure your systems… are ready to handle such load increases.\n2. Keeping Inventory Up To Date\nWhile creating scarcity is a powerful strategy to accelerate the purchase decision, showing products that won’t be shipped to your shoppers before the end of the holiday season will cause users to bounce. It also wastes screen real estate for products that won’t generate any sales.\n\nSo make sure that wherever you display products on your Magento store, whether in your category pages, landing pages, or search results, you have the option to not expose out-of-stock items.\nConsidering the volume and frequency of transactions during the holiday season, it sometimes means millions of updates to your product database every day. Every system needs to pick up on these changes, and this requires a very large scale, agile system.\n3. Anticipate your shoppers’ wishes\nYour product offering is of course one of your greatest assets, so make sure it answers your customer’s needs. Use your Magento search analytics to discover what articles your shoppers are searching for ahead of the holiday season, and most importantly, what they are searching for but not finding. Use those insights to adapt your catalog and plan deals, or to improve your advertising and SEO strategy.\n\n4. Leverage your content\nYou generate valuable content about the products you sell, including blog posts and guides. This content can influence the purchasing decision of your shoppers, and position you as a knowledgeable seller, so don’t hesitate to surface it on your Magento store, alongside product pages — or even in your search results.\n\n5. Be proactive with your merchandising\nUnlike brick-and-mortar stores, you have the luxury to re-organize your Magento store front in one click, or even automatically. Use it! Shoppers will look for the best deals, so use the discount rate in the ranking of your search results. Create scarcity by ranking products with limited sales time first. You have a doorbuster offer, but still want to… preserve your margins? Rank this doorbuster first for relevant queries, and then add your post-discount margins in your ranking logic. The parameters you can play with are endless. The important thing is to be able to configure the ranking logic of your Magento search according to what matters to your shoppers, and you.\n\n6. Optimize your Magento category pages\nLooking at your analytics prior to the holiday season, you probably uncovered categories of products, and even specific products within those categories, having growing performances. Leverage those insights to optimize your category pages for those trends. Make sure your shoppers will find the products that fit their needs -and is the most interesting for you to sell- first when they navigate your Magento store through category pages.\n\n7. Create new landing pages to acquire more traffic\nDuring the holiday season, competition for traffic is at its climax, so any opportunity to acquire new traffic and better convert the traffic landing on your store is to explore. One of those opportunities is to create dedicated landing pages combining relevant products from your catalog with short editorial content to put behind new paid keywords, paid ads or an email campaign, or to better appeal to and convert shoppers already landing on your store via your existing acquisition channels.\n\n8. Adapt to the unexpected\nWhether you like it or not, you will miss some trends and discover high selling offers that you wish you had more promoted. It’s not too late! Tweak your search results ranking logic to promote your best selling products, and capture even more sales.\n\n9. Analyze!\nThrough your site search, your shoppers literally speak to you. Leverage those invaluable insights! What were they looking for that you did not offer? What search results converted the most? How did people navigate your store?  Did they filter by a specific brand when looking for TVs? All this data can help you improve your offering and the way you… surface it to your shoppers next year.\n10. Turn first time shoppers into engaged customers\nThroughout the Holiday season, you will acquire new shoppers, thanks to your unmatched offers on specific products, and increased advertising efforts. Chances are, if they like the overall experience, they’ll return to your store. So when things calm down, reflect on what went well and what did not, and start planning long-term investments for your user experience. Studies report that every $1 invested in UX yields a $2 to $100 return.\nWe have more tips and best practices for your Magento search and discovery - if you'd like to speak to a search specialist, click here. Meanwhile, we'd love to hear your tips for holiday site search. Leave a comment or tweet to us.\nSearch and discovery is only one part of the journey. Read Magento’s 2019 Holiday Guide - The Journey of a Gift to discover the best solutions to optimize Magento store’s payment processing, shipping, customer service modules and many more.\n "
  },
  {
    "id": "8977-0",
    "title": "Trading up: Implementing Algolia search to improve the trading card experience ",
    "author": "Eiji Shinohara",
    "text": "Are you familiar with trading card games (TCG) like Yu-Gi-Oh!? The Yu-Gi-Oh! Trading Card Game is a Japanese collectible card game based on the Duel Monsters card game popularized from the manga franchise, Yu-Gi-Oh!. The goal of this trading card game is to gather multiple cards of the fictitious monsters and create powerful decks to challenge (and win against) other players’ cards.\nWith over 25 billion Yi-Gi-Oh! trading cards sold, the Yu-Gi-Oh! Trading Card Game is one of the most popular trading card games and has a strong following all around the world. Based on its origin, it is especially strong in Japan where players actively seek to trade cards with one another to improve their deck collection both in person and online.\nTo help players find cards and trade with one another online, ka-nabell is one of the most popular and long-established Yu-Gi-Oh! Trading Cards marketplaces in Japan. To deliver a great experience to those players actively in search of a prized card to add to their deck, ka-nabell knew it had to deliver an extremely fast search and discovery experience that allows for granular filtering functions that return highly relevant results so that players can find exactly what they want, quickly. Before introducing Algolia search, ka-nabell heavily relied on SQL queries. After implementing Algolia, ka-nabell is improving the customer experience.\nWe had an interview with ka-nabell engineers Hirotaka Koga and Toshiharu Nishiwaki who shared their experience with us.\nCourtesy of ka-nabell engineers, Hirotaka Koga and Toshiharu Nishiwak\nDeveloping a better search experience\nHirotaka and Toshiharu said that ka-nabell was looking for a faster search solution. Then, they came across a site called SEARCHSTONE. \n\nWith its ease of use to buy multiple cards with granular filtering such as Price, Category, Effects, and so on, they thought this search capability on Searchstone (powered by Algolia) is ideal for creating Trading Cards decks. They also were… encouraged by the value they could get through conversion rate improvement as seen on Algolia’s website (E-commerce search solutions to accelerate conversions).\nIn addition, Hirotaka and Toshiharu  wanted to minimize the front-end development tasks using Algolia. Vue InstantSearch was a fit for them to build a faster and smoother user interface (UI). Hirotaka said that he “enjoyed coding to build Horizontally-Scrollable UI with Vue InstantSearch”. To insert and update data, they added some codes into existing system built by PHP. For batching, they wrote a Node.js code from scratch. \nMeeting the Language Challenge\nThe Japanese language has three alphabets called Hiragana, Katakana, and Kanji. To enable search in any and all of the possible alphabets (e.g., searching Katakana words with Hiragana), they added fields and converted characters before indexing. They also tailored the search experience by making searchable commonly used, special characters specific to gaming and trading cards such as ☆ and Y18. \nTurning Great Search into Great Results\nAfter implementing Algolia for their project in less than one month, there already has been positive feedback from customers with comments like \n\n“Ka-nabell search is super useful.” \n“Real-Time! How much did they spend to build this?” \n“High quality search. Typo is fine.” \n“Super fast and useful. Totally different from before.”\n\nIn terms of improved conversion rate, Hirotaka and Toshiharu are going to track this over time with Algolia’s Analytics features.\nThe best is yet to come. Since this project was successful, they decided to introduce Algolia to their brand new service. It will be launched in September.\nFuture Objectives\nMoving forward, ka-nabell has more planned with its Algolia implementation. Because of Algolia’s Search-as-a-Service platform, ka-nabell will be able to leverage its existing Algolia implementation and agilely take advantage of Algolia’s other capabilities… including:\nusing Algolia’s Advanced Analytic to identify opportunities to optimize the search experience for users and improve conversion rates.\nExpand beyond only Trading Cards search to other businesses they run. \n "
  },
  {
    "id": "8965-0",
    "title": "Algolia Honored to Be Named Again to the Inc. 5000",
    "author": "Nicolas Dessaigne",
    "text": "Today, we are humbled to have Algolia (#448) be named to the Inc 5000 fastest-growing private companies list for the second consecutive year. This recognition is a meaningful milestone to reflect on the three components that I believe has made this possible.\nFirst, this is really a testament to the modern consumer and a sign of our times. Today, consumers have all the “power” and are calling the shots as they search for and buy more products online; as they consume more content via their mobile devices than ever before; as they interact with their voice-activated devices to find the information they need right now.\nToday and in the future, it’s these “Now Moments” that determine if a customer stays on a site to engage and buy vs. moving on and going to her default — Google or Amazon.com. It’s this consumer-driven trend that is driving more and more businesses to step-up to meet this expectation head on.\nAs seen from the Algolia Summer ‘19 product launch, it’s what inspires Algolia to continue to innovate on behalf of our customers to deliver on Now Moments through our search-as-a service platform to deliver an amazing search and discovery experience across any device, anywhere.\nSecond, this is a tribute to our 7,500 customers worldwide.  The consumer expectations for “Now moments” is what has led many companies like Birchbox, Twitch, Lacoste, WW and National Geographic to Algolia to deliver instant, frictionless digital experiences that provide the exact information or products consumers are looking for. \nWithout you, this would not have been possible. On behalf of all of us at Algolia, a heartfelt ‘thank you.’\nFinally, the Inc 5000 recognition is a tribute to our 350 employees who, even as the company grows, continue to work together as one team to innovate and deliver great results — whether from our SF, Paris, London, Tokyo, Atlanta, or New York office.\nARTICLE CARD\nThis type of growth year-over-year is not easy. To scale and grow at… any stage of a company, it requires actively trusting and caring for one another, as well as having a high degree of candor, grit and humility along the way. This is the Algolia culture and continues to keep us thinking globally, acting locally to maximize our potential as we scale and grow to meet the new challenges and opportunities ahead.\nToday is a great milestone, and I’m confident that the best is yet to come!"
  },
  {
    "id": "8833-0",
    "title": "Using NetInfo API to Improve Algolia’s JavaScript Client",
    "author": "Jonas Badalic",
    "text": "Algolia’s architecture is heavily redundant, hosting every application on at least three different servers (called clusters). We do this to ensure the greatest possible reliability. This underpins our SLA, ensuring a 99.99(9)% uptime, allowing you to reliably ship enterprise-grade search without having to worry about the underlying infrastructure.\nRedundancy is, however, only one part of a high-availability system; there is still a large unknown between our users and our infrastructure–the  network. All search queries rely on network availability and are subject to its underlying latency. To manage network fallibility, Algolia’s API Clients implement a “retry strategy”, where a server is determined to be “unavailable” based on a set of simple heuristics which, when triggered, cause the API Client to retry the same query by bypassing the main application DNS record and targeting one of the server nodes directly.\nThis retry logic switches the targeted server whenever it detects that one of them is down or unreachable. Therefore, a given request will not fail unless all servers are down or unreachable at the same time. Which is unlikely - especially given that a cluster’s servers are in different data centers which are separated geographically.\nFor this purpose, we need to decide what “unavailable” means (a topic that could probably deserve its own blog post). For simplicity’s sake, a server node is deemed unavailable if the request to retrieve results takes more than X amount of time. For each time a request would take more than X amount of time, we would increment Y and start a new request, this time targeting a specific server node. We defined X to be a starting value of 1000ms multiplied by Y, where Y was the number of times a request has timed out in the current session. This gave us a basic retry strategy that worked but was not adjusted for different network conditions.\nAdjusting for a variable network environment\nThere is sadly no such… thing as a perfect nor reliable network and no two networks or providers are the same. Enter an old concrete building or an event where the local network is overloaded by peers and you will quickly understand how unreliable Wi-Fi or 4G can get. As a SaaS business, we cannot guarantee the connection of our users, but we do however want to provide a good search experience for all of them, irrespective of what their network capabilities are.\nThis is where our retry strategy definition overlaps with our business goals and poses a potential risk. Because we use a default static number (1000ms) for our timeout logic, we are not catering to the real network conditions that our customers are subjected to. Users on slow connections will trigger our retry strategy when they do their first query to algolia servers because our client timeout defaults are too low; this, in turn, causes their original search requests to be canceled and another request to be sent to Algolia–doubling the time they need to receive the request.\nIf we could know the network capabilities of these users, we could set our timeouts high enough to prevent false positives while still preserving the retry logic for when a server request might fail.\nMotivations for improvements\nDelivering fast(er) queries\nIf you are an e-commerce, media, or any other kind of online business, customer retention and satisfaction are key. In a world where nobody likes waiting, the last thing you want is to have your users stare at a blank screen waiting for the results of a search query. Amazon found that a 100ms delay causes about a 1% drop in revenue and that loading 30 results instead of 10 delayed the page rendering by .5 seconds and caused a 20% drop in traffic. \nCapitalizing on a fast search experience could be one of the key differentiators between you and your competitors, and it just might be the reason why users end up using your website rather than a competitor’s one.\nShifting the blame\nSlow server response… time, however, is not necessarily an engineering fault. As software engineers, we can do a lot to improve how our content is delivered, but because we don’t entirely control the network stack, our users are still left with a best-effort service, which often suffers from the last mile problem.\nTake, for example, a user who is trying to access your website from a poor network. If you tell this user “Your results are loading”, they might be frustrated by your service and leave your site–even though it is their network and not your service which is the cause of the slowdown.\nEnhancing the experience by saying “You are on a poor network, the results might take a while” shifts the blame to their provider, making them more likely to wait for the actual results to arrive, knowing that your website is not to be blamed. (Facebook study about the psychological effects of loaders.)\nGoogle Maps example (center of Paris on iPhone XS, Free provider)\n\n\nUsing NetInfo API\nThe Network Information API is currently a living draft document under WICG. It holds important information about the network capabilities of the client such as its effective connection type, downlink, and round-trip time information. If we were blindly initializing the timeout times to an arbitrary number before, we can now make better-educated adjustments and initialize the client with a better timeout configuration.\nNetwork Information API also ships with a very useful onchange callback that is invoked when the connection of our user changes. This means we can now cater for users that start using our page on a good connection but later experience a slow connection due to external conditions (such as the concrete building example above). Or vice versa. \nBy leveraging the NetInfo API, we can start adjusting for some use cases that we could not handle before. Let's look at some examples below:\nSearch query under perfect network conditions (4G, Wi-Fi):\n\nQuery was sent under good network conditions, no… timeout was triggered, and it took a total of 600ms for the request to travel over the network. This includes resolving DNS, negotiating TLS, and establishing a TCP connection.\nSearch query under slow network conditions (3G):\nQuery was sent under degraded network conditions, which in turn exceeded the static timeouts we initially set (1000ms) and a query was retried. We can quickly see how not knowing the network that the client is subject to has cost us an extra request which increased the total time needed to retrieve a response (in this case 1000ms + 1500ms instead of only 1500ms).\nSearch query under under terrible network conditions (slow 2G):\n\nSearch query is sent under terrible network conditions, each request is retried with an exponentially increasing timeout, ultimately retrieving a response.\nSearch query under changing network conditions:\nSearch query is sent under good network conditions, which might change during the session.\nNetwork-adjusted search query:  \nBy leveraging the NetInfo API, we can adjust the timeout settings of the client and appropriately set timeouts based on actual network conditions that our user is experiencing, thus reducing the number of retried requests and decreasing the total time it takes for our search engine to respond with results.\nImplementing our solution\nKnowing the use cases, API’s, and our desired outcome, we can now leverage the NetInfo API to adjust the timeout strategy of our clients based on real network conditions.\nBecause Network Information is an experimental API, we need to check for feature support. Once we assert the availability of the API, we can then initialize the timeouts of our JavaScript client from the network RTT times and register the onchange handler.\n(Important: You should also check for connection.rtt being a number, because of partial support in some cases. Note that pseudo-code above is not meant to be copy-pasted.)\nIf NetInfo API is not available, we can use the Resource Timing API. As… pointed out by Ilya on twitter, it contains enough information for us to build a better-informed retry strategy without having the NetInfo API available.\nUsing the two API’s we are now closer to having a dynamic retry strategy based on real-world network conditions. See below example of how hn.algolia.com has been updated to resemble real network conditions.\n\nWe are now able to set better timeout defaults and avoid the retry strategy from being triggered due to bad defaults; original requests are no longer cancelled and retried when users are subject to bad network conditions. This, in turn reduces the amount of time it takes for those users to get their search results, saves device bandwidth, and saves CPU on our search server (because only a single request is sent, the engine doesn’t have to do the work twice).\nHere is a side-by-side rendering performance comparison of our old (left) vs our new retry strategy (right) under throttled 3G network conditions.\nLeft: Static timeout value. Right: network adjusted timeout value.\nIf you look at the Chrome developer tools on the left, you can see that the original request times out and is retried by our client whereas, on the right, the timeout is set depending on actual network conditions. With this modification, we managed to save about ~800ms under current simulation.\nMaking sure we have data\nBefore doing all these changes, we setup a monitoring API so that we could measure our changes. This API reported each timeout occurrence as well as client and network information. We have added “slow-connection” events to GoogleAnalytics for every time a user would see the \"You are on a slow connection\" loader. This allows segmentation of user sessions into slow and fast connections.\n\nThough Algolia provides one of the fastest search engines, thanks in part to its distributed search network, which places the data closer to your users, we can still observe that about ~9% of this application’s users are experiencing slow… times from querying to rendering results due to a slow network.\n\nUsing Google Analytics, we can now compare users on slow connections and see that they are spending almost double the time on the page. Because search is a primary function of hn.algolia.com, we can imagine that users spend most of their time waiting for results. \nImproving the developer and user experience\nKnowing a response will be slow, we can start by enhancing the user experience by showing a loader and explaining where the delay is coming from. This allows us to better manage user expectations as well as explain why a certain action is delaying the experience, avoiding frustration, and hopefully resulting in increased time that our users are willing to spend waiting.\nLet’s look at the following example:\n\nBy adding a built-in method, we would give developers a quick way to decide how to handle these cases, helping them focus on the actual implementation of their search UI and keeping their users happier even under degraded network conditions. This puts us one step closer towards helping you build the best search experience for your users.\n \nExample of hn.algolia.com handling users on slow connections.\nGoing beyond the slow network loader\nHaving a slow network indicator is a good first step, but we can now go beyond explaining the situation to actually speeding up the queries for these users. One of the options that we can now consider is to reduce the number of search results that we are requesting, decreasing the total response size and getting those results on the screen faster for our users. Depending on the transfer size of your search results, this can have a significant impact on how fast your search results are displayed. \nTested on algolia.com/doc, same query with different hitsPerPage parameter. Response size is ~3x lower and the query only takes 10% of it's original time.\nAnother option that we can now consider is debouncing queries for users on slow connections. If we know the… requests are going to be slow, we can save bandwidth and not send queries at every keystroke, thus saving device bandwidth for other critical operations that it might need it for. We can also do this if the navigator.connection.saveData is true, an attribute which indicates if the user has requested a reduced data usage mode.\nWhat this means for us at Algolia and our users\nAdjusting for these situations provides a better search experience for all of our users, irrespective of their location, network, or device capabilities, and brings us a step closer to providing a better search experience for everyone. NetInfo API also opens up opportunities for developers to test and decide on how to best handle these use cases. As we continue to work on improving search for both you and your users, we are looking for feedback and early adopters. If you are interested in this, please reach out in the comments below, we would love to hear your thoughts.\nBig thanks to Adam Surak for joining the efforts on this topic!\nBonus link: There is a similar spec called Client hints, which expresses network information via a new set of request headers. I recommend you read the spec draft itself or read the blog post from Google Developers to see how you can leverage these headers."
  },
  {
    "id": "8781-0",
    "title": "DX Matters - Creating Great Search for Documentation",
    "author": "Sylvain Pace",
    "text": "Developer eXperience, known as DX, is key to building great software, as it helps your developer-users leverage the best out of your software.\nPart of good DX is searching through documentation, not only as a means to finding information, but also as a tool to structure technical content. In this article we show you a how a simple project – Algolia’s free DocSearch platform – has come to solve a common pain point in the open source community: that of searching and structuring documentation. We show the positive impact of DocSearch as well as the cost in developing it.\nWe also focus on the value of search as compared to the widely used Ctrl+F. \nFinally, we present some ideas we have about how to make DocSearch even more helpful.\nMaking Documentation Easy for Developers\nThe developer community is growing fast. Their impact is broader and stronger. We wanted to use our search technology to help developers do their magic and make this world a better place. It was time to focus on a proper DX. \nThat’s where Algolia and its DocSearch tool comes in: making search come alive with a learn-as-you-type experience.\nSearch solves a regular issue: every developer needs to use documentation whenever they need to learn or use new technology. Search brings love to a project. But most projects have limited resources, or they lack the knowledge to create a good search experience for their documentation. They focus on building an amazing tool. DocSearch makes their documentation come alive – for free and out of the box. \nDocSearch has been warmly welcomed in the community. It proudly helps 2.3 millions unique users a month.It is self-serve and customizable, a complete out-of-the-box experience. It is adaptable to any software, like Stripe, React, and jQuery. Instead of guessing or leafing through pages, your users will leaf through instant search results. It also captures analytics that are key to understanding what your users do and need. You are now data driven to… produce great documentation. \nBut DocSearch also raises the bar of modern documentation, it standardizes a well-structured approach to documentation. It does this with its crawling mechanism, which its uses to add search to your website. Crawling documentation requires content to follow a certain structure. \nWe also improved DocSearch over time. In searching and structuring content, we encountered some fundamental and eternal developer-issues. The biggest one being to choose between space or tabs to indent a page. This was the only negative feedback we had when we integrated DocSearch to jQuery. But we solved this issue, thank to a simple PR:\n\nWhy is Ctrl-F Not Enough?\nWhy go that far with DocSearch when people can just as easily use Ctrl-F? There are many reasons for this:\n\nYou'll search the entire documentation site instead of only one downloaded page.\nLearn-as-you-type experience. Instant search results and a categorized dropdown box enable users to not only search but browse, discover, and learn. We have noticed that people rephrase, correct, and erase their initial search due to what they learned during the search.\nSpeed. In moving from page to page and quickly finding what you need. Millisecond matters.\nTypo tolerance. Our search corrects wrong queries and mistypes. This is key when you search for unknown concepts or technology\nTie-breaking algorithm. This unique concept of Algolia helps the maintainer better redirect their users, helping their users in an interactive way. Less need for support!\nAnalytics. Our search helps you understand what your users are doing. \n\nLet’s See How it Works\nWe crawl our 1600+ DocSearch websites every 24H to keep the search synchronized with their content. This process requires a strong and secure infrastructure. We use an infrastructure that scales since we are only at the beginning of the road. This where Kubernetes steps in. Kubernetes (K8s) is an open-source system for automating deployment, scaling, and managing… containerized applications. It is now the basis of DocSearch's daily crawls of over 1600+ documentation websites. If you want more details regarding the architecture chosen, please ping us 🙂\nAll this comes with a cost. For sure, this project requires a strong and reliable codebase. It all started during a vacation/off site in Porto, Portugal. This is a classic story that we will explain in another blog. Like every project, time is key. We have one main motto on our team: automation! We've even built a complete tool that gathers public data about projects in order to provide the best customized help. This too makes for a good blog, which is on its way. To automate properly, we needed to refactor every piece of our tooling. This point is key and helped us ship out our search experience quicker, stronger, and further. \nDX matters even in how we deliver it. We help every compliant project that reaches out to us. However, we’ve noticed that the best way to help big projects is to reach out ourselves. We are hiring to help us with that. Future users of DocSearch only need to click on the “merge” button to see the learn-as-you-type experience alive and in action.\nNext Steps for DX\nThe stable state of our project encourages us to go even further. We always want to show the huge love we have for the community. The road will be long and amazing. We are looking forward to it and we have defined some future additions to DocSearch.\n\nBetter analytics.\nA new version (v3) of the UI.\nReal-time: only crawl website when triggered (that is, when content has changed).\nDashboard: Let our users create their own configuration thanks to a UI dashboard.\nImprove and make our DocSearch Hub public.\n\nHappy coding and writing great documentation!"
  },
  {
    "id": "8751-0",
    "title": "Built for The Now Economy — Algolia Summer ‘19 ",
    "author": "Julien Lemoine",
    "text": "Consumers today want instant, frictionless Google- and Amazon-like experiences when looking for information, media content and products. They demand intuitive experiences that deliver exactly what they are looking for — regardless of the digital interface — and they want it Now. Today, this is what determines whether a customer engages, subscribes, and/or buys from a business.\nAt Algolia, we call this the “Now Economy.”\nWith over 3.8 billion global internet users constantly searching for anything and everything online, in the Now Economy, businesses must meet this challenge and take advantage of each moment of digital interaction — the “Now Moments”. In these Now Moments, businesses must provide engaging, personalized experiences that connect a consumer’s intent with the best, most relevant content, regardless of the digital interface.\nThe need to deliver on Now Moments anywhere and all the time is what drives Algolia’s product and engineering team to continually innovate on our customers’ behalf. Today, I’m excited to share with you that it’s what has driven us to deliver Algolia Summer’ 19. \nAlgolia Summer ‘19 is all about delivering fast, tailored and personalized experiences across any channel. With this latest release, developers and businesses of any size can deliver search and discovery “Now Moments” across web, mobile and voice channels through the following innovations and enhancements.\n\nPersonalizing Experiences for Now Moments\n\nWith Algolia Summer ‘19, business users can quickly create personalization rules based on ingested customer preference data such as click data and purchase data to tailor search results for customers. Through Algolia’s new personalization simulator tool, users are able to tune the weighting of personalization attributes and then visualize the end user experience in real time to ensure the intended results are delivered.\nIn addition, developers can build a unified personalization experience… across search, browse, and email marketing through integration with personalization solutions including Dynamic Yield's AI-powered Personalization Anywhere™ platform, Target2Sell's AI Platform for 1to1 Commerce, and Raptor's Smart Advisor. Developers can also leverage Algolia’s new User Profile API to utilize Algolia-generated user profiles in third party solutions.\n\nPioneering Intelligent Voice Search\n\nWith consumers increasingly adopting voice-activated devices like Alexa and Google Home, along with using their voice to quickly seek information on their mobile device, Algolia Summer ‘19 has further enhanced its voice search capabilities to help businesses meet this growing trend and respond to user requests with the most relevant results.\nWith Algolia Summer ‘19, through improved entity matching, developers can easily build voice search experiences that account for differences in the ways people speak and vocalize what they are looking for to deliver more accurate search results. In addition, new pre-built JavaScript Speech-to-Text integrations allow developers to embed voice search into the browser in minutes. This works on the desktop, iOS and Android. \n\nPowering “On-the-Go” Search\n\nWith the proliferation of mobile devices for “on-the-go, always on” consumers, delivering on Now Moments through simple, intuitive mobile search is critical. With new pre-built mobile UI libraries, Algolia Summer ‘19 provides developers increased flexibility to control the mobile user experience, implement mobile-optimized menus and filtering, and integrate the branding of their search user interface for mobile websites and apps. \nDevelopers also now can leverage Algolia’s search API in 14 different development environments, as well as a new API client for Kotlin, the new language of choice for Android that provides another option for developing mobile experiences. \n\nPerforming Even Better\n\nFinally, to deliver on Now Moments, personalized results and… experiences must be returned with lightning fast speed. With Algolia Summer ‘19, Algolia continues to push the boundaries by improving its discovery experience by up to 30% using filtered searches. Algolia has also reduced indexing time by up to 10% allowing companies to shorten the time between new content going live and its appearance in search results.\n\nMany Thanks and More To Come\n\nAlgolia was purpose-built as a modern “search-as-a-service platform” to give companies of all sizes the best of both worlds with a pre-packaged, high performing search and delivery platform that they can quickly and cost effectively customize to their very specific business and user needs. Algolia Summer ‘19 is another step in advancing our mission.\nThank you to our customers who provide valuable, ongoing feedback directly, as well as through developer forums and our customer success team. It is your ongoing partnership and trust in us to listen and act that drives the innovation of our 120+ engineers worldwide. To learn more about Algolia’s Summer ‘19 release, please check out our on-demand Algolia Summer ‘19 webinar at www.algolia.com/webinar/summer-19, reach out to your Algolia Customer Success manager, or visit www.algolia.com/products/whats-new/.\nFinally, thank you to our product management and development teams. It is through your active, ongoing engagement and long hours in product sprints, testing and customer feedback sessions that has enabled Algolia to be the chosen platform by 7,500+ customers to continuously deliver on Now Moments for consumers globally across 70+ languages. Onward to our next milestone!\n "
  },
  {
    "id": "8743-0",
    "title": "Enterprise Tech is Alive, Well, and Thriving",
    "author": "Nicolas Dessaigne",
    "text": "Technology innovation continues to be a leading driver of growth for the global economy. It’s this innovation that impacts consumer experiences and business productivity. While there are all types of innovation happening all over the world, it’s understandable that the majority of news headlines related to technology companies are of those in the B2C category. From Google, to Pinterest, to Airbnb, to Uber and Lyft, these are companies having a direct impact to the lives of everyday consumers— from search, to social media, to home- and ride-sharing. \nOften less covered are those enterprise B2B technology companies that enable other businesses to power engaging online customer experiences or increase workplace productivity. \nToday, we are honored to have Algolia be named to the Enterprise Tech 30, a new report that identifies and recognizes the most promising private companies in enterprise technology through a poll of 73 investors from 55 venture capital firms that collectively “have funded 95 of the 115 enterprise tech unicorns, including financings and exits, since January 1, 2016.”\nThe Enterprise Tech 30 is the brainchild of Rajeev Chand and Peter Wagner at Wing Venture Capital that was borne from a new way to determine a list of promising enterprise technology companies based on what venture capital investors themselves are seeing. Similar to the Coaches Polls in college football, the Enterprise Tech 30 surveys experienced, in-the-know technology investors on which companies are the leading enterprise tech startups. \nDeveloped over the past 6+ months, Rajeev, Peter and the team at Wing (including Brendan Baker and Lizzy Labeeuw-Anderson) took this idea and made it a reality by developing the guiding principles (be institutional, exhaustive, collaborative, and neutral), refining the approach, doing the research of which investors to poll, building the universe of 3,600 private companies, and actually conducting the survey and analysis to finally… producing the inaugural Enterprise Tech 30.\nIn the mid-stage category, Algolia is both humbled and proud to be recognized by the investor community, as well as be associated with other disruptive and innovative growth stage  technology companies\nCongratulations to all the companies in the inaugural Enterprise Tech 30. With companies represented across categories including SaaS, data and analytics infrastructure, automation and modern software development, it’s clear that enterprise technology is alive, well, and well-represented with the best still yet to come.\nTo see a copy of the full report, go here."
  },
  {
    "id": "8699-0",
    "title": "A Journey Into SRE",
    "author": "Sergio Galvan",
    "text": "Site Reliability Engineers (SREs) design and manage efficient processes and operations, and they keep a company’s infrastructure in healthy working order. \nHere at Algolia, our team has grown from 4 to 10 in less than two years, a growth rate similar to the company as a whole. The team has grown not only in number but in how we work together and create sound operational processes. \nThis blog is about how a group of hard-working individuals, with unique skills and working methods, managed to create a successful SRE team.\nMy own journey into the SRE field reflects this maturation process. Before I joined Algolia, I was traveling around the world as an Integration Engineer for a telco company. On more than one occasion, I found the opportunity to build small tools that helped me be more productive. This increased my confidence to go further professionally. That’s when I started digging into what an SRE is. Here’s what Google VP Ben Trayner says about SREs:\nAn SRE is what happens when a software engineer is tasked with what used to be called operations.\nAlthough I wasn’t technically a software engineer, my career followed this same pattern: writing code to manage operational tasks. I was therefore excited by the SRE role and ready for a change.\nWhat SREs do at Algolia\nEvery member of the SRE team gets involved in all three of these activities:\n\nProjects\nOperations\nOn call\n\nProjects: We work on different kinds of projects. Some of them have a direct impact on the business and others help improve the global infrastructure. \nOperations: The majority of our infrastructure is bare metal, which means that we require a decent amount of automation and work on installing/destroying/repairing servers. We need to debug live applications and support other teams searching for technical advice. On top of that, we have to contact providers to manage any issue with the providers.\nOn call: We need to provide support for the whole infrastructure. This means that each one of… us has to be on call 1 week every 4 weeks, 24/7.\nMeetings\nEvery Monday we have a one-hour meeting where we review the previous week and plan the current one. We talk about:\n\nOn-call issues\nOperations\nProject statuses \nPersonal objectives and initiatives\n\nOperations\nOperations at Algolia has changed over time. For instance, when I joined, operations were performed on a daily basis, which made it difficult to gain a deep context about what tasks had been completed.  Our golden rule regarding priority is \n\nResponding to customers first\nResponding internally to questions on Slack\nSolving incidents\nProvisioning infrastructure \n\nOperations are now performed on a weekly basis. You can see the rotation plan below. On top of this, we have two levels of on call. In the end, on call should not really be different from the normal operations we do. Overall, operations is more than maintaining and improving our infrastructure.\n\nHow we work as a team\nCommunications   \nIn my first days at Algolia, I noticed right away that my colleagues were communicating mainly through Slack - even if they were just a few meters from each other. This felt a bit cold, especially given my natural inclination to get up and talk to people. Additionally, the team was scattered across the office. It didn’t feel like a cohesive unit. For these reasons, communication was difficult. \nInterestingly enough, I wasn’t the only one experiencing this. Newcomers arrived and noticed the same issues. \nNewcomers\nCritical mass probably pushed us in the right direction: there were just too many tasks to continue functioning as we did before. Our first step was to set expectations for every member, so that everyone would know in advance what they can get from each other.\nNew people bring in unexpected benefits and qualities to the team that the older members wouldn’t have expected. Additionally, if they are open to it, older members can actually learn from the incoming ones and have these new… people/qualities change the team for the better. The right mix of old and new is what makes a team great. \nOne initiative we took was to create a coffee break culture, two days a week. During these breaks, we speak about different things, work or not work related. We get to know each other and communicate better.\nPairing creates a team\nInitially each member of the squad worked individually on their given projects, choosing for themselves the subject they wanted to work on. This kind of autonomy and personal initiative wasn’t all that bad, but for a newcomer it was overwhelming; it forced me to switch gears often: to learn and do my daily support and come up with project ideas and complete them, all on my own. \nBecause we were growing, we quickly realized that we needed to change this, we needed to start working together. The first step was to do some pairing, working on teams of two people. This gave us an opportunity to interact and get to know other members on the team. \nThree Projects\n1 - Reverse engineering a Vault/Consul server\nOn this project, Paul and I worked on finding out how the current deployment worked, how we can recover in a disaster scenario, and how much time the recovery would take.   \nThe project took 2 weeks. Once we were done, we had the choice to change partners or continue working together. At the end of these two weeks we had: \n\nDeep understanding of the project\nAutomated deployment\nReplicate data\n\n2 - Load Balancer knowledge sharing\nThe next two weeks I continued to work with Paul on getting more insights on the new Load Balancer. If you have not read his blog post on one year of load balancing, I suggest you do it.\nThe main problem we found was that during operations or on call, any request regarding the load balancer had to be forward to Paul. This knowledge sharing has two benefits: first, it provides insights to more than one person; secondly, it offloads tasks from the person having all the knowledge.\nRecently, a new member… joined Algolia and started working on the load balancer full-time. Dedicated support on the load balancer not only provides the new member with more operational skills, but also gives the team more people to consult.\n3 - Backup solution\nIn this project, three members of the Foundation squad worked together to bring up a new backup solution. The interesting part here is that we decided to start using Scrum methodology. This was a huge success as it allowed us to:\n\nDefine small tasks inside more complex ones, with the benefit that all three of us would work on it at some point.\nEstablish a time estimation, which helped us foresee how further in the project we were.\nAuto-assign tasks. This was the best part for me, being able to start working on something else once I was done with what I had to do.  \nHave greater visibility on the project due to the fact that we all work on almost all parts.\n\nThe journey continues ..\nOur efforts have worked so well, and our team has become so stable, that I’ve taken a fresh look at the original “problems” I encountered when I started: the overuse of Slack and a scattered team not sitting together. Today, this is not a problem: we use Slack constantly, and yet the conversation feels natural and direct. And I can sit on a different floor or desk every day and easily collaborate with my team – because I know how the people on my team work. This is what makes for a great SRE experience: effective communication, efficient processes."
  },
  {
    "id": "8696-0",
    "title": "Sweeping Away Our Environmental Footprint",
    "author": "Peter Villani",
    "text": "Companies throughout the world - especially tech startups - are taking their ecological footprints seriously and making conscious efforts to reduce them. Algolia has followed suit by adopting some critical eco-friendly measures such as carbon offsetting/compensation, and choosing clean energy data centers. These are important steps towards protecting the environment. \nAchieving Carbon Neutrality\nWe’ll start with what we ourselves can do. Every tech company has the power to compensate for its carbon emissions by reducing the greenhouse gases their technology generates. One way to do this is by planting trees and restoring our forests, which in turn reduces greenhouse gases at the same rate as they are created. This compensation process cancels out - or offsets - carbon emissions, thus achieving carbon neutrality.\nCarbon neutrality is essential to any eco-friendly policy. It’s a form of giving back. Data centers used by cloud companies emit 2% of all greenhouse gases in the world. \nWith the help of a company specialized in ensuring carbon neutrality, Pachama, we've been able to assess our carbon footprint and take steps to sweep it away. \nOur ultimate intention is to help scale up the protection and restoration of the forests of the planet, which recapture carbon from the atmosphere, reversing climate change. \n- Pachama\nWe chose Pachama not only for its thorough approach to assessing carbon usage, but for its innovative technology: they use deep learning models and data-intensive algorithms to assess the exact carbon footprint of a company. But more significantly, these algorithms rely on satellites and drones to ensure that the data used is 100% reliable. \nWe source high-definition, high-frequency satellite images to estimate carbon storage in forests and predict future deforestation or potential for forest restoration.\n- Pachama\nThanks to Pachama’s careful analysis, we're assured that our claim of carbon neutrality is real and accurate.\nUsing Clean Energy… in Our Data Centers\nWhile carbon neutrality is important, choosing energy that does not produce greenhouse gases in the first place is even better. The tech industry needs to find ways to reduce their reliance on fossil fuels and other forms of “dirty” energies. We can’t do this alone, as many of us do not own the data centers we use. Instead, we rely on the eco-friendly efforts by our data center providers. \nThe trend for data center owners has been to switch to renewable energy, relying on such “clean” power as sunlight, wind, rain, tides, waves, and geothermal heat. The data centers that Algolia uses, located throughout the world, are making substantial progress in reducing their use of fossil fuels. Many of our most important centers have published their ecological footprint, such as Google, Microsoft Azure, and Amazon. The numbers are impressive: some are at 100% renewable energy. Clean energy is a trend that Algolia has jumped at to help sweep away its footprint.\nCustomers and Employees Are Asking Us to Give Back\nCarbon neutrality and clean energy are just two ways for global businesses to give back. Prospects and customers are beginning to ask questions about these and other such efforts. Employees and potential employees are equally interested in our global care. They ask about how we give back to the community. Caring about the environment is only one way. There are others – financing open source, matching charity donations, setting up a $1000 referral program, signing a founder’s pledge. We feel that giving back through pro-ecological and community-based charity is a responsibility for all cloud companies.\nWe continue to look at other green initiatives – led by our employees as well as our client and partner ecosystem. Today, it is up to all members of the tech industry to be proactive in protecting the environment."
  },
  {
    "id": "8649-0",
    "title": "Looking for \"Lesbians\"",
    "author": "Algolia LGBTQ &amp; Allies",
    "text": "Searching the web for the word “lesbian” is like walking through a minefield. Type \"lesbian\", press enter, and you’re very likely to be flooded with porn. Add a word like “medical services” or “events”, and you get information about lesbian-friendly doctors and events. Mistype “gyno” (for example, “gyon”), or add ambiguous search terms like “lesbian sex”, and you’re brought back into the noise of lesbian-related porn.\nHaving to sift through porn while searching for lesbian-related information creates an unsatisfying, embarrassing, if not offensive search experience. It also conveys the wrong impression that, by default, lesbian content is pornographic. We can easily understand why this is pointed out by the LGBTQ+ community as a visibility issue.\n\nPride Month seemed like a good time for Algolia to look at this issue through the lens of search technology, to shed some light on what’s going on here and how it can be addressed.\nAlgolia provides site search for individual sites. Site search relies on structuring content, or at least formatting the content in a knowing way, as opposed to web search, where content is largely unstructured and unpredictable. By formatting and structuring data, Algolia gives customers control over filtering, ranking, and selecting content based on meaning, which allows separating adult content from other content. This is more difficult for Google. To understand why, let's compare what we do and what Google does, and see what steps to take to address the problem.\nKnow Thy Data\nTo better structure your data, you need to know your data. When you know your data, you can name and classify it, and place each item in a specific context - medical, entertainment, erotica. This enables filtering, which is a powerful tool that gives users power over what they see. Knowing data gives website owners the ability to pick and choose which parts of their data is searched, prioritize some data over others, and adjust wording, add… synonyms, and apply many other such tools to gain greater control over the search engine. For example, our customers can impact the order of search results using customized ranking.\nThis is what Algolia provides for site search, where the content is known by the owner of the website.\nSearch Engine Optimization\nThis kind of control is not available to Google because they are not offering the same service as Algolia. As a web search engine, Google does not and cannot know the data it searches. \nInstead, Google uses a ranking algorithm that is often referred to as alchemy or magic; and while they don’t give away their secrets, they do offer a set of techniques that website owners can use to improve their ranking, and therefore increase their web traffic and presence on the web. These techniques are grouped under the SEO (search engine optimization) label. We see the effects of SEO all the time. For example, some websites are favored over others, first name/last name combinations return famous people first, and it’s always easy to find someone’s LinkedIn account.\nSo let’s return to the point. The traffic on the word “lesbian” is largely traffic in search of erotica. According to the most popular porn websites, the word “lesbian” is the number one searched for term. You can see how this could be unsolvable if Google were to apply simple traffic-based popularity algorithm – such as, always show the most-visited websites first.\nLuckily, SEO is far more sophisticated. SEO offers website owners and communities ways to improve the ranking of their own websites and that of the community they represent. \nFor our purposes, SEO would be a tool used to generate relevant content when searching for the word ‘lesbian’. And there may be a few ideas worth exploring to make the “lesbian” search experience more relevant, and less adventurous.\nA Few Thoughts\nLeveraging popular websites\nLeveraging the traffic of powerhouse sites like Wikipedia and other non-porn,… high-traffic sites (e.g., medical websites, news and entertainment media) could be a suggestion. By leveraging, we mean creating a greater presence on these sites by adding more information about lesbians and using the word “lesbian” and other important terminology more consistently.\nEssentially, this would mean adding more trustworthy content to every official website: .net, .gov, .edu, .org, and so on.\nStructuring Content\nAnother option could be to structure websites that contain information or services important to lesbians. Key websites need to present their content so that Google can more easily understand it - and trust it as well. Large companies hire SEO experts to advise on every word and page of a website. Most suggestions involve a consistent terminology, smaller webpages, short, clear text, and smart use of microdata (Google’s way of adding clearly-defined structure to a website). This list is not exhaustive\nBacklinking\nHere is where the community plays the greatest role. For Google, this represents the inner linking (called “backlinking”) within the community. Sites within the community need to hyperlink to each other, to facilitate better surfing. This builds a more significant presence in these sites, and also more trust in the community as a whole. Backlinking combines content and creates a collective voice.\nMeta tags\nWhile Google has lost some confidence in meta tags, because of abuse and faking it, there are still some critical meta information that you need to concern yourself with, such as  and  tag on every page.\nNo guarantees\nNote that these suggestions bear no guarantee. Anything that you do is also done by porn website owners. It’s a competitive race to the top. Your efforts have to be rigorous and unrelenting.\n女同志, Lesbian, Lesbienne, lesbiyanki, समलैंगिकों, ...\nInterestingly enough, not every country has the same behavior. For example, English-speaking countries do not experience this, but French and… Russian ones do. Hindi shows only links to Pride parades. Somehow, every country has a different set of results.\n\nWe can’t know exactly why this is. Some say it’s because Google turns safe search on in the background for this and other such potentially offensive ambiguities. Groups are organizing worldwide to raise awareness. If we consider these differences in light of our above suggestions:\n\nThe English word “lesbian” is prevalent throughout the web – especially on sites like Wikipedia and WebMed, as well as all news and entertainment media. By contrast, the word “lesbienne” is spelled differently on Wikipedia (“lesbianisme”). You’ll want to modify Wikipedia to be consistent and more aligned to common usage.\nThe LGBTQ+ community in the US is larger and better financed than in any other country, so it can hire SEO experts to ensure that lesbian-friendly content is well-structured, optimized, and thoroughly backlinked.\n\nAll this makes a difference.\nPride! 🔎👭\nThere is no doubt that you can create an alternative meaning of the word “lesbian” on the web. Google’s machines learn quickly, and while indeed the ranking algorithm is sometimes mysterious, it is far more intelligent and nuanced than a simple “traffic” algorithm. Search engines are not traffic cops that get run down by all the speeding motorists out for a cheap thrill. Search engines can detect competing voices, and help them be heard, but you have to know how to get and keep their attention."
  },
  {
    "id": "8615-0",
    "title": "Fast and relevant user experiences at Azure scale",
    "author": "Hank Humphreys",
    "text": "Consumers expect instant, rewarding experiences no matter the volume of content those experiences rely on. After all, they find what they are looking for on Google, Amazon or Netflix in no time. But all companies do not have the resources of Google, Amazon and Netflix, and  building those search and discovery experiences at ultra large scale is hard.\nUntil now, companies with the world's largest websites or applications were forced to build out their own search solution and manage infrastructure around the world.\nIntroducing Algolia on Azure\nWe believe that matching your users intent with the right content is even more critical among very large collections of content. That’s the point of Algolia on Azure. \nAlgolia on Azure combines the power and scalability of Microsoft Azure’s highest performing Virtual Machines, the reliability of Microsoft Azure global infrastructure, and Algolia’s search and discovery engine speed and relevance so you can deliver your users the experience they expect over terabytes of content.\nScalable and robust do not mean rough. Algolia on Azure comes with the full capabilities of Algolia, as a service:\n\nFully-managed by Algolia: Algolia deployable on-demand and ready to operate in minutes, we take care of everything from scaling to security, so you can focus on building the best experience and optimizing relevance to serve both the business and the end-user.\nUnprecedented control and insight into the business: Algolia on Azure comes with our extensive suite of business tools, including experimentation, merchandising, personalization and analytics, to control and optimize digital user experiences\n\nSeveral of our customers are already leveraging the scalability of Algolia on Azure, including Depop. Depop is a pioneering global marketplace connecting together 10M+ sellers and buyers across the world over dozens of millions of designer clothes, vintage items and pieces of arts. \n“We chose Algolia on Microsoft Azure as it is one of the… only search and discovery solutions that is scalable and flexible enough to meet our constantly changing business and customer needs and provide access to our 25.6 million products. We know Algolia on Azure will seamlessly grow with us and our ever-growing community, while we focus on continually innovating our platform.” — Faris Aziz, Product Manager at Depop\nMicrosoft and Algolia: a strong alliance for our customers\nAlgolia on Azure is the core element of an extensive partnership between Algolia and Microsoft. Both teams closely collaborate on co-marketing and co-selling activities to deliver the benefits of Algolia on Azure to organizations building innovative user experiences while relying on large amounts of content all over the world. \n“Today’s consumers expect a fast, relevant and intuitive digital experience connecting them to the information, products and services they need, when and where they need them,” said Gavriella Schuster, Corporate Vice President, One Commercial Partner Organization at Microsoft. “To help large, enterprise organizations keep pace with consumer demands, we are very excited to work with Algolia to provide our customers with an end-to-end search and discovery solution to optimize the user experience.” \nWe are very excited to work closely with Microsoft at defining the future of digital experiences and are looking forward to helping more organizations delight their users!\nClick here to learn more about Algolia on Azure.\n "
  },
  {
    "id": "8624-0",
    "title": "Search Party #18 — Crawling Edition",
    "author": "Julie Reboul",
    "text": "We were happy to organize our regular Search Party last Wednesday, June 12th, 2019. This time it was about crawling web content. \nPeople tend to think crawling is about stealing other people’s data. Although some crawlers do that, crawling itself is simply the act of extracting content from websites. The motive is more often legitimate than illegal. During this event, we had three amazing talks that presented different ways to crawl web content and discussed some easily overlooked challenges when developing a crawler.\nThe challenges of crawling the web — and how to overcome them\n\nSamuel Bodin, Algolia\nIn the first presentation, Samuel Bodin gave us a glance into how Algolia indexes complex documents like PDFs, Words, Spreadsheets, … Also, how to render websites with javascript at enormous scale.\nHe also spoke about the common trap with websites, more specifically, the “Rabbit Hole”, a place where your crawler gets stuck forever.\nLast but not least, he gave a quick presentation about how Algolia manages crawling with security concerns. Especially when executing javascript written by customers on Algolia’s server without exposing any sensitive data.\n \nWriting a distributed crawler architecture\n\nNenad Tičarić, TNT Studio\nIn the second presentation, Nenad Tičarić talked about the architecture of a web crawler and how to code it quickly with the php framework Laravel.\nHe broke his presentation down into two parts. He started with a good overview of crawlers and introduced a few terms that you’ll likely want to know before digging into the subject. He also described how to design and architect an automatic web crawler at scale.\nThe second part focused on how to achieve that very simply with PHP, and more specifically Laravel, and a very few basic tools like Guzzle and Artisan.\n \nAutomatic extraction of structured data from the Web\n\nKarl Leicht, Fabriks\nFor the last talk of the day, Karl Leicht spoke about how to achieve automatic and smart attribute… extraction with a crawler.\nHow to crawl millions of different websites? That’s the interesting question Karl asked us today. He described how to scale your code without reinventing the wheel for every website.\nWe saw how to differentiate programmatically a listing page and a product page, the importance of microdata and where to find the more valuable information in a page.\nThe second part focused on the challenges of maintaining this code in the long run, with a long look at tests and monitoring.\nThe Next Event\nWe host our Search Party in our Paris office. It’s for everyone and… it’s free! Please join us next time.\nFollow us on EventBrite so you can be notified for the next event. "
  },
  {
    "id": "8574-0",
    "title": "Learning from Our Natural Language and Voice Search Internal Hackathon",
    "author": "Dustin Coates",
    "text": "Voice is a huge topic for Algolia. The best perspective I can provide is that in early 2018, we had one or two customers each week asking us about voice in passing. Today, most of our conversations touch on voice. We’re at an exciting spot where our core interests are meeting the market demands perfectly.\nVoice touches on everything\n Everybody has a hand in the future of voice. We need to provide good relevancy for natural language queries, build services that enable people to adjust that relevancy on their own, and create tools that help developers implement voice search on any platform. We also want to work directly with our business users to design the best voice experience This isn’t the effort of a small group of people, but of an entire company. All of Algolia believes in the future of voice search.\nThat’s why I loved this recent internal hackathon we held. We invited the entire team to participate for two days to experiment with natural language interactions. We wanted everyone to daydream about where voice could head.\nA hackathon for everybody\nOften, when people hear “hackathon,” they think of groups of devs huddled in a circle, click-clacking on keyboards. It’s true that, in my opinion, our engineering team is one of the best for our size in the world. But we also have talented people in every single job role. If we limited the hackathon just to the coders, we would lose out on these other perspectives. While we called the event a hackathon, we also thought of it as less of an event for hacking and more of an event for building. Anyone can build, even if they can’t code.\nI think the team that best embraced the call for unique viewpoints was “Team 28.” When you went by their sprawling area during the event, you might have wondered if their name represented the number of team members they had. (In truth, Team 28 had eleven people.) Software Engineer Jonathan Montané started the team, and he was intent on recruiting from as many different… job functions as he could.\nMontané told me that the mixture of coders and non-coders was vital to their project’s success. He said that because voice app builders spend much of their time understanding how users will interact with the apps, the non-technical members of the team were able to help decide what features Team 28’s app would support. This led to the creation of “primary” and “secondary” functionality that the coders on the team might not have discovered if they had started immediately with the code.\nAlgolia’s an international company. We have six offices around the world, and even in our Paris office nearly a third of our employees come from outside of France. Everyone brings a unique perspective.\nWe wanted all of our offices to participate, and I was happy to see our New York and Atlanta offices get involved. Nearly every person in these offices contributed–incredible when you realize that these are primarily sales offices! New York focused on a common Big Apple problem of what to eat for lunch, with an assistant that first determined the identity of the asker before turning to Algolia search to provide restaurant recommendations. Atlanta, meanwhile, built an Alexa skill to serve their own sales needs. A user can ask for “opportunities in pipeline,” and Algolia’s query rules will determine intent to provide the right answer.\nWhats next for Algolia?\nThe hackathon projects have helped us as we expand our voice search and natural language processing capabilities. Don’t fret–we aren’t shipping code directly from the hackathon. We did, however, learn a lot about what developers with little NLP experience are looking for as they work with existing tools. We’re now taking this experience and combining it with customer feedback and our understanding of where voice is headed in the coming years. There is much opportunity for natural language search, and I expect that Algolia is going to continue our leadership.\nI hope people will… check out what we have to offer for their voice search needs on mobile, voice-first, or anywhere else. Check out what we’re doing at algolia.com/voice and let’s talk."
  },
  {
    "id": "8587-0",
    "title": "8 Algolia-Tested Best Practices for Kubernetes",
    "author": "Rémy-Christophe Schermesser",
    "text": "At Algolia, we started using Kubernetes almost two years ago. We had new services to deploy, and even if we’re big users of bare metal machines, we needed more flexibility. Therefore, we decided to test and use Kubernetes on new systems. Two years later, most of our products are deployed on Kubernetes. As more and more teams started to use it internally, we created an internal training. And today, we’re proud to make this training open source, so anyone can learn from it and contribute.\nWe extracted eight practices from this training that we consider to be key for using Kubernetes correctly. Let’s review them.\n1. Do not use root user in your containers\nThe container paradigm, and the way it’s implemented on Linux, wasn’t built with security in mind. It only exists to restrict resources, such as CPU and RAM, like the documentation of Docker explains. This implies that your container shouldn’t use the “root” user to run commands. Running a program in a container is almost the same as running a program on the host itself. If you are interested in knowing more, check this article to understand why.\nThus, add those lines on all your images to make your application run with a dedicated user. Replace “appuser” with a name more relevant for you.\n\nThis can also be ensured at the cluster level with pod security policies.\n2. Handle the “SIGTERM” signal\nKubernetes sends the “SIGTERM” signal whenever it wants to gracefully stop a container. You should listen to it and react accordingly in your application (by closing connections, save a state, etc.) In general, following the twelve-factor app recommendations for your application is considered good practice. Also, don’t forget to configure  on your pods. The default is 30 seconds, but your application might need more (or less) time to properly terminate.\n3. Use a declarative management for your manifests\nUse declarative manifests so you can rollback your code and infrastructure efficiently. It means… that your source versioning should be the source of truth of your manifests.\nIt implies that you only use  to update or create your Kubernetes resources, but also that you don’t use the  tag for your image containers. Each version of your containers should be unique, and using Git hashes is a good practice. When deploying a new version of your application, you should update the manifest by specifying a new version for the containers, then commit the manifest in your source control, and finally run .\n4. Lint your manifests\nYAML is a tricky format. We use yamllint, because it supports multi-documents in a single file.\nYou can also use Kubernetes-specifics linters:\n\nkube-score lints your manifests and enforce good practices.\nkubeval also lints the manifests, but only checks validity.\n\nIn Kubernetes 1.13, the  option appeared on  which lets Kubernetes check your manifests without applying them. You can use this feature to check if your YAML files are valid for Kubernetes.\n5. Configure the liveness and readiness probes\nLiveness and readiness are ways for an application to communicate its health to Kubernetes. Configuring both helps Kubernetes handle your pods correctly, and react accordingly to state change.\nThe liveness probe is here to assess whether if a container is still alive; meaning, if the container is not in a broken state, a deadlock, or anything similar. From there, it can take decisions such as restarting it.\nThe readiness probe is here to detect if a container is ready to accept traffic, block a rollout, influence the Pod Disruption Budget (PDB), etc. It’s particularly useful when your container is set to receive external traffic by Kubernetes (most of the time, when it's an API).\nUsually, having the same probe for readiness and liveness is acceptable. In some cases though, you might want them to be different. A good example is a container running a single-threaded application that accepts HTTP calls (like PHP). Let’s say you have an incoming request… that takes a long time to process. Your application can’t receive any other request, as it’s blocked by the incoming requests; therefore it’s not “ready”. On the other hand, it’s processing a request, therefore it’s “alive”.\nAnother thing to keep in mind, your probes shouldn’t call dependent services of your application. This prevents cascading failure.\n6. Configure resource requests and limits\nKubernetes lets you configure “requests” and “limits” of the resources for pods (CPU, RAM and disk). Configuring the “requests” helps Kubernetes schedule your pods more easily, and better pack workloads on your nodes.\nMost of the time you could define . But be careful, as your pod will be terminated if it goes above the .\nUnless your applications are designed to use multiple cores, it is usually a best practice to keep the CPU request at  or below.\n7. Specify pod anti-affinity\nWhen you deploy an application with a lot of replicas, you most probably want them to be evenly spread across all nodes of the Kubernetes cluster. If you have all your pods running on the same node, and this node dies, this will kill all your pods. Specifying a pod anti-affinity for your deployments ensures that Kubernetes schedules your pods across all nodes.\nA good practice is to specify a  on the hostname of the node:\n\nHere we have a deployment “my-application” with two replicas, and we specify a  specification with a soft requirement (, see here for more details), so we don’t schedule the pods on the same hostname ().\n8. Specify a Pod Disruption Budget (PDB)\nIn Kubernetes, pods have a limited lifespan and can be terminated at any time. This phenomenon is called a “disruption”.\nDisruptions can either be voluntary or involuntary. Involuntary disruptions means, as its name suggests, that it wasn’t something anyone could expect (a hardware failure for example). Voluntary disruptions are initiated by someone or something, like the upgrade of a node, a new… deployment, etc.\nDefining a “Pod Disruption Budget” helps Kubernetes manage your pods when a voluntary disruption happens. Kubernetes will try to ensure that enough that match a given selector are remains available at the same time. Specifying a PDB improves the availability of your services.\nConclusion\nWe recommend you adjust those practices based on the specifics of your applications and workload. Yet, we think these are fine defaults, and we apply them on all our apps in Kubernetes.\nYou can find more details on these good practices on the dedicated section of the training."
  },
  {
    "id": "8578-0",
    "title": "How Algolia Helped Us Improve UX, Dump Legacy Code and 4x Engagement",
    "author": "Jason Frueh",
    "text": "When I stumbled across Algolia years ago, I knew it was going to be incredible for my business, MyCreativeShop.com. As a small business owner, increasing efficiency is very important to me. I knew that Algolia could take work off my plate and do it better than we ever could.\nI could talk at length about the technical problems Algolia has helped us solve, but the most exciting part has been the ease of optimizing our site for search engines. Our business relies heavily on inbound traffic to high-converting pages. The code behind these was shaky considering how important they are to the business. Algolia helped us create an efficient, clean code base, create a better user experience and maintain the search traffic that we’ve worked so hard to earn.\nIf you run an e-commerce site, you know that a large product catalog creates a lot of SEO opportunity, but also plenty of challenges. Before we implemented Algolia, we were struggling with a slow browsing experience, inaccurate results and less-than-ideal on-page SEO.\nThese are common SEO problems for the e-commerce sites (and just the tip of the iceberg). I have a unique perspective on this topic because I’m the business owner and the lead developer (yeah, we are a pretty small team) at MyCreativeShop.com. This means I have to understand the business implications of SEO and get in the weeds to make changes.\nBut regardless of the size of your company, the struggle to solve these problems are often the same:\n\nAs the business owner I demand that the user experience be great with absolutely NO SEO risk. Every little bit of traffic matters and it gets a lot of my focus. As the developer, I see new shiny tools all of the time and I get eager to leverage them, especially when they make my life easier. I value efficiencies in coding but admittedly don’t always see where code fits into the business’ bigger picture.\nI realized immediately that Algolia could make both roles happy—really happy. Here’s how:\n\nFor the… business owner: Algolia gives your development team the flexibility to create a user experience and implement on-page SEO in whatever way is best for your company and customers. Think of it as a tool that allows your development team to deliver exactly what you or your design team has envisioned, but in a fraction of the time. And it’s infinitely scalable and always available.\nFor the developer: Gone are the days of dreading making customizations to your product catalog just to try to meet the seemingly crazy demands of management. As a developer, I know the pain of working in an antiquated product catalog environment and it’s terrible. Move your data set to Algolia and start using their well documented API to handle everything search and catalog related. The ease of integration is incredible.\n\nAs I began to integrate Algolia, it became clear to me that this was going to give us an incredible amount of flexibility. I took one day—yes, literally only one day—and integrated our data with Algolia. Within four days, I threw away our old catalog code and rewrote everything in our catalog from the ground up. Once our data was synced, I could use the incredible API to power all of our catalog searching, browsing and filtering that historically had been so incredibly difficult to manage. Instantly, any problems with speed and accuracy of the search results just went away.\nMore pageviews, more traffic, more business\nMoving to Algolia has been easy from a technical perspective, but what about the business side?\nSince integration, we've seen that visitors who land on our product catalog convert to paying customers at a 38% higher rate than before. We attribute this to speed of browsing the products. Additionally, pages viewed per session have increased over 200% thanks to a better user experience. \nAlgolia allows users to leverage either ‘front end search’ or ‘back end search.’ (You can read about each here). We use a combination of both search methods because… we offer search in multiple areas of our website and application.\nThe most critical part of our website is our design templates. These pages drive high-intent traffic and are key to driving new customers. It’s vital that all the pages in this section—the template pages, the category pages and the search result pages—are well-optimized for search. We need to be able to control on-page SEO down to tiny factors like URL structure. \nFor our front facing design templates, we felt that back end search was the smartest decision for us for a few reasons:\n\nIt removes all risk from an on-page SEO perspective. \nThe buzz term today is ‘Server Side Rendering’, or SSR. Using Algolia this way allows to generate our HTML on the server (true SSR) and then display it to the browser ensuring that all web crawlers see the it the same way. \nIt gives us a level of confidence where we know exactly what the crawlers are analyzing. \nIt allowed us to maintain our existing URL structure which was critical.\n\nWe also use search inside our password-protected web app. SEO is not a factor, but user experience is hugely important. For this, front end search via Algolia’s instantsearch.js libraries was the obvious choice since:\n\nIt’s incredibly fast\nThere are built in widgets to help us build the UI really quickly\nWe had no SEO concerns or legacy URL structures that needed to be maintained.\n\nIn just a few short weeks, Algolia gave us the ability to create the user experience that we wanted. Speed and accuracy improved significantly. And we have precise control over exactly how we display results - either by using a true server side rendering (SSR) approach or by leveraging Algolia’s awesome instantsearch.js libraries. \nTo me this flexibility is really the unsung hero of what Algolia is. It allows us to sleep well knowing that we are in total control of how customers are interacting with our product catalog and how crawlers are viewing the results. \nIf you are struggling with product… navigation and search, give Algolia a look. You’ll be amazed at how what was once hard becomes incredibly simple."
  },
  {
    "id": "8561-0",
    "title": "Vive l'innovation: Celebrating Innovation across Europe at VivaTech 2019",
    "author": "Julien Lemoine",
    "text": "Living in Paris, I often get asked by venture capital (VC) companies about how attractive the investment environment is in Europe — especially in Paris.\nWhen Algolia was founded in 2012, Paris was perceived quite negatively by U.S. VCs. I heard all of the typical stereotypes and misconceptions about the work culture in France — from 35-hour work weeks to long vacations during the summer months. While there is some historical relevance for such stereotypes, I had to explain several times that people actually work hard in France!\nFast forward to 2019, and the European and Paris ecosystem have vastly transformed to be both dynamic and attractive to investors and companies alike. In recent years, many Paris-based technology companies, including Criteo, Deezer and Dailymotion, have grown to be significant players on the global stage. Algolia itself is now used by 6,500 companies around the world and has offices in every major region following our recent expansion into APAC and Japan.\nIn contrast to 2012, I now get more questions from both VCs and U.S.-based startups about how to open an office in Paris, more so than any other European city. With over $3.5 billion in VC-led investments and over 300 deals with Paris-based technology companies in the past year alone, Paris is one of the most active and attractive technology ecosystems.\nThis is no more apparent than when I recently attended VivaTech 2019, a three-day event earlier this month that brought together the world’s leaders in technology and business, with the most promising startups and the disruptors of tomorrow, to explore transformational ideas on a global scale. As its tagline suggests, VivaTech is “the world’s rendezvous for startups and leaders to celebrate innovation.”\nI have been following Viva Technology since its first event in 2016 and have been very impressed by the importance of this event in the tech world. The numbers from VivaTech 2019 speak for themselves with over 120,000 attendees,… including over 13,000 of the most innovative start-ups. I was also fascinated by the international profiles of leaders and 3,300 investors around the world that came to Paris to attend VivaTech 2019.\nAs part of the event, VivaTech hosted the inaugural Next European Unicorn Awards. Algolia was honored to have been the winner in the Deep Tech category, and it was a privilege for me to have accepted this award on behalf of all of our global employees. Anytime you get recognized by the industry, it is a great milestone and reminder of how far we have come since we got our start in 2012.\n\nWe are humbled by these awards. It provides us with perspective on how far we’ve come, how we got here, who we serve and how much more we want to continue to innovate, grow and contribute to our customers’ success. With our rapid growth (90%+ growth last year), it gives us an opportunity to recognize, celebrate and thank our 300+ employees and our 6,500 customers worldwide that have gotten us to this point. Thank you all!\n\nBuilding and scaling a technology company is hard work, and requires passion, innovation and perseverance. It’s moments like these that further validate we are on the right path and fuel our desire to accelerate our efforts. If you are inspired by innovation, driven, collaborative and want to join us for the ride, we’d love to hear from you, and I would encourage you to please take a look at open positions on our Careers Page.\nCongratulations to others winners including PayFit (B2B SMB), Snyk (B2B Enterprise), Vinted (B2C) and Olio & OpenClassrooms (Tech 4 Good) that also won the grand prize. Here’s to more success for all of the European start-ups, as we continue to make our mark on the world and become a modern and lasting hub of innovation.\n  "
  },
  {
    "id": "8531-0",
    "title": "Simplifying Parcel Delivery at Algolia",
    "author": "Clément Denoix",
    "text": "On a daily basis, Algolia employees receive loads of packages at the Paris office. So far, Kumiko, our office coordinator, has been taking care of them. Every time a new package arrives, Kumiko has to search the label to find who it’s for, then find the person on Slack and let them know their package is waiting at the front desk.\n\nThis manual process was working, but Algolia is rapidly growing: only last year, the number of employees in the Paris office has more than doubled. Handling parcel dispatch by hand started taking more and more time for Kumiko. During the holiday season, it got really out of hand.\nA typical day at Algolia, a month before Christmas\nObviously, manual handling couldn’t scale.\nI work in the Internal Tools squad at Algolia. Our mission is to make Algolia’s teams more efficient by automating inefficient processes, making tools, and providing technical support. I thought there should be a faster, easier, scalable way to help dispatch packages.\nI decided to build a web application for it. My goal was to automate the process as much as possible, from scanning the label to notifying people on Slack.\nFirst attempt: the barcode\nMy first idea was to use the barcode that’s on the label. I thought I could extract the employee’s first and last name from it. However, I quickly discovered that a barcode doesn’t contain the same kind of data as you have in QR codes. Most of the time, they only contain EAN identifiers. These numbers are intended to query private carrier APIs to fetch the parcel’s details.\nSecond attempt: the OCR\nWe have an Algolia index with every Algolia employee, which we use on our about page. I thought it could be an interesting starting point. The idea was to “read” the parcel label with an optical character recognition engine (OCR), and match it against the right record in the index.\nStep 1: read the package label\nThere are several open source libraries for handling the OCR part. The most popular one is Tesseract.… However, you typically need to perform some pre-processing on the image before being able to ask Tesseract to recognize the characters (desaturation, contrast, de-skewing, etc.) Also, some of the parcel labels we receive are handwritten! Tesseract is not good at reading handwritten words, and the preprocessing part was a lot of work. Therefore, I decided that this solution was a no-go.\nI knew about Google's Vision API, which offers OCR capabilities, and wanted to try it, so I decided to give it a go. Among other things, it provides:\n\n1,000 free API calls per month (which is more than enough to start).\nHandwritten characters detection (in beta).\n\nI created a React app, and installed the React Webcam component to access the device’s camera. Internally, this React component leverages the getUserMedia API.\nOnce the user captures a label using their phone, the app sends it to an Express backend. This takes care of proxying the base64-encoded image to the Google Vision API. Vision then returns a JSON payload containing the data as text.\nStep 2: searching with Algolia\nLabels aren’t pretty. They contain a lot of noise. The relevant information is baked somewhere, surrounded by other data: characters that are only relevant to the delivery person, label numbers, the sender's address, etc. Additionally, the order isn’t consistent, and the information isn’t always complete, so we can’t rely on word ordering or element position to extract relevant sections, before sending them to Algolia.\nObviously, I didn’t want to add an extra manual step for our office manager to select the right parts. This would be cumbersome, and defeat the whole purpose of the app.\nFortunately, the Algolia search API has an interesting parameter: .\nWhen you set this parameter to  and the engine fails to find any results with the original query, it makes a second attempt while treating all words as optional. This is equivalent to transforming the implicit  operators between words to .\nUsually,… this parameter is helpful to improve results when a query is too restrictive. In my case, it allowed me to send the extracted data unprocessed; I trusted the Algolia engine to “ignore” the extraneous words from my query, and only take the important ones into account.\nThis left only a few steps: extracting the first hit from the list of Algolia search results, and displaying it. From there, our office manager could confirm the result, and automatically send a Slack message to the right employee.\n\nWhen Kumiko takes a picture of the package label, the app sends it to Google Vision through the Express backend. Google Vision returns a JSON payload with the recognized text, which the backend sends to Algolia as a search query, along with the  option. Algolia returns a list of matching records, from which the backend extracts the first hit, and returns it to the React app. This allows Kumiko to Slack the person directly, in a single tap.\n\nAlgolia is a powerful search engine, but search isn’t limited to a search box. With a bit of imagination, you can push the usage of Algolia far beyond the box, and solve a variety of problems.\nThis was enabled by Algolia's strong culture. This project stemmed from one of Algolia's core values: care. We try to be as helpful as possible with one another. And I did it during Algolia's monthly engineering off-sprints, which allows employees to experiment!"
  },
  {
    "id": "8551-0",
    "title": "Think Globally, Act Locally — Announcing our Expansion into Japan",
    "author": "Nicolas Dessaigne",
    "text": "For the past six years, Algolia has been fortunate to have worked with leading companies around the world — from one of the world’s largest online marketplaces to one of the world’s largest gaming companies to many, many more in between. In addition to providing lightning-fast search and discovery experiences that deliver improved business results across modern websites and mobile apps, Algolia’s search-as-a-service platform is language agnostic which has helped its rapid ascent to be the top search solution used by over 6,500 companies around the world.\nAs we build on our hypergrowth and continue to scale the company globally, we believe that we need to be closer to our customers — to listen, to learn, and to build longstanding partnerships.\nToday, we took another major step towards this goal and announced our global expansion into the Asia-Pacific region along with the opening of our new office in Tokyo, Japan.\nWith over 100 Japanese-headquartered customers including Cookpad (Komerco), CYDAS, alu, and Cansell, Algolia Tokyo will be able to better support our local customers with improved Japanese search relevancy, gain a richer understanding of our global customer base and attract local talent to add to our world-class workforce.\nJapan is culturally-rich and unique in its language, customs and norms. With our new Tokyo office, we are now even better positioned to understand the Japan-specific business and technology requirements to, for example, address three alphabets including Kanji, Katakana, and Hiragana, tokenize Japanese text for better search relevance, and fine-tune our solution for our customers.\nWith offices in Paris, San Francisco, New York, Atlanta, London, and now Tokyo and with over 70 data centers around the world, Algolia is well-positioned to support our customers, wherever they may need and in any language they require, all while delivering a modern, lightning fast experience that their own customers expect.\nWe are excited to continue… to invest in our global footprint and our global platform. Today is an example of this and a sign of things to come. Stay tuned for more exciting news on this front in the weeks and months ahead — onward!"
  },
  {
    "id": "8522-0",
    "title": "Redesigning our Docs – Part 7 – What's next to come",
    "author": "Marie-Laure Thuret",
    "text": "This is the last article in a seven-part series of blogs that describe our most recent changes to the architecture and content of our documentation. We focus here on our next steps.\nWhen we hit the \"Deploy\" button last December and went live with the redesign of our docs, we knew it was only the beginning. From completely restructuring our content to optimizing our technical stack, we had set ourselves up for a great journey. In 2019, we are committed to pushing this effort further, making sure to offer the best experience on how to leverage the power of Algolia.\nFor that reason, and to conclude this series, we wanted to present a preview of what’s coming next.\nA UI/UX showcase to inspire\nToday, with our Algolia search engine and all the tools we provide on top of it, we equip people to build the best search and discovery experience they can. Yet, there's no single great search experience. When we dissect what makes great search, we realize it’s a subtle and thorough mix of several UI & UX elements. Once combined, they offer the end user the ability to quickly and smoothly find what they are looking for.\nA great search experience goes beyond the classic autocomplete dropdown or display of basic filters. Today, let’s say you have this idea of adding a related items section to your product pages, or you want your users to browse their search history—all those kind of gems are missing from the front pages of our documentation. Worse, our docs do not provide a showcase for what they are and how to build them.\nWe're truly convinced that exposing these unknown best practices in a clear and engaging manner will inspire you to build the best search and discovery experience for your end users.\nReaching out to our business users\nWe primarily designed our docs to address a technical audience. One of our major redesign efforts was to make sure we addressed the whole spectrum of our technical audience, from beginner to advanced levels.\nHowever, the implementation phase… of Algolia is only the beginning of your search journey. Once it’s live, you need to go back and forth with your search analytics to understand your current pitfalls, so you can take actions and fix relevance. Additionally, if you’re an e-commerce website, you’ll probably want to run merchandising campaigns and you need to know how to do that. During the research phase, we quickly understood that our audience was not only technical. We also have other profiles with different needs and expectations. In 2019, we want to address those.\nMake our content speak - Interactive tools\nBringing interactivity into our docs was one of last year’s main objectives. It started in June 2018 when we shipped our interactive tutorial. Our goal was to quickly walk our new users through all the steps of an Algolia implementation, from pushing their data to building their UI.  Yet, we wanted to do that while introducing them to all the tooling we provide, and allow them to get their hands dirty, playing with our APIs.\nRight after, during the summer, we also gave our users a filter syntax validator. We had realized that we receive many questions about our  parameter. Most of the time it’s about syntax, and the error messages or limitations people encountered. We decided to add this validator directly in our docs, so that users could directly test whether their filters were correct.\nFinally, we released an interactive showcase for our InstantSearch libraries. Instead of giving users a long list of widgets that InstantSearch provides, we thought it was better to directly show them the kind of interface they could build and to let them directly interact with every widget to see their purpose. While this required some extra effort on our part, it was clearly worth it, as it has since then proven to be the most popular section in our docs!\nProviding more interactivity to our users is a good way to ensure they engage with our content, and have an alternative way to digest the… information. We’re going to continue following that path in 2019.\nBetter performance\nWe strive to make our docs faster and more stylish. With the redesign, we started by reducing the size and memory footprint of our CSS, making styles lightning-fast to load for end users. This also makes them more scalable. This year we want to continue that optimization by reducing the size of our JavaScript and other UI scripts to make sure the first experience our readers get is as smooth and as fast as possible.\nKeep supporting our Algolia teams while we add new product features\nAll those projects do not replace our primary mission, which is to ensure better implementations and deeper understandings, and thus more confidence in using our product. That’s the reason why we have streamlined the way external contributors can contribute to our docs, to support all our teams in the best possible way when they release a new product or feature.\nWe want to make sure our contributors have a seamless experience when working with us.\nWe need your feedback!\nThese projects are only a subset of what we have in mind for the full year and beyond. Our team has learned a lot on how to make this journey fun and productive. We're going to pursue our research and work because we're committed to making documentation match our readers’ needs. That's why your feedback is really important to us!\nIf you have any thoughts, feedback, or ideas about our docs, you're more than welcome to tell us, we'll listen attentively. To reach out, feel free to comment on this article! You can also use our feedback form available on every page of the documentation, or send us an email at support@algolia.com."
  },
  {
    "id": "8321-0",
    "title": "One Year of Load Balancing",
    "author": "Paul Berthaux",
    "text": "From the beginning at Algolia, we decided not to place any load balancing infrastructure between our users and our search API servers. We made this choice to keep things simple, to remove any potential single point of failure and to avoid the costs of monitoring and maintaining such a system.\nAn Algolia application runs on top of the following infrastructure components:\n\na cluster of 3 servers which process both indexing and search queries,\nsome DSNs servers (not DNS). These are read-only replicas serving only search queries. Their primary purpose is to provide faster search to people located geographically far away from the main cluster.\n\nInstead of putting hardware or software between our search servers and our users, we chose to rely on the round-robin feature of DNS to spread the load across the servers. Each Algolia application instance is associated with a unique DNS record, which responds in a round-robin fashion with one of the bare metal servers that handles the given Algolia app.\n\nWe consider the most common and optimal usage of Algolia to be with a front-end implementation. In this case, mobile devices or laptops directly establish communication with our bare metal servers. In such a context, we can assume there will be a significant amount of DNS resolution, each leading to a few search requests. This is the best situation to rely on round-robin DNS for load balancing: a large number of users request the DNS to access Algolia servers, and they perform a few searches. This leads to a server load that matches the round-robin DNS resolution. Additionally, to enforce even more DNS resolution, we decreased the DNS TTL to one minute.\n\nIn the end, this system was simple. It didn’t use any dedicated hardware or software to manage on our own, and things went pretty well.\nThat is, until Black Friday.\nDNS-based load balancing limitations\nBack-end implementation and uneven load balancing\nAs mentioned earlier, we strongly recommend our customers to go with… front-end search implementations. Many parameters are motivating this choice; one of which is to leverage our DNS-based load balancing system. Yet, this isn’t always doable: some clients have specific constraints, like legacy design or security concerns, which lead them to opt for a back-end implementation. Doing so, their back-end servers relay all search queries to our infrastructure.\nIn this specific context, we already knew that our DNS-based load balancing was suboptimal:\n\nNow, a small group of servers perform a few DNS resolutions and forward a considerable number of requests to the chosen back-end server. Instead of 1,000 users making 10 queries each, we now have 1 user making 10,000 queries.\nAs the sessions with our search servers can live longer, the back-end server can send even more requests without needing to re-perform DNS resolution.\nSometimes, the customer servers even override our DNS TTL so they can use their DNS cache longer.\n\nThat said, the main focus we had when we designed our infrastructure was resilience. This means that, for most customers, a single cluster node can handle all the search load. Consequently, an uneven load across the cluster nodes wouldn’t have any impact on the search experience.\nDSN for horizontal scaling\nInitially, the DSNs were introduced to increase performance for users who perform search requests far away from the main cluster, by bringing read-only servers near them. Yet, we soon realized that it was also an easy way to bring more search capacity in a given region, by scaling the servers horizontally to absorb more search requests.\nThe Black Friday Incident\nWe had a big customer with a back-end implementation for which the load was too big to be handled by a single server. We had already deployed many DSNs in addition to the cluster, all in the same region, to absorb the search load coming from their back-end servers.\nYet, when Black Friday arrived, they started to experience an increased number of search queries.… Even if we had worked on dimensioning the infrastructure to absorb the load, they ended up in a situation with slow search queries and even some failing ones. For end users, this meant a highly degraded search experience with increased latency, during a time of the year when you expect an e-commerce website to be highly performant.\n\nThe load was uneven: the total number of available servers on our side to handle their requests outnumbered the number of servers on their side able to send requests. We ended up in a situation where, in the best case scenario, with our DNS-based load balancing, each of their servers would choose one of ours and stick to it for few minutes, overloading it, and leaving a few others not used at all.\n\nThis made us reconsider our DNS-based load balancing method, at least in this specific use case which combines heavy search load with back-end implementation.\nHere comes the Load Balancer\n\nFirst iteration\nTo solve the issue during Black Friday, we went for a quick fix, and we deployed a rudimentary load balancer. We leveraged Nginx, and its ability to proxy requests and load balance them toward a group of upstream servers (in our case, the Algolia servers).\n\nWe saved the day, and the traffic was evenly load balanced. This confirmed we needed such a system in some cases. Yet, at this point, it was more a workaround than an actual long-term solution. The whole thing was mainly static, with customer-specific parameters hardcoded in the Nginx configuration. This situation raised many interrogations:\n\nHow to make such a system customer-agnostic?\nHow to dynamically target the right group of search API servers for a given incoming request?\nHow to make it handle our daily infrastructure operations like changing, adding, or removing servers over time?\n\nSecond iteration\nFor the second iteration, the focus was to find a way to make the load balancer generic. The primary challenge was to dynamically build the list of upstream servers able to serve an… incoming request. To solve this kind of issue, you can think of two opposite approaches:\n\neither the load balancers know in advance all the information they need to operate,\nor they learn what they need to know when they handle the incoming requests.\n\nWe went for the second solution, mostly because the total amount of data we would have to go through for each request was too significant and impactful to keep a low latency on search requests. We implemented a slow learning workflow, to try and make everything as simple as possible, and avoid to manage a complicated and huge distributed data store system.\nEach time the load balancer receives a request from a customer it doesn’t already know about, it goes through a slower process to get the list of upstream servers associated with this customer. All the following requests for the same customer are handled much faster, as they then fetch the needed upstream information directly from the local cache.\n\nWe tried several technical solutions to achieve this:\n\nHAProxy offers a Lua support for dynamic configuration, but from what we tested, it was too limited for our use case.\nEnvoy was (and still is) quite promising but the learning curve is pretty steep, and even though we managed to make a working PoC, their current load balancing algorithms are too restrictive for our long-term vision.\nWe tried to make a custom load balancer in Go. The PoC was working fine, but it remains difficult to assess the level of security and performance of such a solution on our own. It’s also a lot harder to maintain.\nWe finally tried OpenResty, which is Nginx-based and lets you run custom Lua code at different steps of the requests processing. It has a quite well-developed community, there are a bunch of available modules, either official or community-driven, and the documentation is good.\n\nWe decided to go with OpenResty. We combined it with Redis for the caching part, as OpenResty offers a convenient module to interact with Redis:\n\nWith… this iteration, we managed to make our load balancer more scalable and easily maintainable by finding mechanisms to remove any static configuration from it. Yet still, a few things were missing to make it production-proof:\n\nHow to make sure it correctly and transparently handles upstream server failures?\nHow to make sure we can still operate changes on the infrastructure, as we do daily?\nWhat happens if it can no longer access our internal API?\n\nThird (and current) iteration\nIn the third and latest implementation, we introduced some mechanisms to make the whole system more failure-proof.\nIn addition to OpenResty handling the load balancing logic, and Redis caching the dynamic data, we added lb-helper, a custom Go daemon.\nThe complete load balancer now looks like this:\n\nThe lb-helper daemon has two different roles:\n\nAbstract our internal API. OpenResty learns about the upstream servers through the local lb-helper, which periodically fetch data from our internal API. If the load balancer fails to connect to our internal API, it can still operate with potential slightly outdated data.\nManage failures. Each time an upstream server fails more than 10 times in a row, we consider it as down and remove it from the active cache. From there, the lb-helper probes the down upstream to check whether it’s back or not.\n\nBottom line\nToday, we still mainly rely on our DNS-based load balancing, as it fits 99% of our use cases. That said, we’re now also aware that this approach has some limitations in certain situations, such as customers with back-end implementations combined to a heavy search load. In such a context, deploying a set of our load balancers brings back an even load on the search infrastructure.\nRequests per second distribution over time for a set of servers, first without, then with a load balancer.\nAlso, these experiments showed us that we built much more than a simple load balancing device. It brings an abstraction layer on top of our search infrastructure,… making failures, infrastructure changes or scaling almost fully transparent to our customers.\nAs we’re currently working on the fourth iteration, we’re attempting to introduce a latency-based algorithm to replace the current round-robin. The long-term plan is to check whether we can bring a worldwide abstraction layer on top of our search infrastructure. Yet, trying to go global at this scale brings a new set of constraints. That’s a topic for another blog post!"
  },
  {
    "id": "8463-0",
    "title": "Redesigning our Docs – Part 6 – The processes and logistics of a large scale project",
    "author": "Maxime Locqueville",
    "text": "This is the sixth article in a seven-part series of blogs that describe our most recent changes to the architecture and content of our documentation. We focus here on the challenges of taking on such an ambitious undertaking. \n\nRedesigning our docs took nearly one year to complete, with close to twenty people working on it. We took our time, planned carefully, created a collaborative and engaging work environment, and anticipated the impact on our contributors and customers. All of this played a role in delivering our new docs. \nAnd the first step? Knowing when to take the first step.\nWaiting for the right moment\nLet’s go back one year before the project began. We already knew we needed a change, but we were only two on the doc team—one tech writer and one back-end developer. It would have been impossible. This kind of big revamp needed time and more hands on deck. It had to be done at the right moment.\nBut this didn’t stop us from doing some preliminary work to move us closer to our goal. In parallel to our usual day-to-day activity of maintaining our documentation, we also took small steps to lay the groundwork to a full-scale architectural change,  For example, we started to restructure our how-to and tutorial pages, having in mind that at one point we’ll want this content to be easily accessible from our guides.\nWhen our team grew from 2 to 5, and outside contributors were ready to go, and our backlog of tasks-in-waiting for the redesign reached orange alert—we decided to start the project.\nPlanning\nWhat can be easily underestimated when doing such a big revamp is the planning phase. The full project took us 9 months to complete but the actual work—rewriting the content and coding the new UI - took us only 3 months. \nThat left us 6 months of upfront design. This might seem crazy but it was required in our case. Let’s take a look at why.\nWriting down a first vision\nYou need every member of the docs team to be aligned. You also need the other… contributing teams to understand the context in which they will add their work. Finally you need to explain more globally why you’re doing such a big revamp. For that you need to have something in writing.\nThis vision needs to be a shared effort between all contributing  teams, so that everyone feels involved in the project and understands their impact. If you are not doing that for a project of this size, there is a high chance people won’t want to work on it or will get discouraged quickly.\nIterating on the vision with user research\nAs explained in parts two and three of this series, we conducted extensive user research that allowed us to validate our vision and make better decisions.\nAlso during this time, we iterated on what we wanted the experience to be—designing the information flow, bootstrapping the pages, and outlining the content.\nThis was also a good time to think ahead, to anticipate potential issues. We slowed down the process by spending several months on the design alone. This not only helped us to align, but it ensured that everyone would have the big picture of what we were trying to achieve. The more we moved forward, the more we involved other Algolians for advice and opinion. This created a mindset where a lot of people in the company were aware of the project and supported us in the process.\nPlanning for production\nOnce we knew exactly what we needed to do, we moved to planning. We all agreed that it would be better not to spread the work over too long a period of time, so we time-boxed it to a single quarter. We split the work between what was necessary for the release and what could be done in later iterations.\nThe amount of work required was huge and the number of people needed for contributions was huge as well. We took a week-by-week approach, where we sprinkled the different tasks over the full 3-month target period. This kind of planning impacted all of the contributors and played a critical role in managing dependencies and… deadlines.\nSplitting the project into smaller sub-projects\nEvery team would not participate in all parts of the project. Aligning the different parts was therefore important to the overall goal. Just knowing the end goal was not enough; every team needed to know precisely the scope of their responsibilities.\nHere’s how we split the work:\n\nResearch (Design + Docs team)\nRewrites of guides, tutorials, and how-to pages (Docs team)\nCode snippets for guides in all languages (Docs team, API Client team)\nWeb (InstantSearch web team)\nMobile (InstantSearch mobile team)\nWidget showcase (Design + Docs team)\nRedesign (Design + Docs team)\n\nThis was just a first split. By doing this, we were able to know which team was responsible for which part. Depending on the team and the sub-project it was sometimes necessary to split the work even more, enumerating individual responsibilities. Some teams did this split on their own (verbally) and that was sufficient. Other teams needed to write down the individual tasks.\nHow Formal should the process be?\nMy advice is to put in place a formal process only when needed. If a team is able to and wants to organize itself better, give them that autonomy and flexibility. But if things go wrong, you need to react quickly and put in processes to get back on track. This can be done in several ways:\n\nMore detailed planning.\nAlignment Meetings.\n\nThis is project management. We sometimes as a startup want to do things without process, without planning; however, for a large project, this is just not possible. From my experience at Algolia, as soon as you have more than 4 or 5 people working on the same project, you need a plan.\nManaging Externalities\nAt Algolia we have a documentation team, but everyone is able to contribute to the docs, fix issues, update content. A lot of people contribute to our docs, developers as well as non-developers. Our support team members are also big contributors. So we needed to make sure that this project did not interrupt… these important forms of contributions.\nMitigating Impact on documentation contributions\nPrior to the redesign, our docs were composed of several parts:\n\nGuides\nTutorials\nAPI clients\nREST APIs\nFrameworks integrations\n\nOnly the first two parts (Guides and tutorials) were going to be impacted by our revamp. Us redoing two parts should not impact the work of these people. So we decided that for as long a possible we wanted:\n\nTo always allow contributions.\nTo be able to deploy these contributions to production,\n\nModifying our repo\nOur website codebase and content are hosted on a single private github repo: algolia/doc.\nIf we schematize the structure of the repo it would look something like this:\n\nWe knew that the redesign would change this basic structure.  We therefore introduced a new folder doc2/ that was not deployed in production but available in development mode, which we could preview in our github Pull Requests.\n\nThis allowed us to work on the new content, without impacting the current docs.\nWe were also able to fix things and add new features on the soon-to-be-removed parts and deploy them to production in parallel.\nThere was one downside to this approach. For any change or fix that we did under /doc/guides or /doc/tutorials, we needed to integrate it somewhere into the new content. This can be easy if you have only a few people contributing, but in our case, with so many contributors, it was a challenge. What saved us was the fact that the docs team were the ones approving and merging changes to the repo. The docs team therefore was able to take full responsibility of making sure the duplication choices were done well.\nWe took a similar approach for working on our new design. We duplicated templates, layouts, and stylesheets.\nFreezing deployment (but not contribution)\nWhen we arrived at the end of the project, we knew we had to freeze deployment in order to finalize everything and remove the old content. Doing such a freeze impacted the full company, so we… needed to plan it in advance and communicate our timeline. There are a few recommendations I would give when doing such things.\n\nCommunicate early.\nGive people time to push things before the freeze starts to avoid issues and potential frustration.\nTell people when the freeze is going to end.\nBefore freezing, make sure everybody involved in the deployment is clear on their role and has dedicated time to work on it (because the freeze is not something you want to extend). More on that later.\nCommunicate again when you’re done.\n\nFreezing deployment did not stop contributions. Everyone was still able to contribute. This is important. The success of our documentation relies on our contributors. If you start telling motivated contributors to stop pushing changes, you threaten to lose their reflex in doing so.\nImpact on our customers\nWe knew that restructuring the docs was going to require our audience to change their habits. Worse, some customers who were currently implementing their first solution were going to be shocked and get lost in the information shuffle.\nAs much as possible we wanted to reduce this potential friction. What we decided was to continue to make available the old version of the docs . We did this for one month to ease the transition.\nWe realized later that we could have done better. We could have done the inverse: keep the old docs for a month, but add a banner “try the new docs”. This would have allowed customers to know that something was going to change and to let them adapt to the new design at their own pace. Doing this could also have given us an opportunity to get early feedback from our customers.\nRole of the Engineering and Product Managers\nAt Algolia, each team has one product manager and one engineering manager. One of their missions is to ensure that everyone is aligned. These two persons need to drive the project and make sure it gets completed in a reasonable amount of time. A lot of what we did to lead the project was based on… instinct, and so very unique to the context. But there are some techniques worth mentioning that could be applied to other projects.\nDedicated time to work together\nAt some key moment, we decided to group everyone together in a room. For many developers, writing documentation and code snippets is not very enjoyable, and so doing these tasks together is far better than doing it alone.\nThis also helps with managing time. Every team has their own priorities, so aligning different teams can be challenging. By booking time in advance for all involved, we managed to have everyone working on key parts of the project and create momentum.\nDon’t add time, reduce scope\nFor a big project like this, you don’t want to plan for three months of work and in the end extend that to six months. The strategy that we decided to follow therefore was to reduce scope instead of extend time. As soon as we saw something that would take too long and could delay the release, we asked ourselves the following questions:\n\nIs everything in this task needed for the release?\nCan we do a big part fast and save the rest for a later iteration?\n\nIn most situations, we were able to find compromises that allowed us to ship one part faster, with a bit less features or content. This kind of re-scoping put us back on track.\nEnsure focus\nWhen there are a lot of things to do, you want everyone to focus only on what they need to. Nothing more, nothing less. Every person needs to be accountable for their part. As soon as you have overlaps you are losing time and potentially delaying the product. When you are in “production mode”, this is not something you can do, you need somebody to have the overview. For us it was the PM and engineering manager.\nGet your hands dirty\nOnce the team is in building mode, you don’t want to change their focus. So you need to help on all the small tasks for the team. Handling support, fixing small issues, reviewing the work, coding small features that would allow the team… to focus on bigger tasks. Be available, on hand.\nYou also want to make sure that if there is anything you can do to increase focus, you should do that. This can be done by:\n\nCreating a script for a task.\nPutting in place a process.\nUnlocking discussions/decisions.\n\nMake Decisions\nFinally, making a decision on something takes up brain time. If you have to focus on a task, you don’t want to have to think about making decisions for anything other than the topic you are currently working on. Our goal as managers was to make sure that we freed up time by being accountable on all tasks and teams, by involving the necessary person for each. This can only work if the teams trust you to make the right decisions for the project.\nTrust was something we built up with time. Good decisions, as well as some good old-fashioned values like grit, care, candor, and humility, helped build that trust. \nWith this experience behind us, we are now ready to take on the next big project—which is what our final blog in this series will discuss."
  },
  {
    "id": "8425-0",
    "title": "Deep Diving into Vue InstantSearch Version 2",
    "author": "Haroen Viaene",
    "text": "This post was made collaboratively with Bram Adams.\nIn the previous post we announced the release of our 2.0 library for Vue Instant Search. That post introduced us to the new features included in the release. This time we'd like to take a deeper look into how we updated the library, and why we made the decisions we did.\nWhen your legs don't work like they used to 🎶\nWe've learned a lot since our first Vue InstantSearch release back in 2017. The library began to fall behind our more up-to-date InstantSearch.js library, and we were able to take many of the lessons that we learned and apply them to Vue.\nThe original version of Vue InstantSearch started as a small hack by Raymond. We were very excited about the trajectory of Vue and wanted to be able to release a library that would make it easier for Algolia users to be able to integrate it into their frameworks.\nThe first iteration of our Vue library had a few problems. The main issue was that we built it in the dark! Well, I mean, not really, our computers were backlit. It was dark in the sense that we built the library without much community input.\nThe other major issue is that a lot of our components lacked customizability, requiring people to often copy our components completely to be able to edit them sufficiently.\nBy acknowledging our weaknesses, we were able to build a much better library. We hope you'll like it too. If you have used the previous version of Vue InstantSearch, and are wondering which new features were added, check out the previous post.\nMiro, Miro on the wall, who's the most abstract of them all?\nJoan Miro – Portrait IV (1938)\nWhen it comes to abstractions, Vue InstantSearch now has you covered. We wanted to extend the customizability of components, to let the end user (that's you!) decide how their Algolia experience should look and feel.\nIn the following paragraphs, we will discuss how to customize InstantSearch components. Each layer is more customizable than the last, allowing you to… decide if you'd like an easy-to-integrate already-built component, or have complete control over the look, feel, and functionality.\nImagine the example of a search box. This might seem like a simple component, but it's actually one of our more complex widgets, and supports all these different levels of customization.\nThe default ais-search-box component\nText as a prop\nOne of the easiest to modify parts of a search box is the . This is the text that will be visible in the input element itself. We specifically chose to forward this prop to the inner input, since we wrap the input in a form (see more on why we architect our search box like this in the detailed blog post about SearchBox).\nWhen a prop gets forwarded to an inner component, it always stays a prop on your outer component, either with the same name, or with a consistent prefix indicating which of the inner elements this gets applied to.\n\nChild components as slots\nWithin the search box, we also have a button with an icon that lets a user clear the query. At this point, not only would a string be valid, so would any Vue component. Since it doesn't need access to any of the states depending on its render, it's a regular slot.\nYou can read more about Vue slots here if you're unfamiliar.\n\nRewriting the full DOM as scoped slot\nAs a final touch, we also add a scoped slot just inside the root of the component. To that slot we provide the information we retrieve from the widget's data layer (). Anything within this template now has access to the , , and everything else needed to render that component.\nThanks to the  we now can easily rewrite the look of a widget without ever having to leave the template.\n\nWriting an all-new Vue component\nOccasionally, the level of control  provides still isn't enough for what you're trying to build. This can be the case for two different reasons: needing access to the scope provided by the underlying business logic () in component life cycles, or writing a custom connector yourself.… We made this process simpler by abstracting away the logic to create a \"widget\" from a connector and registering it to its root component with the  mixin.\nLet's say that you would like to render your own search box (instead of ). We can use the  to fetch the data and  to directly insert the Algolia data into our template. From here, the sky is the limit. You can make your component look anyway you want! The reason we can do this is that the data is populated to the  key during the  hook. This allows you to be in full control over both the render logic and the transforming business logic.\n\nIn this component there isn't much use case for going for a completely custom widget. However, for wrapping more advanced third-party components, or for modifying the way the  works, this is a very useful API.\nBut let's take a look at why we might want to split the logic in the first place.\nIs it irrational to split logic?\nWith Vue InstantSearch 2 we chose to split the concerns between rendering components and connectors responsible for business logic. This is useful when you are creating your own custom components and want to use data from your Algolia results. By separating business logic from the components themselves, a component which is conceptually the same as another existing one doesn't need to have their business logic rewritten from scratch.\nAn abstraction should be able to fit any use case with the same conceptual idea\nSplitting logic requires finding a balance. You can't separate it too much, because it becomes tedious to build all the boilerplate required for different flavors and frameworks. However, you can't have it too view-specific either, because then you lose the flexibility to add new components in the future. A rule of thumb we used here is that any connector should be able to fit into all rendered components with the same conceptual idea.\nAn example here is a menu (a list with only one selection possible) which is showing as a list, or a menu which is… showing as a dropdown. If in a business logic abstraction you put code dealing with the selection logic, it would overstep its bounds and you would be more likely to make an abstraction for each case.\nThe choice at that point is deciding whether the logic is easy enough to be rewritten in the view layer (using things like event handlers), and thus making only a single business logic abstraction. On the other hand, sometimes an idea turns out to be two completely different abstractions.\nTwo completely different abstractions in this context would be Hits (i.e. results) and InfiniteHits. Both of these read from the results of the query to display the ones that currently apply, but the infinite hits abstraction would contain things like keeping the previous hits in scope so that a concatenation of all visible hits can be done.\nConsistency Between Business Logic Abstractions\nAlgolia prides itself in having the best developer experience possible. To achieve this goal, we focused on building a consistent foundation between our different JavaScript flavors. These connectors work whether you're in Vue, React, or even vanilla JavaScript. If your business case is outside of what our widgets can handle, we encourage you to check them out!\nThe move to a consistent abstraction here has proven very useful in making libraries like Vue InstantSearch. We only need to account for one type of API (using a ) in a Vue component to make it aware of the business logic (reading from state, as well as changing refinement state) consistently.\nIf we had chosen to split this up in functions, so that each would have its own API, it would have made those functions individually simpler—but harder to work together. The goal here is to make the view layers as simple as possible, to make adding new framework flavors easier.\nConclusion\nWe hope you enjoyed this look into the internals of our new Vue library, and we hope you enjoy the library as much as we do."
  },
  {
    "id": "8367-0",
    "title": "Redesigning Our Docs – Part 5 – Building an Interactive InstantSearch Showcase",
    "author": "Sarah Dayan",
    "text": "This is the fifth article in a seven-part series of blogs that describe our most recent changes to the architecture and content of our documentation. We focus here on CSS architecture and improving the way we deal with assets.\nA search interface is like a jigsaw puzzle. It's composed of many small parts that, once put together, make a coherent whole. When building a search interface, you're assembling the pieces and designing the puzzle. There's no one-size-fits-all solution, only countless possibilities to fulfill unique use cases.\nThe pieces that compose a search interface\nInstantSearch is a family of front-end libraries that provide the building blocks to help you build search interfaces. It gives you infinite possibilities to build the ideal search experience you’ve imagined for your users. In many cases, what you need comes right out of the box, but you can also customize existing widgets or build your own.\nYet, how do you 10x on DX—the developer experience—when documenting such a library?\nThe challenges of documenting visual components\nWritten documentation works wonders for renderless software such as API clients or REST APIs. In such cases, meaningful content, handy examples, and a powerful search go a long way.\nDocumenting UI widgets is another story. When you're building a user interface, the thought process radically changes. You're no longer thinking about the data you have or what method you need to perform a task. Instead, you're pondering about what your search interface should look like, and how your end users can interact with it. Maybe you even have a mockup ready and you're trying to find the right widget to implement it. In this situation, written documentation falls short. No matter how well you describe a visual component, nothing beats seeing it in action.\nWe wanted to reverse the traditional documentation pattern for InstantSearch. Instead of letting users search for what they need through guides and long lists of widgets, we thought… it would be more helpful to give them an immediate, visual idea of what they can do. We wanted to show widgets in action in a real search interface, and let them access the documentation from there.\nSo, we built a fully working, interactive showcase of the InstantSearch widgets, and integrated it right in the documentation.\n\nUI/UX Process\nDesigning the showcase was a challenge. Nicolas Meuzard, our product designer, had complex specifications to start from. We wanted to display the full library, show each widget's possible variations, and redirect to their respective documentation pages. However, it also needed to have a cohesive feel: we wanted the showcase to look like an actual search interface. It was important so users could project themselves and quickly know what they need.\nInstantSearch is a rich library, so we decided to break down the showcase into three different experiences: a regular full search interface, an autocomplete widget, and a geo search experience.\nAfter several iterations, Nicolas came up with a design that seamlessly integrates into the docs. It lets you switch between three views, and displays each widget (or group of widgets) as dotted blocks. When you hover any of them, you reveal a link that redirects you to their documentation.\nBuilding the showcase\nThe InstantSearch showcase isn't the first interactive tool we've built for the docs. Before that, we developed a dynamic filters syntax validator and an interactive tutorial. Both times, we've assessed it's a much better idea to keep these tools separate, in their own repositories, and find a way to integrate them in the docs, instead of shoving everything in the documentation repository.\nFor the showcase, we decided to go with Vue.js as we did for the interactive tutorial. We serve the app on a separate S3 bucket and integrate it into the docs website using an .\nThe implementation itself was quite straight-forward. We use Vue InstantSearch to display widgets, which does most of the heavy… lifting for us. However, it did come with a few interesting challenges.\nOne showcase for all\nInstantSearch for web comes in four flavors: vanilla JavaScript, React, Vue, and Angular. Each of them has specificities. For example, InstantSearch.js and React InstantSearch come with a geo search widget, while Vue and Angular InstantSearch don't. Therefore, whenever users visit the showcase for vanilla JS or React, it should point to the geo search widget documentation. When visiting Vue or Angular, it should lead to our guides on how to build your own geo search component.\nIt was clear to us that we should build and use a unique showcase for all InstantSearch flavors. This is easier to manage and maintain. However, we also needed to take those specificities into account without cluttering the code.\nWe have a relatively low number of different flavors, and they don't diverge too much, so we decided not to over-engineer and went the simple path. Since we embed the showcase in the documentation with an , we also pass the current InstantSearch flavor as a query parameter. If you're visiting the showcase from , the showcase, therefore, receives \"react\" as the current flavor, which allows it to apply conditional logic.\nHandling conflicting widgets\nSome InstantSearch widgets serve similar purposes, so we needed to find a way to display them all to users without cluttering the UI. For this, Nicolas came up with a switcher pattern. We group all similar widgets into the same block, we display all titles, and we let users click on them to switch.\nYet, sometimes, these widgets have incompatible behaviors. This means you can't have them all in the same UI at the same time.\nWe solved that issue by using dynamic components, a Vue.js feature that lets you load components on the fly, at runtime. We're only showing a single widget at a time, so instead of showing it and hiding the others, we can leverage dynamic components to swap between widgets by destroying the former and mounting the… new one.\nGuessing the height\nOne of the challenges with using an  is how to handle content with variable height gracefully. In our case, the showcase loads dynamic content (search results) and therefore changes height when you perform a search, load more hits, switch to another experience, etc. Unfortunately, from the outside, it's impossible to detect that the height has changed.\nWe circumvented this issue by leveraging the  method, which lets us establish communication between the docs and the showcase. Vue.js provides an  lifecycle hook which fires every time the DOM re-renders. We use this hook to emit the new height of the body whenever the showcase changes. The website listens for new messages and updates the height of the  whenever it receives a new height.\n10x DX\nThe showcase is a direct corollary of the InstantSearch widgets list but visually. It helps discover InstantSearch without feeling overwhelmed by the amount of choice. It also helps users being more productive by not having to browse widgets to find what they're looking for. Funny enough, several of Algolia's solutions engineers and sales representatives now use the showcase when pitching InstantSearch to prospects 🙂\nOf course, none of this could have happened without robust processes in place. This is what our next article covers, so stay tuned!"
  },
  {
    "id": "8378-0",
    "title": "A Tale of Two Engines - Algolia & Unity",
    "author": "Antoine Hemery",
    "text": "A search engine inside a game engine? It’s an idea that took shape just before one of Algolia’s off-sprint days, an event where Algolians experiment with new concepts. \nDuring the previous off-sprint, my colleague Samuel Bodin and I were thinking: Algolia has a .NET API Client, and Unity supports C# as a scripting language–Why not try to bring them together?\nOur idea was to implement a search-as-you-type experience inside a Unity game scene. We decided to create a marketplace, a common use case for searching within a game.\nThe result? Success. Except for a small challenge with performance, which was solved by switching into asynchronous mode, at the end of the day we had a fully functioning Algolia search within Unity, complete with an index of game assets and images, and a full search and results UI. Let’s see how this was done!\nUsing Algolia with Unity. Getting started!\nUnity does not support the .NET package manager NuGet. To address this, you have to download the zip package of the Algolia library directly on nuget.org, and then unzip the contents in the Assets/Plugins folder of your Unity Project. \nAfter that step, you have to add the  statement in a C# script to start using Algolia with Unity. \nNow that we have Algolia plugged into Unity, the next step is to create the game scene. \nFinding Assets\nOur first task was to find game assets and a dataset to craft our game scene. \nWe found this excellent starter kit from Unity and used it as a foundation for our experiment. The good thing is, it was packed with a lot of UI elements–we just had to reuse them to create the marketplace scene and add a menu to access it!\nCreating a dataset of planets\nBefore digging into the game design, we had to create a dataset. Since science-fiction was the theme of the assets, we decided to create a dataset composed of planets. We created a thousand fake planets with a powerful dataset generator, Mockaroo. We then used CC images to illustrate the dataset. Once that was… done, we were ready to go! If you want to reuse the data you can find it on GitHub.\nCrafting the marketplace\nThe final and most challenging step was to create the game scene for the marketplace. We wanted it to be simple: it would be composed of a search bar and a panel displaying the results. To achieve this, we took the controls scene from the starter kit and customized it. The result—a panel for the planets—looked like this:\n\nAfter scaffolding the scene, a little bit of work was required to bring it to life. We needed to script some C# to turn the empty box into planets. \nScripting the as-you-type search\nBefore sending requests to Algolia, we needed to catch every keystroke the user was typing. To achieve this, we attached a C# script to the InputField to catch the ValueChangeCheck() event. All good, we were able to retrieve the input keystrokes! Making a request to Algolia with the retrieved string was as simple as:\n\nNote: we had already instantiated the AlgoliaClient and the Index object in the Start() method to avoid creating a client for each request.\nWe got the search working, and Algolia's C# API returned a list of planets (List). Now we had to display them. We had to create GameObjects.\nWe wrote the method, which creates game objects from List. You can find the source code on GitHub. Here we will point out only a few parts. In short, the method loops on the planets to create game objects, one for each planet. \nPerfect.\n\nNo, not perfect. Not yet: the display of planets was too slow, and the image refreshes were choppy.  \nAsynchronicity \nOur first idea was to create two game objects per planet: a RawImage and a Text, and then to do some computations to position them in the scene. The search experience was OK, but it was a bit laggy.\nSo we looked into this and discovered a flaw in how we displayed the planets. We were loading the planets’ textures synchronously, and that’s what made it seem slow. The solution to make the search experience more… fluid was to load the planets’ textures asynchronously. To achieve this, we used the  method from Unity in our LoadTexture\n\nYou can see the difference in the video below:\n\nPerfect! But not done. While the search experience was fast enough for our POC, we had not yet loaded the scene from the main menu.\nLoading the scene from the main menu\nTo close the loop, we created a button in the main menu to access the marketplace. We attached a LoadSceneOnClick script onto the  event of this button. This script, as its name implies, loads a scene with its index after the button is clicked: \n\nWe now have all the pieces in place, so let’s see the result!\n\nAre we done?\nIn one day, we created a simple marketplace in Unity with a search powered by Algolia's C# API client. And we are not finished! One interesting direction is to try the same with other game engines; for example, another project would be to reproduce this integration with CryEngine, which also supports C# as a scripting language!\nBut the real next step is to add all of Algolia’s features into the solution. There are still a lot of possibilities for us to explore: creating a search in an inventory in MMO games, adding a UI to search servers, adding facets and filtering, deepening the dataset, adding analytics and personalization, etc. Essentially, as your game increases in complexity, your marketplace, and any search-based feature in Unity, will need to follow suit. Happy gaming! \nWritten in collaboration with Samuel Bodin."
  },
  {
    "id": "8349-0",
    "title": "Redesigning Our Docs – Part 4 – Building a Scalable CSS Architecture",
    "author": "Sarah Dayan",
    "text": "This is the fourth article in a seven-part series of blogs that describe our most recent changes to the architecture and content of our documentation. We focus here on CSS architecture and improving the way we deal with assets.\nWhen working on a documentation website, it's easy to focus solely on content and to put the rest on the back burner. This is especially true for the Algolia docs: over the years, it has undergone some dramatic changes. With every new feature, API client, and product, the website has had to quickly expand and transform to cater to new needs.\nKeeping up with the growth of Algolia has also meant going through many redesigns, often by many different people. It contributed to a front-end codebase that became increasingly hard to maintain. Therefore, when we decided to redesign the website at the end of 2018, we deemed this was a great time to take a hard look at the state of our CSS. It had become increasingly difficult to change something without breaking something else. A simple change could turn into an hour of keeping duplicates up to date and battling media queries. We were no longer in control, and every edit felt like adding a band-aid on top of a bunch of others.\nWe therefore decided that our docs redesign was an excellent opportunity for us to wipe the slate clean and start over. After all, we were getting a brand new design, so it made sense to rewrite our entire CSS codebase and define an unambiguous and robust methodology that could keep up with our growth for the next few years.\nPinpointing pain points\nCSS is notoriously hard to scale. It offers powerful tools, but making mistakes doesn't forgive easily. Global scope and specificity are good examples: when mastered and used purposefully, it can do wonders. When used reactively, problems start piling up, and it becomes tough to opt out of them.\nIn our CSS, we used to have much reactive specificity. Instead of using it with control, we used it to fix issues.\n\nThe above snippet is a… typical example of a specificity battle stemming from a wish to apply rules globally, then finding exceptions. The problem with these kinds of overly-qualified rules with many descendants is that not only is it fragile and hard to maintain by hand, but it can also become costlier in terms of performance, as it takes longer for the browser to match.\nAnother underlying issue which comes with writing reactive CSS is that your codebase keeps on growing. As you add new components that look slightly different from existing ones, or keep patching up issues to fix earlier misguided decisions, assets become increasingly heavier.\nWhen we audited our CSS, the conclusion was beyond dispute; the cause of our issues was that we had no visible CSS architecture. To be exact, we used a mix of several CSS methodologies. Over the years, successive contributors had sprinkled some globally applied styles, some component-oriented CSS (OOCSS), some utility-first CSS, and plenty of hacks and overrides. While all contributors had the best of intentions, unfortunately, without clear guidelines and no assigned front-end owner, their contributions quickly resulted in tangled CSS with repetitive and unused classes, side effects that were hard to track, and a codebase that grew every time we added something new.\nIt was time for us to set up a proper architecture, enforce conventions, and to start treating CSS as a first-class citizen.\nA composable approach\nAfter assessing several popular methodologies (including pure, Bootstrap-like OOCSS, BEM, etc.), we decided to go with utility-first CSS, a methodology which encourages composition through the use of atomic classes, and lets you abstract into components when necessary. We integrated it in a loose version of the ITCSS architecture.\nAmong the things that seduced about utility-first, we enjoyed that it pairs well with branding guidelines. Utility-first is designed to directly map onto stylistic rules instead of letting you write rulesets… freehand, without any framework.\nA strong foundation\nInstead of manually maintaining a raw stylesheet of atomic styles, we decided to go with Tailwind CSS, a utility-first framework that generates CSS classes from a JavaScript manifest file. Tailwind has great documentation, an active community, and is compression-friendly (more on this later).\nOne of the advantages of computing CSS instead of maintaining it by hand is that it works wonders with design systems. We get to use the full power of JavaScript to generate styles based logically on predetermined rules. This method is much more manageable and time-effective than maintaining rulesets by hand. Need more spacing utilities? No need to manually write them one by one. Simply increase the counter.\n\nWe also get a configuration file that we can share across projects. This is extremely useful, as the documentation scope includes several separate projects (like the interactive tutorial or the widget showcase). Being able to reuse rules and cherry pick what we need is far easier to do with a JavaScript configuration file than with a bunch of scattered CSS files.\nControlled file size\nOnline documentation should load quickly. When building something, developers often refer back and forth to docs for small bits of information. What was the name of that parameter again? And the return type of this method? These are questions for which they need fast answers, without ever going out of their zone.\nBrowsing online documentation should be a seamless experience. At Algolia, we've built our documentation for speed (statically-generated pages, Algolia-based search) but there was one area that we had neglected so far: assets size.\nSteady size\nUtility-first is a great way to keep your CSS steadily small. Because we're only reusing atomic classes instead of creating new rules, we don't introduce new CSS code that may grow the assets. Even when we introduce pages with a different layout and new components, as long as it follows our… existing UI system, the CSS doesn't grow.\nSmaller browser memory footprint\nAnother perk of going utility-first is that the browser doesn't have to work as much to resolve styles. Utility-first CSS enforces using classes only (no matching on tag names) and keeps specificity low (most of the time, utility-classes are one level deep). It reduces (if not eliminates) style overrides, which reduce the overhead of determining the final styles for a given DOM element.\nAutomated purge\nIn our case, we're using TailwindCSS to generate our atomic classes. These are based on our brand colors, the grid size we've selected, the number of variations we need, etc. As you can guess, this can generate many classes, most of which we don't need.\nFor that reason, we've set up Purgecss to automatically strip out any class we don't need in production. This tool allows us to have access to the full catalog of available classes in development mode but filters out all unwanted bits from the production build.\nWhat about HTML bloat?\nKeeping CSS small is great, but what about HTML? Aren't we just moving the bloat from CSS to HTML files? This is the primary concern I usually hear when advocating for utility-first CSS.\nIt’s important to keep in mind that CSS and HTML are different. Compression algorithms like Gzip and Brotli beautifully handle repeated class names, as they're themselves based on algorithms that are specifically designed to compress duplicate strings. The resulting file size for HTML documents makes little or no difference whether you use a few or many classes.\nContent-friendly CSS\nAtomic classes are great, but what about user-generated content? We can't ask our technical writers, nor our contributors from other squads, to add utility classes to the content they write. It is neither user-friendly nor maintainable, and it would also significantly hurt content readability during code reviews.\nFor this specific use case, we needed to step away from utility-first without creating new… problems, nor allow breaking out from the systems we had put in place.\nPreventing style leaks\nTechnical writers and content contributors write in Markdown, which then compiles to plain HTML. Therefore, we needed to have styles that apply directly to HTML tags, without them leaking on other areas of the website.\nSince CSS applies globally, the only way for us to scope rules was to use specificity as a way to namespace all CSS rules for content. In our Haml templates, we added a specific CSS class on the containers that surround outputs from Markdown file and used this class in our CSS to contain rulesets that would otherwise apply much widely.\nHere's what a Markdown content file looks like:\n\nAnd how we render it, in a Haml layout:\n\nThese compile down to the following HTML code:\n\nThen, we can style the content safely (here in Sass):\n\nThis technique works well, but has one downside. Since it relies on specificity, each ruleset scores higher than any one-level deep utility class. In the above example, a generated ruleset  has a specificity score of  while a utility class  has a score of . If we wanted to override the margin of a specific  element with a utility class, specificity would get in our way.\nTailwind lets you circumvent this issue by allowing you to make all utility classes !important. However, keep in mind that this is an aggressive option that you need to manipulate with the uppermost caution.\nIn the future, we may look into CSS Modules for this part, so we can namespace class names directly without relying on specificity. Going this route would involve making it work with Markdown in a Haml/ERB context, as well as making them play nice with TailwindCSS.\nSingle source of truth\nWhen writing scoped CSS rules by hand, it becomes easy to derail from the framework that utility classes impose, and to duplicate code. Fortunately, this is where one of the best Tailwind features comes in handy.\nTailwind provides an  directive, which, contrary to , mixins, or… placeholders in Sass, copies the CSS declarations from the reference class into a new ruleset. If you're familiar with Less,  works like Less mixins: it uses actual classes that you can apply directly, and copies all their declarations into a destination ruleset.\nTake the following class:\n\nInstead of copying its content into new rulesets, we use  to reuse it:\n\nThis compiles down to:\n\nWhenever we need to write custom CSS, we reuse utility classes via  instead of writing new rules. It forces us not to derail from our design system and end up with inconsistent and exotic styling.\nUtility-first, second to none\nWe've had great success with going utility-first in the Algolia documentation, reducing our CSS from over 125 KB (and continuously growing) to less than 50 KB (9 KB GZipped, 7 KB with Brotli compression), which represents a 60% size decrease! From a user perspective, this is a guarantee of lightning-fast loading styles with extremely low overhead.\nFrom a project perspective, our new CSS methodology has allowed us to achieve more consistency in the way we style things. It also lets us keep our styles aligned with the Algolia branding guidelines and the UI rules that our product designer has set out for us.\nAt Algolia, we consider documentation to be a fully-fledged product in and of itself, not only a byproduct of our APIs, clients, and libraries. For this reason, we strive to ensure that we don't neglect any aspect of the website. This new CSS architecture is in line with our goals: continually improve developer experience (DX) and build scalable, sustainable systems.\nA new CSS architecture isn’t the only thing that happened on the front end of our docs. Our next article deep dives into the making of the new interactive showcase we built to document InstantSearch."
  },
  {
    "id": "8325-0",
    "title": "One Year of Show & Tell",
    "author": "Rémy-Christophe Schermesser",
    "text": "When I joined Algolia, we were 25. We had a weekly meeting of the whole company, in which everyone could speak up and share what they did during the week. The communication was smooth, and everyone knew what everyone else was working on.\nFour years later, we are 350. It has become more difficult - even in the engineering team - to share knowledge among team members. At Algolia, we firmly believe that the best way for a company to grow is by learning from each other. Therefore, we need to be able to share knowledge proactively.\nThat is why, more than a year ago, we started “Show & Tell”, our weekly session of knowledge sharing. During Show & Tell, people do live presentations on stage in front of their fellow employees on an Algolia-related topic. Let’s deep dive into the first year of this event.\nWhy\nBefore going into details on the how of Show & Tell, let’s see why we decided to create this event. Sure, knowledge sharing is key to the success of our company, and that’s the primary reason why we wanted to do this. But it’s not the only one.\nAlgolia has had exponential growth. When we started Show & Tell, we were 150 employees; today, a little more than a year later, we are 350. This makes it a lot harder to know and meet everyone. Speaking in front of the company is an excellent way to make yourself known!\nOne positive side effect is that people start to identify you as an expert on a specific topic. This allows you to share skills across the company and lets people know they can go to you if they have an issue with a particular technology or topic.\nAlgolia values public speaking skills; it provides a 2-day public speaking workshop for any employee who desires it. Show & Tell offers us an opportunity to put this workshop into practice, helping us hone our public speaking skills in a safe environment, where we speak only in front of our colleagues.\nHow\nTo provide this safe opportunity to get to know each other and practice public speaking skills, we… needed to make sure the event was well organized.\nSince the beginning, we decided to go for a weekly, 30-minute format. This is short enough not to impact daily work and long enough for two or three presentations. We decided to schedule it just before “Happy Hour” - our less formal weekly get-to-know-each-other event, where we share a relaxed time with our coworkers. As we are across 5 time zones, with 10 hours of time difference, we wanted it to be during most of the employees’ office hours. Hence we decided to go for 4:30 pm GMT, which is the beginning of the workday in San Francisco, the middle for Atlanta and New York, and the end for London, Paris, Lyon, Nantes, and Prague. We wanted to make it easy for everyone to attend.\nWe started small. In the beginning, there were 3 presentations, 10 minutes each. Speakers were assigned slots and were given only a few days to prepare. We quickly realized that 30 minutes of presentation easily turns into 45 minutes when you add questions and answers.\nDuring this year we learned and improved the organization of the event. Let’s look at the actual anatomy of our “Show & Tell” event, S&T for those in the know.\nAnatomy of Show & Tell\nThe organization of S&T is split into 4 parts:\n1. Picking topics\n2. Preparing the evening\n3. Running the show\n4. Post-show follow ups\nPicking topics\nBefore presenting anything, we need to select the topics. Since the beginning, we decided to only choose Algolia-related subjects, such as projects done for Algolia, new features, new tools, or rehearsals for presentations that would be given outside of the company. We now select two topics instead of three. The only constraint is 10 minutes, so that each speaker has enough time to answer questions.\nWe also encourage new employees to speak about their onboarding project, and we include non-technical talks by our marketing, sales, and recruiting coworkers.\nWe now pick topics a month in advance, to give the speakers time to prepare. This… enabled us to more easily replace last-minute cancellations, which only occurred twice on more than 120 presentations.\nPreparing the evening\nTo further prevent last-minute cancellations, we notify the speaker two weeks before their presentation. One week before, we send a template of slides with some idea of a plan. It’s not mandatory to follow it, but often it helps the speaker to structure their presentation.\nWe also encourage speakers to rehearse, by providing them time slots when they can come and do it in front of the Show & Tell team. This way they can get feedback beforehand, and improve their slides and public speaking skills.\nWe are big Slack users; a few hours before S&T, we send a reminder on the #algolia channel with the program of the day, so employees can decide whether they want to attend or not.\nRunning the show\nA few minutes before the actual event, we prepare the room where the presentations take place. We also live stream it for the whole company, which requires a bit of preparation. We record the live stream so those unable to attend can view it later on.\nEach week we have a Master of Ceremony who runs S&T. Their role is to start the show, present the speakers, close the session, and introduce speakers for the following week.\nPost-show follow ups\nOn the day after the event, we put the recording and slides in our internal learning platform, where anyone can access the previous presentations. We also send a recap email to the whole company with links to the learning platform, as well as the schedule for the following week.\nTips & Tricks\nDuring this year of organizing such an event, we learned some tricks from the trenches.\nThe most important is to celebrate the speakers, who, every week, present a topic in front of the whole company. This event wouldn’t be anything without their dedication. Speakers rarely come forward and propose topics on their own; most of them think they have “nothing to share”, or that “it won’t be interesting”.… Therefore, it’s essential to identify who can talk, and then proactively push them, just a little, to convince them to speak. This is why it is necessary to fetch information from technical leaders and managers because they have a better vision on which topics are trending, and who can best represent them from their team. Additionally, keeping an eye open to what happens in the company can help us find speakers. We maintain a list of who spoke, so we can go and find topics from people who never did. Having a backlog of potential presentations can also help fill an empty slot.\nWe are quite fortunate to have a big room for the presentations, with all the equipment to make it work; however, some of us have organized this kind of event in much smaller companies, with fewer resources. The main point is to do it at your own scale - you can even use your break room, do it at a different pace, etc.\n\nLast but not least, ask for feedback! We do it every six months, and each time we get valuable information on what to improve. For example, this led us to move S&T 30 minutes earlier, so more people could attend.\nWe’re continuing S&T this year. For now, we’re keeping the same format, but will experiment more. We already did lightning talks, a special session on a dedicated topic with four shorter presentations (5 minutes). We received good feedback on this. People said it was more dynamic and required less preparation from the speakers, so we’ll do it again!\nWe only had around 5% of non-technical talks so far; we will thrive to have at least 10% in 2019.\nDuring this year of Show & Tell, we had 70 speakers doing 120 different presentations, with topics that included “Low-level performance in C++”, “How to organize a marketing event?”, “What is tech recruiter’s daily job?”, and “How to write better PHP”. I’m very proud to see the dedication of our speakers, and I’m equally dedicated to celebrating them indefinitely!"
  },
  {
    "id": "8240-0",
    "title": "Redesigning Our Docs - Part 3 - The UX/UI Phase",
    "author": "Nicolas Meuzard",
    "text": "This is the third article in a seven-part series of blogs that describe our most recent changes to the architecture and content of our documentation. We focus here on user testing and the decision-making process behind our UX/UI choices. \n\n\"Start with the problem. And don’t think about the solution until you know the problem you are trying to solve.\"\nThese were the words spoken by Sasha Prokhorova, our Product Designer, as we went about redesigning our docs. \nFor us, it was pretty obvious. While our docs were packed with great information, we were never sure our readers knew what to expect. There was an imperfect blend of text and example code, and we often mixed unrelated information on the same page. Furthermore, we did not provide easy navigation: the words we used in our menus, as well as the way we organized our subjects or displayed hyperlinks and calls to actions, did not always match our readers’ expectations.  \nBy the time Sasha arrived, we had already begun to solve the problem. We had redesigned our docs (at least on paper) to include a new menu structure, a changed hierarchy, and new page layouts to mark more clearly the difference between text and code. Said another way, we adapted our architecture and content to the different purposes of our readers.\nClearly, she had her hands full. She was facing a group of experienced developers and technical writers who had already formulated a clear idea of the problem and its solution. \nWhat was she going to teach us?\nWell, from that first day to the final delivery of her UX design, many surprises were in store for us. In the end, even if our original analysis of the problem and solution did not change dramatically, many additional facts and details came to the surface during the product design phase that forced us to reconsider some of our original assumptions. Which reminds me ...\n\"Don’t act on assumptions,\" she said.\nPlaying cards with the Product Designer\nSo Sasha tested our assumptions,… introducing us to a number of UX tools:\n\nCard sorting helped us improve our new hierarchy and vocabulary. We asked random users to put our main subjects into their own wording and categories. While some choices confirmed our initial intuitions, some choices surprised us.\nTeam workshops gave us time to reflect on the navigation and new page layouts. For example, we spent a full day designing one of our most critical page layouts. Each member of the team proposed a design and then we selected the best aspects of each design and put together a new page layout.\nEarly user research with advanced users gave us exceptional insights into our current documentation - the good and the bad.\nIn-person interviews with external users gave us an unbiased look at our new architecture.\nUnmoderated video user testing with anonymous users who were video-taped as they used our docs and responded to a user survey. This final round of tests provided us with invaluable insights on specific areas of content, offering insights into our vocabulary, content flow, topic approach, and the quality of our code and examples. \n\nAfter six-months of collecting such user feedback, and going back and forth with different proposals, we delivered a final UX design - again, on paper - to our UI designer. The team - exhausted by the leg work as well as by the abstract nature of upfront design - were happy to finally move forward and see this hard work be given a vibrant third dimension by our keen-eyed UI designer.\nThe story thus moves on to the UI phase.\n\nBut... What makes great docs, from a UI perspective?\nUI work isn't only about making things pretty, it's also about questioning the way we interact with any given website, and how visually we can make documentation clear and easy to use, and more natural. So after rewatching the user-interview videos we had recorded during the UX phase, we were able to empathize with our audience. For example, seeing navigation habits like the well-known Cmd/Ctrl-F to… track down pieces of content on a page, or discovering developers’ appetite for simplicity (which springs from their use of not very UI-centric code editors and command-line consoles), convinced us that we needed to directly address these expectations—especially as Algolia is quickly becoming a tech leader.\nDon't try to guess, just ask.\nUI can be approached in the same way as UX. You'll have gut feelings about design choices, relying heavily on well-known UI patterns that you can rightfully assume are good practices. However, every website comes with a specific set of challenges, and in this case, some of them are way too important to ignore by relying purely on assumptions. From being able to switch programming languages and interact with code snippets, to handling versioning, to showcasing libraries made of complex components, …, we needed more information about what works and what doesn't. \nLuckily, developers love to explain why they use a specific service among others, so we tried to leverage that. While benchmarking other online documentation, and annotating a bunch of UI layouts and ideas, we launched a call for feedback on Twitter, which became a golden thread of the most appreciated documentation on the web. The response was phenomenal, we quickly learned what makes for state-of-the-art documentation: in-depth code examples, clear writing, good readability, learning curves, an inline console to interact with, and lots more. Extracting these answers, and tagging every argument, we started to expose trends that gave us clues about what resonates with people when using docs.\n\nHey #developers folks — I&#39;ve got a request for y&#39;all! As we are constantly trying to improve our developer experience here @algolia, we want to empower you with the best docs possible. 👉 Would you share your favorite #documentation, and the WHY behind? 📄💻 pic.twitter.com/frNd19BHQb\n— Nicolas Mzrd (@NicolasMzrd) September 3, 2018\n\nTemplating.\nOur documentation is… a collection of templates written in markdown language which are used to generate static web pages. So in a joint effort with the writing team, we started listing every kind of content we wanted to display, to see what they contain, and to attach high-level objectives and expectations to each one. A bit like Design System builders, you don’t start with a bunch of small components that hopefully work together as a whole; instead, you start with one template, focusing heavily on the objectives that you want to achieve, and making sure to avoid elements that discourage readers from learning. We were sure that, if well-designed, this page would set the global direction for our entire UI.\nTaking the best out of online media.\nDocumentation is meant to host a massive amount of information and to cover complex topics that end up generating long pages with lots of different kinds of information. In that way, it was quite apparent that we needed to follow the standards of the media giants who are continually optimizing their webpages for optimal readability, to make it easier for its readers to consume huge volumes of information.\nTherefore, as a foundation for the entire UI, we decided to start with one of our heaviest pages, which included tons of UI components like long text areas, alerts, warnings, side notes, code snippets, links to additional resources, a feedback form, and videos. The idea was that if we could find a design that worked for this page, it would work for 90% of the docs.\nUsability over aesthetics - some best practices.\n\nUsing a pretty narrow content-width to display short lines of text makes it easier to read than having full-width sentences that get lost in your eyes and tire you when going from one line to the next. Best standards are to keep within 600 and 700 pixels wide. We chose to stay in the middle, keeping an average of 150 characters per line.\nOur technical writers already did an amazing job at splitting their content, giving us small… paragraphs to work with instead of delivering huge walls of text. Most users are fine with scrolling by now, especially developers who are big fans of Cmd/Ctrl-F navigation, and who prefer having everything on a single page. Let your content breathe, make it readable by understanding how your intended audience likes to read. \nTo make long reading sessions comfortable, we went for a font size that is easy on the reader’s eyes. For example, users should not have to zoom in to read, especially not an audience that will skip over your product if it's not up to their standards. Don't let them break your layout because they feel the need to zoom in and out. Usability over aesthetics.\nWe designed with accessibility in mind: do not rely on colors that attract attention; make sure your color scheme is bringing value and comprehension to color-blind people; and finally, check your main contrast ratio to ensure good readability even on low-resolution screens.\n\nAfter dozens of tests, we finally reached a balance that was pleasing to developers and aligned with the branding of our company—thus, delivering a consistent message with the same tone and atmosphere of all our online content.\nThe component showcase challenge.\nOne of our greatest challenges, from a design point of view, was the InstantSearch component showcase. This showcase needed to accomplish several goals: allow users to experiment with the full library of interactive components; serve as a hub for the documentation behind each component; and highlight each component’s possible variations. In a bit more design-speak: it needed to work as a whole, while highlighting each component individually, and still visually make it clear that each component can have several variations. \nHere is what we came up with during our kickoff meeting:\n\nLiterally, boxes in boxes in boxes. Though this first draft was definitely good at displaying the global information hierarchy, there were some challenges. There needed to be a way… to know at first glance that it was a search demo, in order to encourage users to experiment. And it needed to clearly identify each component as a single entity with a precise goal. \nWe’ll spare you all the UI explorations. Essentially, we followed the Information Architecture choices we made during the UX phase, making sure that our design actually fit our audience’s expectations. \nWe see the world through our own eyes: work experiences, education, culture,... We are all biased in the way we perceive a design. To balance this, you need to dig into what your audience uses today to accomplishes its tasks, and to build on top of that. This is supposed to be an iterative process for everyone, not a total disconnect with everything they knew before. In other words, don’t re-invent existing and well-known patterns: build on top of existing ones, while trying to adapt them to your own use-cases, and making them more valuable for the final users—even though you didn’t necessarily invent something on your own.\n \n\nHaving addressed the problem of the whole, we focused on bringing subtle but obvious detail for each component and its variations. Splitting apart each component into individual containers was the first step to visually expose them individually. It was also important to add a small tab menu for each one, enabling users to switch between components and still interact with the demo as a whole. Finally, adding a hover state on each block solved the access-to-docs issue, thus avoiding flooding users with a tsunami of individual links constantly visible for each component.\n\nNext steps: optimization and iteration.\nNow that the new design has been rolled out, it’s all about keeping an eye on our important metrics, making sure the main issues were correctly addressed and that our decisions will impact the business positively.\nWe couldn't say more about how important it is to keep track of all feedback—good or bad—to continuously map the overall… experience and performance of your design. This enables you to know where the strengths and most-used areas are in your UI/UX. Even when the project is done, it is good to know which issues remain, and to have them simmer in the back of your mind as you passively think about solutions. At the end of the day, keeping some sort of customer journey up to date will help you identify and communicate quick wins, and clean up negative perceptions with only a few resources.\nStay tuned for more iterations—we have a lot of exciting changes coming your way! But before we go to the future, let's get technical and see the UI's new performance-driven CSS architecture."
  },
  {
    "id": "8229-0",
    "title": "The Who, What, Where, When, and Whys of Developer Relations",
    "author": "Bram Adams",
    "text": "In September 2018, I was sitting on a park bench in Paris, when a nice old lady sat next to me. The problem is, I don't speak French, and she didn't speak English. We attempted to have a conversation anyway. As she was talking about what I could only assume was how great her grandchildren are, I ask myself: how did I get here?\nToday I am a Developer Advocate. In what feels like a past life already, I was a software engineer. If you'll indulge me, I’ll tell you what I've learned these past few months and share some info that may be helpful to you.\nWho is Developer Relations?\nYou! Me! Everyone! That's the thing about Developer Relations that caught me off guard. Anyone can be involved in the process of advocating for a community. In fact, if you have aspirations of becoming an official Developer Advocate, I'd say the best time to start is...now! Find projects that you're passionate about, and/or an audience you're interested in reaching out to, and interact!\nWhat is Developer Relations?\nThis is a question I've asked myself and asked others, and everyone comes back with a different answer. The thing is, Developer Relations means a lot of different things to different people. The role is very dependent on the product or project you work with, the atmosphere of your team, the initiatives of the company, and most importantly, the dynamic of the community that you serve.\nIn other words, I'd say Developer advocacy/relations/evangelism is discovering and delivering the most effective methods to bolster the developer community.\nWhere is Developer Relations?\nSo where does Developer Relations lie in the greater tech ecosystem? Broadly speaking, developer advocates are engineers who sit closer to marketing than traditional engineers. A way to visualize Developer Advocate responsibilities looks a little like this:\n\nFor developers who have interests in public speaking, writing content (like this blog!) and are more interested in supporting developers than being one consider… moving out of Engineeringville and into the Marketing Forest, where their skills can be better realized. Another good analogy for Developer Relations is similar to the relationship between Superman and Clark Kent. A Developer Advocate can live the life of an engineer, but is often called upon to switch into Superman mode and go to a conference or have broader creative responsibilities.\nWhen is Developer Relations?\nAs far as I can find out, developer evangelism started in the 1980's at Apple with Mike Boich. The career has evolved since then, and continues to move as fast as the pace of technology does since you're advocating for technology! It's a very exciting field to be in today, because Developer Relations gives you any number of opportunities to learn about new tools, meet new people, and flex your creativity through public speaking, writing, reading, and of course, coding.\nBrandon West has a great talk about the history of DevRel here.\nWhy is Developer Relations?\nThe most important question of all is why. Why Developer Relations? The thing about \"why\" is that it's extremely dynamic. No two people will have the same \"why\" because no two people share the same inputs and motivations. At the organizational level, this gets even more complicated. So, allow me to tell you my own \"why,\" which can hopefully help you find yours.\nLet's go back to that bench I was on a few months ago. I'd never been to Paris before and had been feeling unfulfilled as a software engineer. I enjoyed the problem-solving aspect of the job, but I felt that something was missing. I wanted a role that would allow me to be versatile, to try many different things and meet many different people from all over the world. I wanted to have a career that would allow me to teach a lot and to learn even more. In the time since I was sitting on that bench, I:\n\nWent to a conference in London and learned about empathy in documentation\nSat with three different engineering teams and contributed to the GitHub… issues they face every day to make the best product they can\nAttended partnership meetings with other organizations, brainstorming creative ways to enhance developer experience\nLearned a lot about marketing, specifically about the subtleties of storytelling to different audiences and the importance of consistent branding, and helped to come up with taglines for a developer event in Paris\nSaw Skrillex perform at AWS re:Invent in Las Vegas\nCoded any number of demo applications using and learning the Algolia API\nEdited blogs from community contributors, having conversations about what value we're trying to convey to their readers\nLearned about open source from lifelong contributors\nMade numerous connections with highly talented individuals that have inspired me to learn new ideas\nFormed a framework for engineers in our office to connect with others in the city\nBuilt out a Discourse channel for people within the company to communicate ideas internally\nHave helped people build out their Algolia implementations\nBeen to multiple meetups where I got to interact with other developers first hand\n\nAnd a whole lot more! Becoming a Developer Advocate has been fun, but by no means has it been easy. I'm very grateful for what I've been able to do these past few months and I'm even more excited for the next ones."
  },
  {
    "id": "8090-0",
    "title": "How to Unlock Time for Side Projects During Working Hours",
    "author": "Nicolas Torres",
    "text": "Algolia is a fast-scaling company: we were about 120 when I joined in August 2017 and grew to 300 by the time of writing this article. We have many offices in different parts of the world, which means different time zones, so it's easy to feel lost, and disconnected from each other's daily lives. This can be especially true for new hires. That could fall under the scope of the Internal Tools squad that I’m a member of, as our mission is to improve productivity and internal communication. We wanted to create something to “cheat” the distance, something for people to feel more like they belong to one company.\nWhat came first to our minds was a TV news screen rotating the week's events: people’s birthdays, hiring anniversaries, new hires, events and meetups, candidate onsite interviews, job openings, as well as time zones—centralizing the company's highlights of the week seemed like a good step into assisting the culture-scaling effort.\nThe screens are now set up in every office!\n\nIt took 10 days to make this project happen, from a manually updated Google Slides deck to today.algolia.com, the fully automated solution. However, the challenge we had to face was to make time for it, while our product roadmap was already quite ambitious. Here are some tips about how to ship a side project when you don’t have work time dedicated to it.\nFrom side proof of concept (POC) to roadmap\nThe very first step was to validate the added value of the project. I took a few minutes to build up a slides deck on Google Slides with birthdays and events of the week, and a few more every Friday afternoon to update it. It was all manual, but the TV screen got much positive attention in our Paris office, making it legitimate to automate the process after a few weeks.\n\nFor this to happen, I needed to unlock some work time that I didn't have, as the project was utterly off-roadmap. The strategy was to build a POC on my free time and present it to Baptiste Coquelle, the manager and tech… lead of the Internal Tools squad. It would be nothing more than an API that would process the data and prove that we could automate everything and make it a one-shot project, reducing maintenance time to zero. Since the project was great for Algolia’s culture, and was already well-planned and contained, this was a win-win situation and made it evident for Baptiste to validate.\nOptimizing production time\nEven though my manager granted some time for the project, there was still a lot to do. Designers at Algolia collaborate with many teams and usually have a lot on their plate. I had spoken with designer Nicolas Meuzard about the project. He was impressed by it and wanted to contribute. To prevent this side project from cannibalizing his time, we needed to work fast and ship fast. We used a few strategies to optimize our collaboration and reduce the number of back and forths.\nPaving the way\nThe first step was to give Nicolas all the information he needed to build the UI. To do so, I used Google Slides and created one page per screen, writing down a simple data schema that the API was returning, and added a few notes for the specific view logic. This came after a few quick chats about the project goal and global form—no other document needed there since the big picture was clear enough to both of us.\n\nReusing UI elements\nNicolas reused the layout of a previous internal tool we did together, making it the first step towards an internal design system that the next projects would rely on. The main grid was the same, we took colors and fonts from the global design system we have for the marketing website, leaving only a few UI elements to tailor and assemble to build the final UI.\n\nServing values\nAt Algolia we use Sketch for designing UIs, and even though I am confident enough with it, it's still not optimal for developers to work with. Even though Sketch has some nice export features, they're still a few clicks away, quickly adding up to much delay when it comes to… building a full stylesheet.\nZeplin, advertised as “the ultimate collaboration tool between designers and developers,” saved the day by giving me access to all the values like margins and paddings, the distance between elements, and font-related metrics, available at a glance on the right pane. It also imports and computes native Sketch files, making it seamless to integrate into the UI designer’s workflow. On my end, I was now fully autonomous on building the UI. The decision to use Zeplin was a no-brainer.\n\nHandling corrections and feedback\nAs in any project that involves design, there was a gap between the initial mockups and the live version. The color calibration of a TV screen is different from the one of a computer, and we have different models of TV screens at Algolia, so we had to fix contrast issues and make it look good across them all. Aspect ratios were inconsistent too, so we had to reposition some elements and create some new responsive rules. People couldn't correctly read our first pagination, so we had to iterate on that too. Add a few more small details like these, and you quickly come up with two weeks of additional delay before launch (especially since we weren't working on this full time).\nTherefore, we decided to collect all the fixes and feedback in one go. For this, I sat down for a full day in the Bridge, our main room in the Paris Office, where the screens are and where all the employees come for coffee breaks. Many came to me naturally with small feedback.\nAt the end of the day, Nicolas and I sat down for half an hour in front of the screens, live tweaking small issues. The next day, he sent me the last iteration of the design, which I implemented on the go. It looked like this:\n\n\n\n\nTips for unlocking time for side projects\nHere are my top tips for working with a tight deadline on side projects without bleeding on planned projects:\n\nDemonstrate the value for the company. Create the world’s most simple POC to create buzz and… excitement around the project.\nCreate a good plan and a simple timeline to make it easier for your team and your manager to say “yes”.\nAnticipate the needs and constraints of other people working with you. Know that they will need to take time on top of busy work hours. Talk to them in advance and get their buy-in.\nUse simple tools that enable you to take over parts of the project from other pros (in my case, our designer).\nCommunicate and fix live instead of asynchronously.\nKnow what’s necessary for the project and remove unnecessary steps. In our case, this meant no tests since it’s not a production, customer-facing app, or no extensive responsive since it’s only meant for TV screens.\n\nCulture eats strategy for breakfast\nAnother reason this project was possible at Algolia is that the Internal Tools squad has fewer constraints than other dev teams, making it easier for us to be agile. We also don't have technological constraints, so we can pick the languages and libraries we're the most productive with.\nAlgolia is big on trust and ownership and encourages people to work on company-related side projects. Yet, we know not all companies or teams or even timeframes favor such initiatives. I used to work in a services company where there was no side project culture because every working hour needed to be justified. Yet, I still managed to work on some internal projects: I anticipated and prepared a POC on my free time to finally get approval from the CTO, and unlock work time to polish them. If you can demonstrate there’s value for the company, and things can be done with ease while not affecting your daily work, you are likely to succeed.\n "
  },
  {
    "id": "8182-0",
    "title": "How Conversational Search is Redefining Digital Interactions",
    "author": "Ryan Chen",
    "text": "The next revolution will be powered by the most natural form of human interaction: voice. \nAlready, 32% of people are searching by voice daily. And there’s hardly an industry where voice search doesn’t add to the user experience. E-commerce alone expects $40 billion voice purchases in the U.S. and U.K., with other industries sure to follow.\nUsing only their voice, a shopper can be perusing your digital storefront to find exactly the pair of pants they’ve been looking for. Or just by speaking a few keywords, a reader can find and consume a piece of breaking news on their mobile device. It’s not hard to see how voice search can redefine the way a user interacts with your business.\nVoice search replaces the traditional keyboard search field with an interactive relationship between the user and the business. How does it all work?\nSearch from directional to conversational\nVoice search is often thought to need a true conversational, back-and-forth experience. Voice can be that, but it doesn’t have to be that. Voice can simply be another input mechanism.\nThe voice stack consists of an input, an output, and the fulfillment in-between. The input represents the spoken request (speech to text) and the output is the application’s response (text to speech). A good rule of thumb is ‘garbage in, garbage out’—that is to say, the better the input, the better the output. Presently, most voice search engines are at about 95% accuracy, which is actually better than human listening. Speech-to-text is good, and getting better.\nThe fulfillment in-between, however, is where complications with voice search may arise. The fulfillment is made up of the intent detection and business logic that takes what may be an incoherent input spoken in natural language and turns it into an actionable output. \nHere are some of the parts that help make the fulfillment in-between possible.\n\nRemoving stop words.  Conversational language doesn’t always follow a phonetically clean,… grammatically correct, or even a contextually coherent pattern. Often times, colloquial speech is messy and laden with stop words, like “find me”, “the” and “about”. For voice search, stop words get in the way of tidy queries that the voice stack can register. A good voice UX will ignore stop words and act like they're not there.\n\nReducing the haystack. For wordier formulations, the engine will need to pluck through many extra words to isolate the relevant ones. Think of the removal process as searching for a needle in a haystack. Instead of looking for the needle by examining each and every stack, it’s more efficient to make smaller stacks until you find the needle. By reducing the haystack, full-text searching becomes easier and more precise. Some functionalities that can reduce the haystack include:\n\nFiltering. Algolia’s built-in search engine has a feature called query rules, which takes the textual queries and looks for filterable values to reduce the stack.\nPersonalization. The users’ own affinities can also be employed to filter results. Some results can be boosted to account for user behavior. The voice search experience is given a bespoke feel when personalization is woven into the fulfillment process.\nContext. The information surrounding the query makes up the context. Information such as a user’s recent queries, time or date of search, and number of user requests are all taken into account.\n\n\n\n\n\n\n\n\nAnalytics. Using analytics, you can extrapolate insights from your voice engine to improve the conversational experience. For example, in the case of synonyms, users are often speaking the different ways to mean the same thing. With analytics at your disposal, you can monitor, iterate, and anticipate a user’s behavior to help them find exactly what they’re looking for the next time. \n\nVoice search has already penetrated various industries and integrated into businesses that sit on the cutting edge. By connecting the user and business… with the most natural form of interaction, successful voice search will inspire customers, drive brand engagement, and increase sales.\n\nLearn more in our eBook: The Next Tech Revolution Will Be Spoken"
  },
  {
    "id": "8209-0",
    "title": "Redesigning our Docs - Part 2 - Making Technical Content Readable for Everybody",
    "author": "Peter Villani",
    "text": "This is the second article in a seven-part series of blogs that describe our most recent changes to the architecture and content of our documentation. We focus here on the writing choices.\n\nMany docs are written by and for developers. This works fine when your readers are developers who require straightforward, immediate information, like details about method parameters, code snippets, and error messages. However, when your company grows not only in customer base but by offering more business-related features, then dictionary-style text - or what is often called reference material - does not sufficiently cover the broader needs of building complex applications with multiple features and best practices. Best practices, for example, are critical: correct and efficient feature implementation is the only way that clients get the best out of what your company has carefully designed.\nAlgolia’s technical writing team has gone in this direction. Our documentation now covers a broad range of subjects, like synonyms, intent detection, NLP, analytics, personalization, and merchandising - all of which impact business as well as implementation. Our readers include not only developers, but CTOs, product managers, and marketers. We therefore need to support readers with different goals and profiles.\nBut every reader has the same goal: to learn about what Algolia offers and to make the best of their investments in both time and money.\nWriting choices\nChange was needed. We were not giving every subject the best coverage, and related information was not always grouped together coherently. Some readers wanted more focus on problem-solving, with examples, code, and technical detail; others wanted more clarity on features, with use-case analysis and  benefits presented in clear, non-technical language. \nTo add to that, our documentation was not always user-friendly: it was jargon-filled, with text too sparse and low-level to be easily readable by all of our readers. The language we… used sometimes alienated or intimidated our readers. \nWe addressed these concerns by including more accessible word choice and more storytelling in the content flow of each page. \nEssentially, we decided to take a progressive approach, where everything - navigation, page structure, and language - starts from the familiar and gets progressively more complex, with clear signs along the way where to find code or to go more in-depth.\nIn the end, we believe our readers - whatever their purpose or profile - can come to our docs and get what they need.\nChoosing vocabulary\nOur first concern was language. Previously, we were not using words and phrases that everybody could understand. We relied a lot on jargon, which had the dual problem of assuming that everyone who read our content was highly technical and also good at reading our minds.\nTake a word like replica. On its own, it means nothing to a first-time reader. It’s an Algolia-centric term: we own its meaning when we speak it. We use it in a very specific way that differs from its dictionary meaning. However, we need to use it: there’s no better word for capturing the idea of “a duplicate index that sorts results differently from other indices”. Granted, that sentence needs a lot more context - we need to define the terms “index” and “sorts”, otherwise, database administrators might come away with the mistaken understanding that an Algolia index sits on top of a database table, whereas in Algolia, an index is the data source itself, and sorting is a complex subject at Algolia, requiring its own page. You get the idea - we stopped taking anything for granted.\nGood word choice follows the general recommendation that technical documentation use plain language. Our approach, therefore, starts with a common word before using a more technical term. A word needs an introduction, like a character who needs to be introduced before speaking or acting. \nWe try to define all terms. Some words require only a single… word or phrase, like “attributes (key/value pairs, fields, information)”; others need a bit more context, like “Searchable attributes (a select group of attributes used for searching as opposed to displaying, filtering, or ranking)”. And some terms (relevance) need a full page (that required days of research and interviews with key Algolians, including our founder and CTO). But the result was worth it, because all of these terms are core concepts and many of them have multiple nuances depending on the context.\nStart with the problem before introducing the solution\nWe considered the first paragraph of every page as an effective method to engage the reader. We asked - What do we want every reader to see when they land on one of our pages? Our answer was: A simple statement or description of the problem that the page was going to solve.\nThis problem-solving approach is consistent with using a clear vocabulary and defining terms. We believe that going directly into a problem puts the reader into a familiar setting and encourages them to continue reading, to find out how to resolve the problem. I say “problem”, but it could be a need or a common use case. Problem-solving speaks directly to any reader interested in solving their own problem. This approach can also appeal to the curious who is looking to opt into Algolia or to enhance their existing solution.\nUsing more examples\nWe tried to stick with one example per page and to thread that example into the general treatment of the problem/solution. We were aware that not every example applied directly to a reader’s particular situation, but we thought that a well-chosen example would nonetheless enable every reader to see how a specific problem gets resolved and be able to adapt it to their own situation. In fact, one page presented several examples, from movies to recipes, to actors, to books, and back to movies, with some repeating characters, like Charlie Chaplin, Harry Potter, and blueberry pie. The… page finally finished by combining movies and actors in the same index. For this page, each example was important to explain a different aspect of the same storyline, which was that formatting your data has many nuances and requires different choices.\nCreating new kinds of pages\nGoing beyond the first paragraph, we considered the whole page - What do we want our readers to see? We came up with three kinds of content: a summary overview, a deeper understanding, and an implementation guide with code examples. \nBefore designing the pages, however, we needed to address two main concerns:\n\nHow to engage non-technical readers, to discourage them from leaving right away, and to avoid too much of a technical feel on the page. \nIn contrast, we needed to avoid over-simplifying or being too general. We needed to satisfy our developer-readers with straight-to-the-point technical explanations as well as clear implementation details.\n\nWe had thus given ourselves a difficult but not impossible task: to satisfy two very different concerns - on the same page, with the same words and same conditions. The solution we came up with was two-fold:\n\nThe non-tech needs to feel welcomed in a language they speak, in terms that they can understand, and with examples that speak to them. They need a progressive flow that starts with the familiar before getting more complex. \nAs for the developers, they need to do a bit more scrolling. But let’s be honest, they were going to scroll anyway, right? So they start scrolling, and quickly they land on what they want - specific answers to specific questions. Additionally, they need to have clear signposts (hyperlinks, buttons) showing them where to find code, other implementation details, or deeper information about a subject.\n\nIn short, we wanted to give all readers, developers included, a clear description of a problem, with examples and descriptions of the feature(s) used to address it, and obvious navigation for more info or code. To do this, we… set up a 3-page approach:\n\nA landing page, which sets the stage by summarizing a problem and describing, in a general way, the how, why, and what of our solution/feature. With that background, readers could take the following next steps: stop reading, because they only needed a summary; go to a how-to page that gives exact implementation details; go to an in-depth page that digs deeper into the problem/solution. \n\n\nA how-to page (as well as our existing API reference pages), which contain plenty of examples, more detailed explanations, and lots of code. Generally, we present a use case or problem, offer an example or real dataset, and then walk the reader through our solution.\n\n\nAn in-depth page that goes beyond the summary description. It takes more time with the subject and gives the reader more information about a feature and its context. This is for the reader who needs or wants to know more. It also gives the writer some breathing space to go fully into a subject and to give it the full treatment it deserves.\n\nThe progressive approach\nAs stated at the outset, what we are describing here is a progressive approach to documentation that is present at every level - architectural, page format, content flow, and words and phrases. Every part of our documentation starts with the familiar and moves towards the more technical and complex, with the hope that the familiar will drive the understanding of the technical and complex.\nAlgolia has worked hard over the years to offer features based in common sense. We wanted to mirror that common sense in our documentation.\nThe redesign of our docs involved more than just a rewrite. Our next article focuses on the look and feel of the new architecture, more specifically, on the decisions we made during the UX and UI phase."
  },
  {
    "id": "8123-0",
    "title": "The Challenging Migration from Heroku to Google Kubernetes Engine",
    "author": "Adrien Joly",
    "text": "The simplicity of Heroku, for a team consisting only of developers, made it easy for us to get our prototype into production. However, as our product matured and customer expectations grew, we needed more robustness and fine-grained control over our infrastructure. We knew that Kubernetes was the right choice for us. However, the migration was not a simple task.\nHere’s the backstory. About a year ago, we decided to prototype a web crawler for Algolia. As you may know, Algolia users make their content searchable by uploading their data to a searchable index, using Algolia’s Search API. Several potential customers asked if we could populate their search index for them, automatically, by crawling their website’s content. In response, we quickly built a web crawler prototype in Node.js and deployed it to Heroku.\nWe were not disappointed: adding services such as a database server or RabbitMQ was just a single click away. All we had to do was  to deploy new versions, and our prototype went to production.\nIn a few months, our web crawler became popular among Algolia customers, and many others started to express a need for it.\nHowever, the crawler required lots of components - some running in the background, others on demand; additionally, some customers required customized components. As the product grew more complex, we asked for help from our infrastructure colleagues.\nA good example of this complexity is with IP Whitelisting. One of our customers wanted us to crawl from a fixed IP address so that they could whitelist that IP for high-rate crawling without being throttled by their load balancer. Only two engineers were developing the crawler, so we asked other colleagues to set up an HTTP proxy with a fixed IP address. Yet, as the number of customers grew, many more started asking for the same thing, and our infrastructure team told us it was time for us to take care of it ourselves.\nTherefore, we decided to move to a cloud platform that would provide more control… over our infrastructure, and eventually allow us to programmatically set up and tear down proxies. That's how we decided it was time for us to migrate from Heroku to Google Kubernetes Engine (GKE). As a first step, we wanted to get our crawler to work on a GKE cluster, with as few codebase changes as possible. Only then would we make it more robust and maintainable for production.\nThis was far from being straightforward, as we had initially thought.\nIn this article, we describe the architecture of our crawler and explain how we made it run on GKE, sharing three challenges that we tackled while migrating. We then conclude with a few lessons learned and benefits brought about by the migration.\nSetting things up\nBefore we dig down into the rabbit hole, let’s do an overview of the web crawler - its architecture, underlying services, and how we make it work locally and in production.\n\nThe crawler itself is a set of three components:\n\nthe worker is responsible for fetching a web page, extracting information from its HTML content, and storing this information into an Algolia index;\nthe manager is responsible for dispatching URLs to be crawled to workers, with rules and constraints associated with each customer (e.g., rate limiting), as well as any configuration updates that they may have requested;\nthe web server is responsible for handling API requests addressed to the crawler (e.g., from the Algolia dashboard) and serving its own management and monitoring dashboard.\n\nThese components sit on top of several services:\n\na RabbitMQ queue that holds the list of URLs to crawl;\na PostgreSQL database that holds the state of the crawler, like customer configurations, a list of URLs, and external data to improve the relevance of searched records;\na Redis store that holds the user sessions of our dashboard;\na Tika server that acts as a proxy to extract content from PDF files and other types of non-HTML documents;\nand a Rendertron server that acts as a proxy to extract content from… single-page applications that require the execution of JavaScript code to render their content in the DOM.\n\nTo run all these components and services locally while developing the crawler, we set up a  file that specifies Docker images and parameters for all of them.\nOn Heroku, we activated add-ons for each service and wrote a  to specify what command should be executed to start each component. Then, by simply executing , we ensured that the latest versions of our components would be automatically uploaded and started in our Heroku dynos. It was a breeze.\n\nKubernetes is a system that can dispatch pods based on developer-defined services and deployments. Our first goal was to make our components and services run the same way as in our existing  file. We would only have to convert that file to the Kubernetes format and find the right command to start them.\nAfter spending a few hours trying to do that with kompose, without much success, we decided to ask for help. A coworker helped us in three ways: setting up a cluster on GKE, providing us with examples of Kubernetes definition files for deployments and services, and recommending that we use services managed by Google (namely, PubSub and CloudSQL) instead of running our own RabbitMQ and PostgreSQL docker containers as pods. This was all excellent advice, but too soon. To better understand how Kubernetes works, and to feel more confident with it, we decided to solve one problem at a time: first, get our services to run in containers by mirroring our  definition, and only then, consider replacing them by Google-managed services.\nWe therefore started writing Kubernetes definition files for each service.\nImplementation - Let’s first get Kubernetes to run\nWe defined them like this:\nView the code on Gist.\nTo summarize:\n\na deployment is the description of a piece of software that can be deployed, run on a given number of instances, and stopped;\na service is a deployment that may handle requests from other parts of the… system.\n\nFor example, to run RabbitMQ on Kubernetes, we need to:\n\ndefine a deployment by specifying a Docker image that runs a RabbitMQ server;\nand define a service that exposes two ports: one for AMQP queries, and an optional one to serve the management UI.\n\nWe defined our crawler's components the same way as deployments, apart from the web server that needed to be defined as a service. Because these components aren’t public Docker images on DockerHub, we also had to write a Dockerfile to generate an image from our source code, upload the image to our Google Registry, and then refer to its identifier from our three deployments. To do that, we had to learn how to use the gcloud and kubectl command-line interface (CLI) tools.\nAfter defining our deployments and services in YAML files, we needed them to connect to each other. For example, our three crawler components expect environment variables containing the URLs of all the services it needs to connect to. In Heroku, we had a list of global environment variables that all our dynos shared. We could edit them from the Heroku dashboard or through their CLI. That said, most of our add-ons (e.g., managed PostgreSQL database) automatically set environment variables to provide direct access to their data, so we didn't need to do much.\nIn the Kubernetes world, environment variables are set at the deployment level. It means each deployment file should contain the values of necessary environment variables. Furthermore, given the fact that Kubernetes can dynamically kill and restart deployments on different nodes (e.g., physical machines of the cluster) at any time, their IP address and port can change. Consequently, we can’t provide hard-coded values to the environment variables of our components.\nFortunately, we learned that Kubernetes dynamically generates cluster-wide environment variables for all services, with the form  and . We also discovered that it was possible to inject the value of environment variables into… others, by using the following YAML syntax:\nView the code on Gist.\nConfidential environment variables, like passwords, needed a different process. For that, we used Kubernetes Secrets.\nSecrets are Kubernetes entities that can hold confidential values. They are recommended for storing password, certificates, and any other kind of private information: these values are never added in plain text in YAML files, and accessing them requires special permissions.\nTo be stored as environment variables, Secrets must also be declared in the YAML files of the deployments where they are required. Yet, they aren’t structured the same way as environment variables: we needed to mount a volume to load Secrets, then import their values as environment variables.\nView the code on Gist.\nWe later learned that it was possible to share environment variables across several deployments using ConfigMaps. These are Kubernetes entities that can hold several named values and be imported as environment variables into deployments.\nUsing ConfigMaps prevented us from duplicating configurations, but we couldn’t find any way to include Secrets, or any other entities, that would wrap the value of other environment variables (e.g., using the  syntax, as seen above) in a Config Map. Therefore, we ended up using ConfigMaps for invariable configuration values, Secrets for passwords and keys, and inline environment variable definitions for the ones that depend on other environment variables.\nFurthermore, as we wanted our YAML files to provision two different clusters (e.g., production and staging) on separate domain names, we ended up turning some of those into templates and writing a script that rendered them into final YAML files, using sed. We’re pretty sure there’s a more standard way to achieve this, but this method was a good compromise for us, given the time we were able to spend on this migration.\nAt that point, we had written 10 YAML files and 5 bash scripts to define our components and… services. We were finally ready to provision our GKE cluster and see them run. The command to upload our YAML files and to get them to run on our cluster was: .\nTo give you an example of the kind of scripts we wrote, here is the list of commands we were running to restart all components after deploying an update that may contain database migrations:\nView the code on Gist.\nNot so fast. A colleague warned us that, in order to make our dashboard accessible from the Internet, we had to define an Ingress resource to connect our \"web\" Service to Google's HTTP Load Balancer.\nHe provided an example that ended up looking like this:\nView the code on Gist.\nAfter a few minutes, our dashboard was finally live!\nUnfortunately, we couldn’t log in using our Single Sign-on system before enabling HTTPS access to that endpoint. Let's dig into that.\nWire an SSL certificate\nIn the Heroku world, enabling HTTPS/SSL is a piece of cake. All you have to do is click a button.\n\nHeroku would automatically generate a free SSL certificate using Let's Encrypt, reply to the ACME challenge, and automatically repeat that process every 3 months to renew the certificate without us knowing it. It just worked.\nWe hoped that Google would also provide an easy way to set that up on our GKE cluster. Think again! GKE's documentation clearly states that, while it's possible to associate an SSL certificate with Google's Load Balancer through kubectl or Google Cloud Console, they don’t provide a way to generate it.\nWe used Google to find solutions, and we found several projects that promised to generate and renew an SSL certificate for your GKE cluster, and would automatically associate it to our Load Balancer. Unfortunately, all of them included disclaimers such as \"do not use in production\" or \"we do not currently offer strong guarantees around our API stability\". Therefore, we decided that, until Google provides a reliable way to do that automatically, we would manually generate a Let's Encrypt… certificate, then attach it to our load balancer. The only thing is, we needed to remember to do this every few months.\nOur crawler was fully functional at that point. The only remaining concern before migrating was the potential loss of data from our PostgreSQL database, as it was still running from a Docker container, without persistent volume nor any backup routine.\nDisclaimer: Since our migration, other solutions have made that process easier. We haven’t tried them yet.\nPlug into a managed database\nData is a serious matter. Our customers rely on us, so their data should be available at all times, be responsive no matter the scale, have no leaks, and be quickly recoverable in case of incident. These are all excellent reasons to trust a managed database service instead of running the database ourselves from a Docker container that Kubernetes can kill at any time.\nAs part of the Google Cloud ecosystem, the CloudSQL product had recently promoted their managed PostgreSQL service as \"production-ready\", so this was a no-brainer for us: we should plug our crawler into that service. Our colleagues told us that we would have to integrate a so-called CloudSQL proxy to connect to a CloudSQL-managed PostgreSQL server.\nTo do this, we followed a tutorial provided by Google. In addition to replacing our PostgreSQL Service by a CloudSQL Proxy Service, we had to:\n\ncreate a database user;\nstore that user's password securely as a Kubernetes Secret;\ncreate a service account to access the CloudSQL instance from our components;\nload the secret as a dynamic environment variable in all our deployments that needed to connect to the database.\n\nDespite the help we got, it was not straightforward to integrate this into our system. The tutorial provided by Google explained how to run that proxy in a \"sidecar\" fashion, meaning that the CloudSQL Proxy would run on the same Pod as the application itself, making it easy to connect to that proxy.\nIn our system, we have three separate components… that need to access the same database, and we felt that attaching a separate CloudSQL Proxy to each of them would have been overkill and harder to maintain. Therefore, we had to take our time to better understand how to configure the deployments. On top of that, it's sometimes necessary to access our production database from outside the cluster (e.g., from our development laptops for debugging purposes). Since all database connections must go through CloudSQL Proxy, we had two options:\n\nconnect through the CloudSQL Proxy that is running in our production cluster;\nor set up a local CloudSQL Proxy with one dedicated service account per developer.\n\nFor security reasons, we picked the second solution. After downloading the JSON key associated with the service account we created for each of us, here's how we were able to run CloudSQL Proxy on our laptops:\nView the code on Gist.\nIf you decide to follow that route, make sure that you do not keep the JSON key associated with your service account on your laptop. We recommend using a system like Vault to either store these keys more securely, or even generate a new, short-lived key with every connection.\nDisclaimer: It’s now possible to access a CloudSQL database directly from GKE. We haven’t tried this yet.\nConclusion\nThe migration took time, despite some of the shortcuts that we took, such as mirroring our system's docker containers first to reach an iso-functional stage, and only then replacing our database container by a managed solution. In the end, we're glad to have taken a step-by-step approach. It allowed us to better understand how Kubernetes works, and to master the tools that we use to maintain our GKE cluster. It also prevented us from having to tackle more than one problem at a time, which could have become a major source of frustration and demotivation given the complexity of GKE and the infinite number of ways problems can be addressed in that ecosystem.\nIt took us several weeks to get our crawler up and… running sustainably on GKE, and to finally shut down our Heroku dynos and add-ons. While the migration was more tedious and less straightforward than we had anticipated - despite the help we got from colleagues who had experience with Kubernetes and Google Cloud products - at the end of the day, we are satisfied by what it brings to the table. For example:\n\nWe now have a better understanding of each of our components' hardware requirements and their behaviors in isolation. Migrating to Kubernetes has made them more robust on a dynamic infrastructure (e.g., nodes can be turned off and re-allocated at any time, and at different IP addresses).\nWe are currently discovering how to horizontally scale the number of replicas of our \"worker\" deployments automatically and in a more efficient way than we could have done on Heroku.\nWe are confident that we will be able to set up static-IP proxy servers programmatically on our cluster, based on our customers' need.\n\nThe good news is that recent evolutions in Google Kubernetes Engine have addressed some of our difficulties and made this process easier.\nBy the way, we're organizing an event about Kubernetes on March 7th in our Paris office with some Google experts. If you're in Europe at that time, feel free to register!\nThe author would like to thank Rémy-Christophe Schermesser, Sarah Dayan, Peter Villani, Angélique Fey and Tiphaine Gillet for their contribution to this article. And all the colleagues who helped proofread it. ❤️"
  },
  {
    "id": "8153-0",
    "title": "Tips to Make Your Life More Vue-tiful 😎",
    "author": "Haroen Viaene",
    "text": "This post was made collaboratively with Bram Adams.\nBig news! We’re proud to announce our second major release of Vue InstantSearch. We’ve added a lot of helpful optimizations and new features. We'll keep this short and focus on our favorites. Let's go ahead and jump into it.\nNew Vue Widgets!\nI love having new toys to play with, don't you? Vue InstantSearch 2 comes with a slew of new components that will make your search awesome. Here are a few you may find interesting.\nWrap your widgets in a container\nThe  is a container widget and can be used to provide a consistent look and feel to your site. You can use this widget to consistently display a header and footer around widgets. Since it will have a  class, it also becomes more consistent to hide or display a widget differently if it can't be used.\nConditionally display results\nEver thought to yourself, I wish I could show these results only sometimes? Well now you can! With , you can choose what to show to your users. It can be used to show a specific \"no results\" page, or a specific banner when someone refines a particular case.\nShowing breadcrumbs\nNow where did I leave that shoe again?... Oh right! Using , you are able to allow users to discover paths and give them another way to navigate your products. I use my breadcrumbs in Notion all the time!\nPerforming inline configuration\nThe power of configuration. The comfort of staying in Vue! Using , you can provide raw search parameters to Algolia. This will allow you to do powerful things on your front end like limit the number of hits returned per query, or only return distinct results.\nDisplay dropdown menus\nA powerful widget, the  gives you a dropdown to show menu items that users will be able to interact with. This widget is heavily customizable, allowing you to pre-render the results, giving you a chance to manipulate them before they show up to your users! Tiny dropdown, big potential!\nView your applied refinements\nThe  widget displays pills for each of the… current refinements a user has selected. You as the developer get to decide what subset of attributes are allowed to show up in this widget. See a trend here? Customizability is here to stay!\nShowing more results\nThe  widget is pretty aptly named! It gives you the component to add a 'Show More Results' button that will show more items when pressed. Just keep scrolling, just keep scrolling...\nCreate numeric menus\nThis  widget is a pre-configurable widget that creates a radio-button menu of numeric objects that can be used as filters. To use this widget, simply create a list of objects containing different numerical values, like below. Mathematical!\n\nToggling a refinement\nWax on, wax off. That is, at least, if \"wax\" is an Algolia attribute! You can use  to display a toggle-able component. As before, you can edit the CSS classes as well to achieve deep customizability with !\nNew Widget... Names!\nWhat's in a name? We’ve updated our widgets names to be closer to the rest of our InstantSearch flavors. The full list can be found here, but the main idea is that any experience you have with Algolia InstantSearch in the past should roll over seamlessly.\nServer-side rendering (SSR)\nWe’ve completely revamped the way server-side rendering works with Vue InstantSearch. The capabilities for server-side rendering didn't change much compared to the previous version, but the main difference is how it is implemented behind the scenes. Instead of waiting until all requests are done (not necessarily always correct), a static method is used to do a back end request. Read more on how to implement SSR in the documentation.\nURL synchronization\nPreviously you would manually have to read and translate each parameter from the \"search store\" manually and keep it in sync with the URL. This was not an easy process and so we simplified it with a dedicated API. We now map over the parameters to turn them into a URL-friendly format, and provide a way to modify that URL as much as you need. Read… more on that in the documentation.\nCustomization of Widgets\nLet’s finish with a feature we are proud to announce. Our widgets are now fully extendable and customizable. We’ll be publishing a second blog on this subject in the near future, but we can give you a few details. To customize a widget, there are now four levels to make this as smooth as possible:\nSimple customization\nThe easiest way to customize a widget is by changing simple text and icons. For this you’ll use regular \"slots\". An example is the reset icon on an .\nCustomizing the complete HTML\nIf you want to go further with customizing and override the whole HTML output of the component, you can do that too, via the \"scoped slot\" which is available at the root of each widget. An example would be to use your own component instead of the search box altogether. You'll get access to the current query, and a function to change the query to the new one ().\nUsing the widget mixin\nIf you find you also need access to lifecycle methods to data provided to this widget, you can add the widget mixin and use the connectors which are available from InstantSearch.\nMatrix-level control\nFinally, you can also enter the \"ultra-matrix-mode\" (as I like to call it) and make your own connectors to encapsulate logic that modifies the search state directly.\nFull control at your fingertips! Check out our tutorial on how to customize a widget.\nTo conclude\nFrom adding widgets to allowing full customizability of their look, feel, and behavior - we’re continuously working to ensure that Vue InstantSearch gives you the best of Algolia’s powerful search technology! Try Vue InstantSearch v2 out now here, or read the documentation."
  },
  {
    "id": "8077-0",
    "title": "Redesigning Our Docs - Part 1 - Why",
    "author": "Marie-Laure Thuret",
    "text": "This is the first article in a seven-part series of blogs that describe our most recent changes to the architecture and content of our documentation. \n\nIn December 2018 we released a major iteration of our technical documentation. This is the first part in a series of posts that will retrace this adventure from the genesis of the project to what’s coming up next by describing the writing, UI/UX, and technical choices we made. \nA key objective at Algolia is how well we improve Developer Experience (DX). Currently, there are around twenty people involved in the various squads regrouped under the DX umbrella, five of whom are dedicated to our technical documentation website. \nImplementing Algolia in no time and with the least amount of friction as well as being able to get the most out of it is the core mission of the documentation squad. We see our documentation as a living thing. We will never be done with it, and it will continue to evolve as long as we have user problems to solve. \nIn 2017 we had our first big iteration involving a lot of people internally. We focused mostly on producing missing content and making a clear distinction between concepts and tutorials. When releasing it, we knew we had created a solid base on which to build, but we also knew that there was a lot more to do. \nDiscovering problems\nAt the beginning of 2018, six months after our last docs iteration, we decided to organize a full-day workshop with participants from various Algolia squads. At that moment we had a few intuitions regarding problems that we wanted to confirm. That day gave us the opportunity to conduct several interviews with various documentation users, and to reflect on their feedback as well as to map the user journey of someone trying to implement Algolia using our docs. \nEventually, at the end of the day, we uncovered several areas that needed improvements. \n\nOur users couldn't grasp easily what Algolia was offering and what it meant to implement Algolia search when… landing on our documentation. You had to choose from the start whether to read our concepts or follow one of our end-to-end tutorials.\nOur users were confused because our technical documentation was spread around 14 websites. We had one main documentation and a lot of different community websites for every project that popped-up the past few years at Algolia such as our InstantSearch libraries or our integration plugins. If you wanted to build a front-end implementation of Algolia you had to go back and forth between two different websites that followed different guidelines. \nWe were using a lot of Algolia jargon that was not helping our users project themselves into our product. We were often using feature names as titles instead of guiding our users toward what the feature was used for. \nWe were addressing all our audiences in the same way. We didn't have a clear strategy about organizing and writing content to address both a non-technical and technical audience as well as handling the different skills or experience developers can have. Often it was resulting in a deceptive experience where people couldn’t find the content they should have easy access to. \n\nThree key principles \nTo resolve those problems, we decided to completely change the philosophy behind our documentation, which resulted in three main choices:\nA single website for all Algolia documentation. \nWe decided that algolia.com/doc should be our unique entry point for all Algolia technical documentation. We decided to start by bringing back InstantSearch, our front-end libraries that provide UI widgets. Now, in the same place, our users can go through all the steps of an Algolia implementation: from pushing their data to creating the kind of search and discovery they want on their website. More than just providing a better flow, this integration of the front and back end helps with the quality of the content now that they share the same architectural and writing structure.  \nMoving away from a… section-centric documentation\nInstead of asking our users to choose between reading concepts or doing A-to-Z tutorials we introduced a sidebar navigation that walks them toward all the steps of an Algolia implementation and how to make the most of their search.  \nAddressing all our audiences \nOur technical documentation is not read by only one single profile but indeed many. From the decision maker who wants to quickly evaluate Algolia’s offer, to the curious who wants to know if X is possible with us; from the marketers who are trying to improve and fine tune their search to developers in the process of implementation. For the developers, some are newcomers looking to have their first Algolia implementation up and running within minutes and others are advanced users who go back and forth with the documentation when looking to improve their current implementation. And let’s not forget to mention Algolians themselves who internally rely on our documentation to serve our customers. \nOne major shift was to introduce the concept of hubs. No matter the profile of our users, every topic they arrive on when navigating with the sidebar should be accessible. Then, depending on who they are, they will have the ability to deep dive into our content. It could be reading some how-to’s if they are developers interested in copy-and-pasting some code snippets or exploring in-depth guides when looking to master a specific subject. \nThose three key principles drove all later choices we made, whether it be the information architecture we put in place or the user experience we wanted to offer. Our next post in the series will dig more into the specific writing choices we made that serve both Technical and Non-Technical Readers."
  },
  {
    "id": "8067-0",
    "title": "Building a query suggestion UX like Google's",
    "author": "Maria Schreiber",
    "text": "Last year was a big year for us here at Algolia. We hired scores of incredible people and released some incredible new features. One of the earliest was query suggestions, released in February. It harnesses our analytics API to create dynamic suggestions ranked by popularity. Its ease of implementation allows our users to, in a matter of hours, create a UX that consumers, “spoiled” by Google, have come to expect. To read more about how the feature works in detail, check out this blog post. \nTL;DR: our query suggestions engine creates an index of the most popular search terms that lead to results (vs those with zero results) and refreshes that every 24 hours. This index can be used to create the autocomplete dropdown that Google has led many of us to know and love. \n\nUnlike Google’s, however, this list is not yet tailored to an individual user.\nWhat about a personalized experience?\nOne trend last year was increasingly personalized content— advertising follows us across devices and curated content is recommended to us based on our past browsing history. Google, for example, saves our searches (among other things) so that two people sitting next to each other, searching for the exact same thing, might get a different set of suggestions.\n\nAlgolia customers can recreate this experience by combining our current query suggestions feature and just a bit of additional leg work: By saving searches on a per-user or per-segment basis, to create a highly personalized search experience.\nPersonalized saved searches allow your users to quickly search for something they look for frequently, while the crowd-sourced suggestions allow them to more easily discover a term they may have been looking to search. Because the suggestions coming from our query suggestions feature are being generated by a wider audience, they will change more frequently and will represent what is trending. For this reason, we recommend sticking to a ratio similar to what is shown above: 1 to 5 for saved… to suggested searches.\nAn added bonus—by building saved searches in Algolia rather than relying on some sort of caching like localStorage, these saved searches are available cross-device This allows a user searching on their phone to access searches they made on their laptop. \nBuilding it\nThe query suggestions feature can be used as a template for how to do this. Query suggestions draw from our analytics API to create a new index; you will have to create a new index with recent searches in it. Algolia pulls results on a per character basis, but it would be best to only send “completed” searches to Algolia for indexing by debouncing — waiting a set amount of time without any new character input. \nEach completed search should be tagged with a timestamp, count, and usertoken for the user who made it. The count attribute is incremented every time the user types the particular search term and can be used as a custom ranking attribute to boost the most popular searches to the top. The timestamp will allow you to both purge old searches (say those older than 30 days) and to rank searches by recency. The usertoken will allow you to only pull up relevant saved searches on a per-user basis. \nIt is better to create a large index with all of your users’ saved searches rather than creating one index per user, as indexing updates to multiple small indices is much slower than indexing the same number of updates to one large index. Additionally, we recommend using our secured API keys feature when pulling results from the API for enhanced history containerization and security.\nOn the front end\nOne caveat is that, since the dropdown is being built from two separate indices, it’s possible that something could come up as *both* a saved search, and a suggestion. This experience looks glitchy and takes up valuable real estate on dropdowns that should stay short. For that reason, it helps to always display the 1-2 saved searches, but then build additional logic to not display… any suggestion that is also found in the saved searches.\nA nice UI touch is to differentiate the saved searches from the suggestions. Google has done this by bolding and adding a “Remove” option.\n\nAdding an icon to denote that this suggestion is being sourced from the search history is another nice option.\n\nAnnouncing automated personalization\nSaving a user’s search behavior and using it to influence the output of future search results is the basic concept behind our biggest feature release of last year. \nOn December 18th, we released an automated personalization feature that allows our users to return individualized search results for their end-users based on their past actions. \nWhat events in that history to consider — whether it be product page views, hitting the “like” button, or reading an article to its completion, how much each event is weighted, and how “similar” items are scored — is up to you and configurable on the Algolia dashboard. The only additional development work is to send these events to our Insights API whenever they occur, and to toggle on the “personalized” option when returning the results. \nWe’ll take care of the rest. \nCheck out the blog post and the product page on personalization to learn more.\nAvailability\nQuery Suggestions is available today for our Business and Enterprise customers and Automated Personalization is available for Enterprise customers only. To enable these features directly on your dashboard, reach out to your product specialist or customer success manager. \nHave feedback? Tweet to us @algolia, or comment below. Thanks for reading!"
  },
  {
    "id": "7999-0",
    "title": "Tips for Retailers to Start 2019 on the Right Foot",
    "author": "Ivana Ivanovic",
    "text": "Looking at last year’s trends, particularly areas where we fell behind, is always a good indication of where we can reach going forward. Internet retailer’s 2018 survey on site search and KPIs — first ever to look at correlation between search and business performance —  is a neat compendium of trends that have been shaping user experience in the e-commerce industry.\nWe have summarized key points in a nifty infographic below; here are our takeaways for 2019:\n\nAt the very minimum, understand how many of your site visitors use site search. You’ll likely find that your site’s conversion and bounce rate are significantly correlated.\nUnderstand and define key KPIs affected by site search: revenue per visit, time on site after search, etc. Start measuring and testing.\nRate your site on top aspects of search user experience, particularly those that affect performance goals: speed, relevance, ease of navigation, personalization. Then check out this great article on the nitty gritty of advanced search user experience by UX planet.\nIdentify where your site sits in the search maturity model. You’ll want to at least plan to be where most advanced retailers are today.\nShift priorities to make sure you are ahead of the curve.\n\nWe hope you’ll enjoy the infographic below. To download the full survey with detailed stats, tips and trends from top retailers, click here."
  },
  {
    "id": "8029-0",
    "title": "Scout Extended: The Full Power of Algolia in Laravel",
    "author": "Nuno Maduro",
    "text": "We are thrilled to announce the release of Scout Extended — the official Algolia Laravel integration. Built on top of the latest release of Laravel Scout, Scout Extended lets you take advantage of all of Scout’s great features, and, at the same time, leverage the complete Algolia’s search experience.\nWhen Laravel Scout was first released, it provided immediate access to Algolia search. For many, this meant simply getting fast and typo-tolerant search out of the box. However, Algolia is way more than speed and typo tolerance. On par with speed, there’s also relevance — having the best matching records always appear at the top of your results. To achieve this, you’ll need to set up a few basics. Scout Extended gives you direct access to these and other settings through Laravel Artisan Commands.\nEasier access to configuring Algolia is only one of the extensions we’ve added on top of Laravel Scout. Another feature of equal importance is support for aggregators: now you can centralize your search, creating a single point of entry for your entire website. Other improvements include full reindexing without any downtime for your users, extending the search builder, getting status overviews, and many other features. Let’s get into some of the details.\nIf you prefer to watch vs. read, here is a talk I recently gave on Scout Extended.\nAggregators — multiple models in one index\nAn aggregator is a clean way to implement site-wide search among multiple models. In other words, it allows you to have multiple models in one index.\nTo create a new aggregator, use the  Artisan command. This command will create a new aggregator class in the  directory:\nphp artisan scout:make-aggregator News\nAfter generating your aggregator, you should fill the  property of the class, which will be used to identify the models that should be aggregated:\nView the code on Gist.\nAn aggregator is a standard  class, and, as usual, you may begin searching on the aggregator using the  method or… the  method:\nView the code on Gist.\nTo learn more about Aggregators in Scout Extended, please review the Scout Extended Documentation.\nZero downtime reimports — painless to deploy\nWith Scout Extended, it’s a breeze to import data in production. To keep your existing search experience available while reimporting your data, use the  Artisan command:\nphp artisan scout:reimport\nThis Artisan command will output the following content:\n\nTo ensure that searches performed on the index during the rebuild will not be interrupted, Scout Extended uses a temporary index to import all records before moving the temporary index to the production index. We have you covered.\nConfiguring Algolia from within Laravel\nYou’ll start with the  Artisan command, to optimize the search experience based on information from the  class:\nphp artisan scout:optimize\nThis might be sufficient.  does its best to generate the settings of your  class index, but you may need to edit those settings in . For example, two of the most important configurations to get right are searchable attributes and custom ranking:\nView the code on Gist.\nOnce you have verified the settings file, all you need to do is synchronize the settings with Algolia using the  Artisan command:\nphp artisan scout:sync\nYou may also edit settings using the Algolia Dashboard. However, make sure you apply those settings locally running the  Artisan command.\nAn improved search builder\nScout Extended pushes the Laravel Scout's search builder to the next level, adding new methods and improving the existing ones. Here are some examples.\n\n\n\nThe  method may be used to compare a field's value against another value. With Scout Extended, this method shares the same API of the Laravel Query Builder, allowing you to filter results either via a comparison or a range numerically:\nView the code on Gist.\nThe supported operators are: .\n\n\n\nThe  method verifies that a field's value is between two values:\nView the code on Gist.\nYou can do the same with ,… , , and . Head over to Scout Extended Documentation to find out more about this.\nStatus overviews\nScout Extended provides a beautiful overview of your application indexes, allowing you to easily monitor the number of records, among other key metrics.\nTo get an overview of your application indexes, use the  Artisan command:\nphp artisan scout:status\nThis Artisan command will output a table with the following content:\n\nAnd more!\n\nAutomatic transformation of data before being send to Algolia.\nEasy integration with InstantSearch libraries.\nAccess API directly using Algolia's Facade.\nNeed to split large records? Use Splitters.\n\nScout Extended is 100% open source, so you’re free to dig through the source code to see exactly how it works. Dig right in! Visit the Scout Extended documentation.\nSee something that needs to be improved? Just send us a pull request on GitHub.\nThanks, we hope you enjoy this new release!"
  },
  {
    "id": "8003-0",
    "title": "Unleash Search Insights with Algolia Connector for Shopify Flow",
    "author": "Matthieu Blandineau",
    "text": "In 2017, Shopify announced Shopify Flow, an e-commerce automation platform enabling merchants to easily automate tasks so they can focus on growing their business.\nToday, we’re thrilled to announce Algolia connector for Shopify Flow, unleashing Algolia’s search and discovery insights via automated workflows between Algolia, Shopify Plus and the innovative e-commerce tools that have already integrated with Flow.\n\"We're excited that Algolia has built the first search-related connector. Businesses can now automate the powerful insights you can receive from a store's search data—a direct window into shoppers' intent and a source of potential opportunities—across their daily workflows, apps, and Shopify\" says Anthony Kentris, Product Marketing Manager at Shopify Plus\nHow does it work?\nCreating a workflow between Algolia, Shopify, or any app that works with Flow involves simply linking three elements, leaving all the hard integration work to the app connectors. The three elements are:\n\nA Trigger: an event happening in Shopify or an App. It can send data associated with the event.\nA Condition: a rule or a set of rules that determine if a workflow should run.\nAn Action: a task performed in Shopify or an App.\n\nExample of a workflow\nAlgolia and Shopify Plus: improved management and even more actionable search insights\nIn the 2018 Internet Retailer KPIs & Site Search Survey, 35.9% of retailers declare that in order to advance their search and discovery strategy, they would benefit from a better integration of search with their other system.\nAlgolia connector for Shopify Flow is all about this integration, starting with three initial triggers and their associated workflows:\n\nActionable Insights with the “Top Searches” and “No Results” triggers:\n\nTop Searches sends a list of the 5 most popular searches of the day, and the first 5 results of those searches. An email or a Slack notification can then be sent to the merchant with that information, so they can adjust… their inventory or merchandising campaigns on a daily basis.\nNo Results sends the most popular queries that returned no results. Like above, an email or Slack notification containing this information can be sent, allowing merchants to adapt their product offering to the wishes of their shoppers, or adopt more targeted acquisition strategies.\n\n\nImproved management with the “Indexing Paused” trigger: the ingestion of the Shopify catalog by Algolia -or indexing, that makes it searchable, can be paused. The “Indexing Paused” trigger is pulled when this situation happens. Merchants can then receive an email or Slack notification, so they don’t have to constantly check their dashboard to monitor indexing, and focus on their business.\n\nWhile we are actively working on additional triggers, workflows and actions to help merchants be more efficient, your digital storefront can already explore possibilities using the Shopify Flow platform. For instance, merchants can already create workflows that will include the top results of the week’s most popular searches in their weekly newsletter, or automatically create Asana tasks to remind them of adding products that would answer the no result searches to their catalogues.\nGet started\nWe’re excited to be the first search and discovery solution to join the Shopify Flow community.\nClick here to get started with Algolia connector for Shopify Flow.\nWe have even more exciting triggers and actions to come, but in the meantime, do not hesitate to share your ideas with us: What triggers or actions should we implement next? What would help you most in your workflow? Do not hesitate to share your ideas in the comments"
  },
  {
    "id": "8009-0",
    "title": "A Time of Transformation, A Look Ahead",
    "author": "Nicolas Dessaigne",
    "text": "For Algolia, 2018 has brought more than the usual fast-paced development and innovation. We’ve seen a continued trend of change in how businesses view search and discovery: not a vehicle for transactional, one-time online interactions, but a part of a new generation of inspirational, personalized experiences that keep users engaged and coming back for more.  \nForrester told us that “digital business leaders in healthcare, retail, financial services, and other industries must master the market forces and options for site search—or jeopardize the projected ROI of their investments in these high-priority, emerging business areas.” In Internet Retailer’s first-ever survey on site search and KPIs, 88% of respondents indicated that advancing their product search and discovery strategy is important. Most importantly, it’s the feedback we get from our customers: search and discovery must accelerate user engagement and improve KPIs. \nAs we enter the New Year, here is a roundup of how Algolia is responding to these market developments and customer feedback. \n2018: a year of more impactful releases \nWhat we heard loud and clear is that we need better clarity in highlighting the value we create for businesses as a whole. Our development in 2018 (a quick summary of our company momentum here) was largely focused on that feedback. It is the year we built a series of tools that augment the capabilities of our core API, aiming to become a comprehensive suite of solutions that offer possibilities to customize, combine, and optimize for specific and exact business needs. \nWe worked on features that would have direct impact on business KPIs such as conversion rates in e-commerce or time on site in media. In 2018, we released A/B testing and Click and conversion analytics: features that, for the first time ever, let businesses understand and measure the impact of changes in their search strategy to key performance indicators. We also launched a new version of… Personalization, which solves the issue of a “one size fits all” approach by dynamically displaying content and objects a user is more likely to be interested in based on prior behavior.\nWe heard from advanced organizations that their user-facing content often lies in various systems managed by different teams, adding complexity and cost to projects relying on such content, including site search and discovery. This is why we launched the Algolia Custom Crawler, removing the need for complex and costly internal project management, and accelerating time to value. Customers like LegalZoom have already benefited from this solution. \n“We realized that search should be a core competence of the LegalZoom enterprise, and we see Algolia\nas a revenue generating product.” \nMrinal Murari, Tools team lead / Senior software engineer\n\nStaying focused on our end-users, we redesigned our dashboard, both modernizing the UI and improving findability based on user feedback. Our new documentation is focused on use cases rather than just features: we are proud of finding a completely new way to architecture the information and making it much easier to discover.  \nAs a result, our momentum continues to accelerate with more than 2,500 new customers joining us, amongst them L’Occitane en Provence, World Wildlife Fund, Amplitude Analytics, PayPal, PubNub, and Rent the Runway. With their help, we learned a lot about how to approach key business topics. \n2019: architecting the future of digital businesses \nWe will continue to provide ultra-relevant, reliable and fast search and discovery experiences, and tailor our product to our customers’ particular business needs. At the same time, we want to help companies realize the larger benefit of integrating search into their business strategy. \nWhat does this mean for our product in 2019? As our customers’ needs evolve, Algolia will help provide inspirational, personalized experiences that not only meet but also anticipate user needs.… We will focus our efforts around the following three areas:\n1) A unique approach to AI in search\nAI (artificial intelligence) is great for automating a very specific and complex problem such as search. Being a hosted search solution with more than 6,500 customers in production whose configurations we can learn from, we are in a very unique position to solve this problem. We are building the AI that will proactively help every customer get smarter search by leveraging anonymous aggregated information from our various product configurations. \nBut AI is not a silver bullet, nor a perfect system. We use it only with \"white box approaches\" where we can give transparency and control of results to our users, and let them manually override specific elements. This approach is critical for complex issues like relevance. \n2) Conversational search\nWe have seen in the last ten years a massive evolution in the way people search. The latest big shift is conversational search, which we have have already worked on with some assistant use cases, and helped our customers add voice to their mobile presence.\n3) Continue building the most intuitive search technology in the market \nIn addition to our strong focus on the developer experience, we are expanding our dashboard to make sure all business users—product managers, e-commerce merchandisers, media content managers, etc.—have the information they need at their fingertips. \nWe are always working to identify and remove any and all end-user frustrations. With the advent of voice search, we envision the future where end-user frustrations with search will disappear, as the technology becomes more invisible and intuitive. \nNew packaging & pricing\nOur pricing in 2018 focused on the underlying hosted search API infrastructure, which didn’t directly tie to the results we delivered to our customers.  Now, we are excited to launch new packages that better tie to customers’ needs and the value we provide by more closely aligning the… price you pay with your search volume. \nOur new developer plans help to get projects off the ground starting with Community at no cost, as well as our paid Starter and Pro plans which provide volume discounts as you scale. Our Enterprise plans have the highest levels of functionality, security and support to scale with you over the lifecycle of your journey with added features & functionality. With these new pricing packages, Algolia can match any level of growth your organization achieves.\nIf you’re a current Algolia customer and are happy with your plan, that’s great! You can remain on your existing plan indefinitely. We support the commitment we made when you first signed up for our service, and will let you decide what works best for you moving forward. \nWe see this rethinking of our plans as the next evolution of our product: one that better delivers value to your users and your company.\nOur attention to communication, detail and feedback remains: talk to your product specialist, email us at hey@algolia.com, and stay tuned for more exciting developments in the new year."
  },
  {
    "id": "7902-0",
    "title": "Increase relevance with Personalization from Algolia",
    "author": "Elliott Gluck",
    "text": "Evolving from “one size fits all”\nThe web has historically been a centralized model for businesses to advertise and sell - one online destination for many to come to access content. Visit any of your favorite websites and you would see the same content as another user clear across the state or country. This standardization ensured a consistent experience whether a user is visiting to check sports scores or the latest performance of the stock market. \nAs the web evolved, so too did the user experience as this “one size fits all” approach was no longer the optimal strategy. From Facebook’s News Feed to YouTube & Netflix recommending videos to watch next based on your prior view history, the homogeneous content strategy began to change. Other businesses began to advance as well terms of how they matched users to the products & experiences they desired, leading to an improved experience. \nA more personal approach\nAt Algolia, we have been focused on personalization for a while now, constantly thinking about how we bring customized search opportunities to our customers. Today, we’re excited to release a brand new version of Personalization, which lets customers use a list of inputs about a user’s behavior to influence the content that this person is going to see. More specifically, we now allow businesses to take a wide range of individual online behavior & preferences, and optimize the search ranking strategy accordingly to show more relevant results. This is accomplished by using a list of inputs about a user’s online behavior to influence the content that this individual will see. While there are many different types of general digital personalization tools (for dynamic website content, email layout etc.) our Algolia solution specifically focuses on search and discovery. \nHow to personalize\nSo how, and more importantly what can be personalized? The two most prominent types are personalized search and personalized lists. Personalized search applies to… changing the search results to match the profile of a user once a search is performed. A personalized list by contrast displays a list of objects that match the profile of a user, but are outside of the search (think the “Recommended for You” choices Netflix provides upon entering your user account). While there are a number of inputs to potentially track, we are starting with the following initial events as potential inputs to personalize someone’s experience:\n\nViews: If a user has seen a product page, or a landing page\nClicks: What a user has clicked on\nConversion: When a user adds an item to its cart, makes a purchase, watches a video, reads an article, etc.\n\nIt is also worth mentioning that there are two big varieties of personalization strategies, that may or may not be combined together: reinforcement & discovery. Reinforcement relates to pushing items that you have directly interacted with individually (i.e you bookmarked a product) or as a group (you visited a landing page with multiple products on them) to try and increase the accuracy of results you are seeing. \nDiscovery by contrast is focused on highlighting items that are related to the items a user has directly interacted with. This works especially well for items that are paired together (need a case to compliment the new cell phone you just bought?) and focuses on discovering interesting items you might otherwise not have seen. All of these specifics are designed to let our users better control how the ranking will be impacted, using a mathematical approach to let businesses prioritize accordingly. While there are many other variables to track, we will look to potentially add more through future releases and iterations of the product. \nOne of the key benefits to our new Personalization release is how easy it is to configure and get up and running quickly. Most personalization strategies take a lot of work and development resources to get set up, and once live, it becomes even more burdensome to… gain visibility and insight into your relevance strategy. With Algolia, all you need to do is set your event weights (the relative importance of each) and start to send us the events, and we do the rest. Best of all, once you’re live, you can iterate on your formula and fine tune it to your needs in an instant through our dashboard.\nOur launch partners\nHelping us to launch Personalization are a select set of beta users who tested to provide critical feedback. One of our beta users is Videdressing, who has seen value in getting a personalized experience up and running quickly, as Product Manager David Piry notes “One of the great things about Personalization with Algolia is how easy it is to get set-up and running - there’s not a ton of development work, just using data you are already collecting.” Personalization solves the issue of a “one size fits all approach” and increases relevancy by displaying content and objects your users are more likely to be interested in. \nWant to learn more? Click here for more details."
  },
  {
    "id": "7954-0",
    "title": "Personalized merchandising: solving the context problem in e-commerce",
    "author": "Ryan Chen",
    "text": "How personalized merchandising gives the right context to products and users\nTelling you that ‘personalized merchandising’ involves merchandising products with personalization sounds like a pretty obvious no-brainer. Context matters. Without it, a definition like this isn’t going to do you much good.\nBut what if I told you that one of the biggest challenges that e-commerce retailers face is merchandising by providing context to their products and their shoppers?\nBeing able to merchandise on your e-commerce storefront should be a given. But when you consider the differences between a brick and mortar shop and a digital omnichannel ecosystem, there are challenges both physical and strategic that need to be overcome.\nUsing site search and personalized merchandising, you can solve the context problem by satisfying your user’s intent while also considering your business needs too. How does it all work?\nThe advantages of real-world merchandising\nMerchandising is a retailer’s way of affixing context to their products. Even as the context—seasonality, popularity, or strategic KPIs for example—continue to change, the goal remains the same: by spotlighting products that align with your context, you can accelerate users to the shopping cart, enlarge the average basket size, and increase overall conversions.\nFor avenues both physical and digital, merchandising should be a big part of a retailer’s promotional strategy, but this just isn't the case. Even as goals are the same, the approach is vastly different.\nConsider the traditional brick and mortar shop. The physical truth of these shops has a level of authenticity that an online storefront can never replicate. \nBrick and mortar shops are limited by their creativity to merchandise. Retailers know that a shopper’s threshold for stimulus can be swayed to trigger buys. With purposeful merchandising, you can squeeze every bit of a product’s exposure to level up visibility, foot traffic, and of course sales.… Some ways to do so include:\n\nWindow and in-store displays\nControlling the floor layout\nColorful decor\nLocation of promoted product\nGrouping related products together\nUpsells by the counter\n\nA retailer that runs a brick and mortar store may take for granted the decision and the action to, say, move a seasonal winter jacket to the very front of the store to promote it. Such flexibility and freedom of movement isn’t available on the digital front.\nMerchandising digitally is a different story\nFor e-commerce, in contrast, senses are replaced with scrolls and clicks. Suddenly, navigation becomes a challenge on digital storefronts. A typical user interface will have a menu bar and maybe some facets on the side, relying on the user to click through page after page to find what they’re looking for. It’s not so easy to get around with such a bare-bones navigation.\nIntegrating search into your omnichannel architecture can help with navigation. Users prefer to maneuver through an internal search engine—59% of users frequently use search to navigate and 15% would rather use search than the menu. As a result, using the search bar shows an uptick in average conversion rate, increasing from 1.7% to 2.4%.\nStill, shoppers need to interpret the online interface to find what they’re looking for, and this puts limitations on how businesses can merchandise digitally. Using search mitigates some of the roadblocks with navigation, but the actual merchandising, the actual product spotlight, is still a challenge.\nThe merchandising choices that e-commerce folks have are limited to the likes of promoting on the homepage or user interface. For example, you may be featuring a graphic advertisement on your homepage targeted at a seasonal winter sale. You may even be featuring several ads that carousel along, each different and tested for optimized performance. \n\nYet, how optimal can such a general merchandising method be? You see, merchandising isn’t a one-size-fit-all strategy. Each… user, each segment is unique, and casting the merchandising net that wide will inevitably lead to opportunities slipping through the cracks. \nHow can e-commerce retailers capture a shopper’s unique preferences while also connecting their context with the products being searched for?\nHow does personalization help?\nAdding a touch of personalization into the merchandising experience may be the answer. \nPersonalization reflects a user’s own unique context. Specificity, it turns out, is really important to users. 53% of online shoppers believe that personalized shopping experiences are valuable. Also, when personalization and proprietary customer data are integrated, revenue increases by 6% to 10%.\nWhen a shopping experience is personalized, outcomes are informed by user behavior in real-time across any touchpoint to show curated content for every segment. \nThat’s really just a fancy way of saying that personalization lets you respond to a user’s intent with content relevant to them, something Algolia has been focused on for some time now. \nIn the context of search, you can also say that personalization is a kind of ‘hyper relevance.’ If a user searches for ‘sweater’ but sees results for ‘socks,’ the shopping experience probably won’t feel very personal. \nPersonalizing search tailors the results shoppers see based on their preferences, and previews behaviors on your omnichannel platform. How accurate a results set is at approximating intent can impact user satisfaction—74% of consumers are frustrated when they see irrelevant results. \nWith a site search architecture in place, you can keep your relevance, and in turn, your personalization sharp. Site search captures intent not so much as a wide net, but like a precise arrow. Weighing several criteria like spelling and filters, products are ranked so users directly see the most relevant results.\nAmong the criteria, business metrics, like margin and units sold for example, can be considered within… the ranking process too. It’s here that search and personalized merchandising come alive.\nProviding context with personalized merchandising\nPersonalized merchandising works around e-commerce problems by communicating with users directly and individually, but most importantly, collaboratively.\nBy now, we know that the physical restraints that come with e-commerce make merchandising really hard. Evolving search beyond the box doesn’t only soften the navigation problem—it gives retailers the ability to tune search relevance and personalize the shopping experience. \nThis is where it gets interesting.\nAs shoppers who use site search moves along their buyer’s journey and peruse your digital shop, the experience will feel nearly personal, on account of the relevant results they’ll see. \nBut that’s not all. \nUsing personalized merchandising, voices from both sides are ringing loud and clear - not only is your user’s intent heard, your business goals are listened to as well.\nEach shopper is unique and their intent can take on infinite voices. A good site search engine provides textual relevance that ensures even proximal queries will match a user’s intent. No matter what the query may be, you’ll be able to return relevant content that touches on your merchandising goals for every variation, for every segment.\nThat’s the beauty of personalized merchandising—it takes the contexts of the user and the business and emphasizes them together, collaboratively. Woven into the results are products that match the user’s search intent as well as your business metrics.\nFor the folks at Lacoste, they know well the impact a site search solution can have on their omnichannel environment. \nA quick look at their homepage will show that on top of smart merchandising choices, they’ve added a prominent search bar at the top of the page.\n\nWhile we don’t know what business metrics the Lacoste team chose to emphasize, it’s clear that a deliberate effort was made to… spotlight some products over others. \nIn a search for ‘tennis,’ the top result a shopper sees is a premium collection that’s being promoted, even as the description is missing the collection search term itself.\n\nA search for ‘leather’ will show products Lacoste have collaborated on with partners to the top of the results.\n\nAfter having site search integrated on their online and mobile platforms, Lacoste saw a 210% increase in search use. As a result, conversion rates increased by 37%, with a 150% increase in sales contributions. \nIf you’re looking to transform your e-commerce business with personalized merchandising and provide the right context to both your shoppers and your products, talk to one of our search specialists."
  },
  {
    "id": "7875-0",
    "title": "Algolia's Global Roadshow",
    "author": "Ryan Chen",
    "text": "Search and discovery: from transactional to inspirational\nWe’ve heard it from experts, industry surveys, and our most successful customers: search and discovery are key to moving the digital conversation forward.\nThat’s why we’re bringing you the 2018 Algolia Global Roadshow! We’re hitting the road and coming to six cities around the world to share how search and discovery have transformed into inspirational experiences with meaningful results for business KPIs. \nThe Algolia Global Roadshow will be an afternoon of deep diving into advancing the conversation on search and discovery. Some topics we’ll focus on are:\n\nHow leading companies are pushing the boundaries of search and discovery to accelerate user engagement and improve KPIs \nImplementations and success stories \nThe future of search and discovery: moving past transactional to predictive and inspirational experiences\n\nIf you’re someone who owns the digital experience of your site and is interested in optimizing the ways your consumers find products and content, then you won’t want to miss our Roadshow!\nSpeakers ranging from CTOs to product leads and top partner agencies will share insights on topics like UX, personalization, voice search and relevance. You’ll also be able to build community with industry peers, Algolia execs, and product experts at the event. \nAnd, you’ll want to stay for the happy hour! \nJoin us at the Global Roadshow by registering for the event in your city. See you there!\nNovember 5, 2018 l  New York\nNovember 8, 2018 l  Amsterdam\nNovember 13, 2018 l London\nNovember 15, 2018 l Cologne\nNovember 29, 2018 l San Francisco\nDecember 4, 2018 l Los Angeles"
  },
  {
    "id": "7855-0",
    "title": "Black Friday & Site Search: small tips, big difference",
    "author": "Matthieu Blandineau",
    "text": "It's no surprise that during the holiday season, Black Friday & Cyber Monday are absolutely critical events for any e-commerce business. According to Adobe, online retailers earned $108.15B between Nov 1 and Dec 31 2017, up 13.8% from 2016. Only in the U.S.\nSince the holiday season presents an enormous opportunity, it can also be a hit or miss. Given how short the season is and how a mere two days of activity can so heavily reflect in sales numbers, something going wrong may mean huge losses in revenue. It happens to the biggest brands—In 2017, large retails experienced online outages on Black Friday, such as Lowe’s or H&M.\nHow do you make the most of the season while avoiding pitfalls? Here are our tips, with a focus on search and discovery.\nPrepare\nAre all your e-commerce systems ready to handle the charge?\nJust like any other day, shoppers will expect a Google or Amazon-like experience on your website. But more than any other day, the cost of failing that expectation is gigantic. \nYour competitors are spending just as much money and effort in acquiring customers as you. According to McKinsey & Company, a user will abandon bad sites for competitors after 2 to 3 seconds. This means that every additional delay on your website will lead visitors to your competitors and cost you revenue. \nTypical culprits that cause friction are page loading time, payment system availability–but did you know that search can also be a major cause of downtime? To give you a sense of the craziness during Black Friday 2017, some of our customers experienced 3,000 search queries per second for hours, if not days, with peaks over 10,000 queries per second, compared to 700 search queries per second on a normal day. \nWhether you handle your own search infrastructure or trust a search provider, you should make sure your systems are ready to handle such load increases.\nIs the entire experience ready for mobile?\nAccording to Adobe 2017 Holiday recap, mobile accounted for 55% of traffic and… 36% of revenue between Thanksgiving and Cyber Monday 2016. \nFrom landing pages to payment, your mobile shopping experience should not be an after-thought. Building mobile experiences means much more than having a responsive website. For instance, many of the difficulties on the mobile platform come from the extreme limitation of display space (learn more in our ebook: Mobile E-commerce Experience).\nA common mistake is wanting to display all the data at hand like you would on a desktop. Start by understanding how shoppers browse your catalog. For items that are selected visually, like shoes or clothes, you could present search results as enlarged pictures.\n\n \nKeeping Inventory Up To Date\nWhile creating scarcity is a powerful strategy to accelerate the purchase decision, showing products that won’t be shipped to your shoppers before the end of the holiday season will cause users to bounce. It also wastes screen real estate for products that won’t generate any sales.\n\n \nSo make sure that wherever you display products on your website, whether as a recommendation, your homepage, or search results, you don’t expose out-of-stock items. \nConsidering the volume and frequency of transactions during the holiday season, it sometimes means millions of updates to your product database every day. Every system needs to pick up on these changes, and this requires a very large scale, agile system.\nAnticipate your shoppers’ wishes\nYour product offering is of course one of your greatest assets, so make sure it answers your customer’s needs. Use your search analytics to discover what articles your shoppers are searching for ahead of the holiday season, and most importantly, what they are searching for but not finding. Use those insights to adapt your catalog and plan deals, or to improve your advertising and SEO strategy.\n\n \nStart Early\nJust like you are preparing for the holiday season, your shoppers are likely searching for the best deals ahead of the actual sales by… comparing prices, shipping times and costs, etc. This means they will visit your website prior to the season. Use it!\nLeverage your content\nYou and your shoppers generate valuable content about the products you sell, including blog posts, guides, product reviews and social media posts. This content can influence the purchasing decision of your shoppers, and position you as a knowledgeable seller, so don’t hesitate to surface it on your store, alongside product pages — or even in your search results.\n\n \nHelp your shoppers research\nShoppers who know what products they want will probably perform refined searches to find out if you sell them, or similar items. You can lead them to the right offers on your site by proposing saved search and search alerts. They will have an incentive to come back to your store (or you will have a strong reason to ping them). \nD-Day\nBe proactive with your merchandising\nUnlike brick-and-mortar stores, you have the luxury to reorganize your store front in one click, or even automatically. Use it! Shoppers will look for the best deals, so use the discount rate in the ranking of your search results. Create scarcity by ranking products with limited sales time first. You have a doorbuster offer, but still want to preserve your margins? Rank this doorbuster first for relevant queries, and then add your post-discount margins in your ranking logic. The parameters you can play with are endless. The important thing is to be able to configure the ranking logic of your search according to what matters to your shoppers, and you.\n\n \nAdapt to the unexpected\nWhether you like it or not, you will miss some trends and discover high selling offers that you wish you had more promoted. It’s not too late! Tweak your search results ranking logic to promote your best selling products, and capture even more sales.\nCapitalize\nAnalyze!\nThrough your site search, your shoppers literally speak to you. Leverage those invaluable insights! What were they looking for… that you did not offer? What search results converted the most? How did people navigate your store?  Did they filter by a specific brand when looking for TVs? All this data can help you improve your offering and the way you surface it to your shoppers next year.\nTurn first time shoppers into engaged customers\nThroughout the Holiday season, you will acquire new shoppers, thanks to your unmatched offers on specific products, and increased advertising efforts. Chances are, if they like the overall experience, they’ll return to your store. So when things calm down, reflect on what went well and what did not, and start planning long-term investments for your user experience. Studies report that every $1 invested in UX yields a $2 to $100 return.\n\nTake search. Search has moved far beyond the search box, to include advanced features like browsing, navigation, personalization.... The logical next step is search without the box. Key e-commerce strategies like omnichannel improvements and personalization are already becoming dependent on advanced UIs such as voice and chatbots (learn more in our ebook: Search beyond the box).\nUltimately, there isn’t one size fits all solution. As Forrester puts it, “Choosing — or even knowing if you need to invest in — a site search solution isn’t simple.” \nWe have more tips and best practices on site search - if you'd like to speak to a search specialist, click here. Meanwhile, we'd love to hear your tips for holiday site search. Leave a comment, tweet to us, or shoot us an email."
  },
  {
    "id": "7844-0",
    "title": "Better Voice Search Tools for Mobile: VoiceOverlay for Android and iOS",
    "author": "Marie-Laure Thuret",
    "text": "Over the past year, the number of companies asking Algolia questions about voice search has gone from a trickle to a torrent. Our customers and prospects have the usual search-related questions—organizing data, configuring relevance—and they want to know how best to handle voice input. To this end, today Algolia is announcing VoiceOverlay for iOS and Android applications, a UI component for developers to accept voice input for search and other purposes—whether they use Algolia or not.\nVoice is becoming a necessity, especially on mobile. You may have heard that 50% of all searches will be through voice by 2020, and already 71% of people would rather use voice to search than a keyboard. Users will consider mobile apps without voice to be less useful than those that do.\nHowever, be it for search or another purpose, handling user speech without tooling isn’t easy. The app needs to handle permissions with all their permutations, listen to the user, display the text on the screen, and then do something with it. Developers need to put a lot of work into piecing all of this together.\nVoiceOverlay gives developers tooling to handle voice input in their mobile applications quickly and easily. Taking inspiration from our InstantSearch libraries, VoiceOverlay reduces development time for this task from hours to minutes. It handles the entire flow, including:\n\nRequesting User permissions\nListening for audio\nRecording the voice input\nRetrieving text from the native iOS and Android speech to text\n\n\nAll of this comes in a nice and customizable UX that will fit in with any app, with a unified experience between the different platforms. Even more, the applications don’t need to use Algolia to use VoiceOverlay. We want all apps to be voice ready, and this is our contribution to making that happen.\nCheck out VoiceOverlay on Android and iOS, and tweet to us with your feedback. Interested in building robust, relevant voice search? Check out Algolia’s voice search… capabilities."
  },
  {
    "id": "7811-0",
    "title": "The journey to better DX: Create InstantSearch App",
    "author": "François Chalifour",
    "text": "InstantSearch by Algolia is a family of frontend libraries to create search UIs on top of Algolia APIs.\nToday, we’re introducing Create InstantSearch App: a command-line interface (CLI) to bootstrap InstantSearch applications from the terminal. All you need is Node.js ≥ 8, you don’t need to install anything else.\nView the code on Gist.\n\nCreate InstantSearch App preview\nWhy\nDeveloper Experience (DX) has always been in Algolia’s DNA. We constantly want to improve the interaction between developers and our products. A lot of efforts have been spent on the Algolia Dashboard to facilitate your search setup and on our API clients to communicate easily with our servers. On the other hand, our frontend solution to build search interfaces — InstantSearch — yet powerful, is quite complex to get started with.\nWhen only InstantSearch.js was available, kicking a vanilla JavaScript application was pretty straightforward. Now that we support many environments (React, React Native, Vue, Angular, iOS, and Android), you’re likely to spend more time on tooling rather than actual search implementation. We already provide good component primitives to build search UIs, but what could we do better on our side?\nWe drew inspiration from Facebook and more specifically Create React App — an awesome project that the React team is conducting to improve the React developer experience and onboarding.\nCreate InstantSearch App lets you focus on your project by taking care of all the tedious and repetitive work, regardless of your ecosystem:\n\nDo you have a new dataset to use? Run the CLI and enter the credentials to access this dataset.\nDo you have a new client to build a demo for? Run the CLI, enter the application credentials and start building the experience.\nDo you need help from the Algolia team? Use an online template generated by Create InstantSearch App and send it to us! We’ll do our best to help you make your search experience great.\n\nThis tool is about actual search… implementation rather than the process of building an app. It creates a working UI based on the given information. We’ve been using Create InstantSearch App internally for a few weeks before announcing it and it has immensely improved our productivity.\nHow we use the tool at Algolia\nHelp quickly delight users\nAt Algolia, Solution Engineers are customer-facing engineers building proof of concepts for clients and partners. They often need to create InstantSearch applications, demo and iterate on them. This tool helps them increase the speed of development to showcase what is possible for their business.\nHelp Algolia help you\nBug reproduction is often the best way to solve issues. Create InstantSearch App allows users to reproduce bugs either:\n\nLocally: by running the InstantSearch CLI and then hosting their app online\nOnline: by using the compiled versions for InstantSearch templates on CodeSandbox (example of InstantSearch.js template)\n\nThe latter is now our go-to way to help customers identify issues. We send them a link to the online template, they fork it and showcase the bug. This happens entirely in the browser.\nHelp you understand InstantSearch\nTooling should never get in the way of new users. While rewriting the entire Algolia documentation, we decided to skip the tooling explanation in all our Getting Started guides and to advocate using Create InstantSearch App. We even used it to bootstrap our new interactive tutorial.\nThis is only the beginning\nCreate InstantSearch App is only the beginning of our journey to better frontend DX. We have much more to achieve and are looking forward to sharing more improvements. Meanwhile, give our InstantSearch libraries a try!\nnpx create-instantsearch-app my-app\nYou can find the documentation and the source code on GitHub. If you have any feedback don't hesitate to let us know on Github, tweet to us or comment on this post."
  },
  {
    "id": "7820-0",
    "title": "For better school projects, a partnership with GitHub",
    "author": "Jessica West",
    "text": "Hello GitHubbers and Algolians alike! We have some exciting news we’d like to share with you. Algolia is so pleased to announce that we have partnered with GitHub’s Student Developer Pack to help students build search functionality into their projects freely and effortlessly 🎉.\n🤔 What does this mean for you? If you are a student and participating with GitHub’s Student Developer Pack, you gain access to Algolia for one year with a specific plan made just for you! You will receive 100k records and 1M operations—a value of $150/month and plenty to build the search project of your dreams—no special search knowledge or experience required!\nAlgolia is known for speed and ease of implementation and a focus on developer experience. We maintain API clients for all major programming languages and platforms, have a wonderful online community where you can show off your projects and get help, and generally do everything we can to make it easy and fun to build search. Check out our docs and tutorials to learn about key elements of great search and get started in no time.\nAnd, there is more! We are honored to be alongside amazing companies that make your coding lives easier: Heroku, JetBrains and GitKraken Glo are partners in this edition of GitHub’s Student Developer Pack update.\nRead more about this awesome collaboration here, and don’t hesitate to ping us with any questions via Twitter or email. We look forward to building search together!"
  },
  {
    "id": "7791-0",
    "title": "Moving on from GSA to Algolia",
    "author": "Elliott Gluck",
    "text": "Moving on from GSA to Algolia\nWith the sunsetting of Google Search Appliance (GSA) on the horizon, there has never been a better time to migrate to the speed, power, and flexibility of Algolia. Since many companies are thinking about switching off Google enterprise search, we wanted to outline some of the key differences between GSA and Algolia, whether large (goodbye hardware!) or small (our exposed relevance logic by default).\nBut before we jump into all the specifics, we want to better describe Algolia and the replacement options available to anyone looking to migrate - so let’s start with a bit about us. Algolia is a search API that gives developers a complete toolkit for building search & discovery experiences into their products. We are a global company with over 5,000 customers located in over 100 countries. Our goal is simple: to help companies across all industries create powerful, relevant and scalable discovery experiences for their users. We do this through our powerful hosted search API that provides product teams with the resources & tools they need to create fast & relevant search.\nBut will Algolia meet the rigorous requirements of your business when you’re used to the on-premise feature set Google Search Appliance provides? Here are a couple of the reasons why the answer is ‘Yes’:\n\nReliability - We know your business relies on search, and we stand behind our products with an available 99.999% uptime SLA with 1000x credit payback guarantee.\nScalability - Algolia effortlessly scales from your first record to millions of records with hosted infrastructure options that meet your business’s unique needs.\nRelevance - Highly customizable and configurable relevance formula and intuitive user dashboard ensure that even non-technical teams are fully in control of search results.\nSecurity - At Algolia, your data’s security is our top priority. Your data is always encrypted in transit, with available encryption at rest and dedicated infrastructure… options on 6 continents.\n\nHowever, we recognize not everyone might be ready to make the full jump from GSA’s on-premise solution straight to Algolia’s enterprise API’s. For these cases where indexing content using our APIs is not an option we also offer Algolia Custom Crawler, which automatically extracts any content directly from your website, turning it into structured data to make it easily searchable.\nUsing a crawler should not impact site search quality. One of our key differences is that we help our customers decide on the best search experience for their users and customize the crawler accordingly. Our customers can then leverage the relevance logic of Algolia that is transparent and completely configurable. In addition, our tie breaking algorithm allows you to choose what attributes to make searchable and rank according to your importance rating. Furthermore, to deliver the most relevant results to your users, our Custom Crawler can be extended beyond your pages to enrich your index with metrics from Google Analytics. If you’re interested about learning more about our Custom Crawler, you can reach out directly to our sales team to get your questions answered immediately. We also offer the support of several solutions partners as part of our recently launched partner program offering transition services to ensure the success of your Google Search Appliance migration.\nStill unsure which GSA replacement option is right for you? We have more information that will help provide further specifics as well as an upcoming webinar you can register for where we will walk through the Algolia product live.\n“Algolia makes the lives of all of our customers easier – and our lives easier, too. We were up and running on 3 different properties within 3 weeks thanks to the ease of development they provide.”\nAnne Obendorf\nDigital Content Manager\nCouchbase\n "
  },
  {
    "id": "7778-0",
    "title": "Introducing Algolia’s New Partner Program",
    "author": "Alex Popp",
    "text": "Today we’re excited to provide a new service to our customers with the launch of the new Algolia Partner Program. Enterprises can now tap into our trusted ecosystem of partners including agencies, systems integrators, and technology providers who can help them build best-in-class, Algolia-powered experiences in their websites and apps. \nBringing our expertise to a wider community of elite developers empowers enterprises to choose the right partners to build and accelerate development of their search solutions and push the boundaries of their user experiences. We’re thrilled to launch our program today with 20 certified partners including Accenture Interactive - Altima, Redbox Digital, Magento (an Adobe company) and Shopify. \nOur commitment to customers and partners \nWe obsess about how people interact with their favorite sites, find new products and articles, and explore content. Regardless of the medium — be it a mobile app, a website, or even voice or VR — people’s expectations for content discovery grow every year. Performance and functionality are increasing factors in whether a site or service will succeed in delivering a compelling experience. The challenge businesses face today is turning a near divine anticipation of desire into a personalized user experience that delights and invites each visitor to deeply engage and convert.[1]  \nAlgolia understands that an API-first approach to building search and navigation enables enterprise customers to be more nimble. It also catalyzes their ability to serve customers where they are and when they want, all while exceeding their expectations of a more personalized and contextual user experience. To build these types of solutions, organizations of all sizes turn to their preferred partners: local agencies, regional consultancies, and global system integrators. With Algolia riding shotgun, these partners no longer need to build dedicated search consultancy teams.\nHear what our partners are saying: \n“Consumer… expectations for content discovery grow every year, which means digital organizations must keep pace and provide personalized search experiences that fit each user’s needs,” said Alice Candidat, e-merchandising consultant for Altima, an Accenture Interactive-owned agency. “Algolia helps differentiate our practice by enabling our consultants to intelligently advise our customers, like Lacoste, on how to drive more revenue through their digital channels by appealing and personalizing to each individual customer.”\nJonty Sutton, CEO of e-commerce agency Redbox Digital, states, “In e-commerce, one the biggest pain points is matching a users’ intent to the product they are looking for. If you don’t match what a user is browsing or searching for, a retailer undeniably loses money.”\n\"The Algolia team has been invaluable in helping our team understand the full capabilities of their software as it relates to our client projects, so we can present the tool as part of our comprehensive web strategy and direction,\" said Jon Immel, senior vice president of digital strategy, TrendyMinds.\nAlgolia is powering toward a future in which search and discovery extends far beyond simple search bars. To ultimately provide UI-less digital products that can deliver extraordinarily relevant experiences, we will have to take that journey with our partners. Today’s launch is the first exciting step toward realizing that vision.\nTo learn more on how to become a consulting partner, navigate to partners.algolia.com, or contact alexandre.popp@algolia.com.\n_______________________________________________________________________\n[1] Source: “Le Search: De L’intention à L’Attention”, Think With Google (June 2018).  https://www.thinkwithgoogle.com/intl/fr-fr/tendances/vision/le-search-de-lintention-a-laction\n "
  },
  {
    "id": "7766-0",
    "title": "The Faces of Algolia: Meet Sasha Prokhorova",
    "author": "Alexandra Prokhorova",
    "text": "Today’s Faces of Algolia story features Sasha Prokhorova, our Product Design Team Lead, whose passion and background in Geography opened some unconventional doors to a world of product and user experience (UX) design. From San Francisco to Paris; new parenthood to design bootcamp; and everything in between — hear her story!\nSasha Prokhorova, Product Design Team Lead, Algolia\nWhat is your role here at Algolia?\nI am the team lead for our Product Design practice. This means that I am looking after the team while still contributing to projects. We’re a small team, so it’s all-hands-on-deck.\nOn the team side, I focus on helping our team scale: hiring and onboarding new team members (we’re hiring!), creating and defining processes to ensure design quality, coaching and mentoring, shepherding career growth, setting goals and objectives for the team, ensuring we’re focusing on the user, and so much more. I also spend a good chunk of my time educating other groups on design thinking and its value. I collaborate with other teams to understand where product design can support them and vice versa.\nOn the project side, I try to bring our users’ voices to what we do. The product design function is quite new to Algolia and many people here have never worked with product designers before, so a large part of what I do is showing and guiding teams through the design thinking process and how listening to our users brings value. \nHow did you get into product design?\nLike many in my field, I found my way to UX and product design by chance. A geographer by education, I've always been fascinated by human-culture-environment interactions. Geography is an incredibly diverse discipline and my favorite parts of it were spatial statistics (Geographic Information Science), data visualization and qualitative research (talking to people, ethnography). Those three balance out my analytical and creative sides, and satisfy my curiosity about social matters, how humans work. This was a… great foundation for UX design, which I didn’t know existed as a field at the time. \nPrior to design, I worked in consulting, client management and operations. When I was an account manager at a cleantech startup, I loved understanding my client’s needs, but I didn’t love being the middleman between them and the solution-makers. I wanted to be the solution-maker. That’s when I started talking to UX designers at my company about what skills I’d need to acquire to make the leap. Those conversations helped me decide between going into a Master’s Program, a design bootcamp, or trying to self-teach. In the end, I chose a design bootcamp, because I felt my academic and professional experience gave me a solid foundation, and the bootcamp would help me acquire basics of visual and interface design.\nNew parenthood, design bootcamp, and a new career are big life changes - what inspired you take these big leaps?\nThe only complicated part of my decision to make a career transition was the fact that my husband and I decided to start a family at the same time. When I quit my job, I was three months pregnant. It was a “now or never” type of situation, and I don’t regret taking the risk one bit. I made my decision after significant reflection on what matters most to me, which strengths of mine I would like to harness, and what type of impact wanted to make. I also asked myself: “What is the worst that could happen?” If design didn’t work out, I could go back to my previous path. My husband’s unconditional support also made me feel more comfortable.  With a baby on the way, I knew I needed to sort out the career side of things and find my place in the professional world, which would help me be a better mom. \nWhy Paris, why now?\nI always had an unexplained love affair with France. Maybe it’s simply because, as a Russian girl, I always learned about France’s profound cultural impact on my homeland. No one in my family was crazy about France, and I’ve… never been to Paris. For some reason, though, at 13 years old, I just had to learn French. I self-taught for a year, because my middle school didn’t allow me to join the class a quarter late for fear of being too far behind. Next year in high school, I started Beginner French. I was quickly taken under the wing of Madame Clarke, a tiny French woman in her 70’s with a lot of punch. She transferred me to her Advanced French class for which I was completely unprepared, and I stayed with her all 4 years of high school. \nI continued French in college and studied abroad in Paris. Living with a host family in the City of Lights gave me a deeper appreciation for French culture and people. I didn’t want to leave. It was not a surprise to anyone that 7 years later, I married a French man. \nWe moved here, because we wanted to live closer to my husband’s family and my relatives in Russia and Kazakhstan. We also felt like France’s family-oriented policies and culture would better support us. One year in, I see the pluses and minuses of life here, but am still completely in love with this place.\nWhat’s the most exciting part about being in product design today?\nJust like with anything in tech, Product Design is constantly in flux. There are new technologies disrupting the way we do things, so we can never get comfortable. We have to keep learning and re-adjusting our processes and approaches. The move away from interfaces is a great example of this. The way you design for a voice-only product is completely different than how you design an experience for a interface. I love that I continue to learn every day and that this field challenges me to have a growth-oriented mindset.\nOn the other hand, in a market like Paris, the understanding of product design is relatively immature, which means I have the opportunity to educate others on the value it can bring. It isn’t always easy, but extremely rewarding when I see a skeptic turn into an advocate."
  },
  {
    "id": "7698-0",
    "title": "Culture as a Growth Driver",
    "author": "Nicolas Dessaigne",
    "text": "Today is a special day for Algolia. Less than a month after being voted as the #1 private cloud company to work for by Glassdoor and Battery Ventures, we are recognized as one of Inc. Magazine’s Best Workplaces.\nWhen we received the Glassdoor award, I found myself saying to friends (only half jokingly) that this was one of the rare awards that really moved me. It’s a recognition of the hard work we have done and our continuous investment in building a company we love.\nToday’s award from Inc. Magazine is the icing on the cake! And it’s all thanks to the great people on our team and their continuous dedication to making the company — and all of us — better.\nCulture eats strategy for breakfast\nCompanies fail every day. Even big companies, with the most advanced strategies crafted by the most competent individuals, get disrupted out of relevance regularly — and that’s especially true for technology companies (think AOL, Blackberry, Nokia, Yahoo..). The best way to avoid that fate is to build a team that is able to ride the waves of disruption, a team that is able to challenge itself, a team that is able to constantly get out of its comfort zone, and most importantly, a team that never becomes complacent. Chances are your next innovation is already in the head of someone in your team, not your CEO. This person needs to feel empowered to step forward.\nThis is the very reason why we have built a culture-first company.\nIt’s impossible to anticipate every challenge but it’s possible to intentionally design your culture and build the right muscles to address them.\nCulture as growth strategy\nThe common take on culture vs. growth is that you have to make a choice. Will you risk sacrificing the culture to grow your business? Or will you risk sacrificing your business to preserve your culture?\nThis past year has been significant for us in terms of growth. In 2017, we crossed the $20M ARR milestone, opened new offices in New York, Atlanta and… London, added over 100+ people to our global team, and introduced new leadership to our executive team. With such growth, there is great temptation to take a few shortcuts on culture: things like cutting corners on our culture-first hiring process, decreasing face time between our many teams and offices, or not devoting enough executive time to employee satisfaction and growth.\nMy take is that sacrificing the culture may indeed help in the short term, but it’s creating cultural and people debt that we would eventually have to pay back. Either big time by enabling a culture that goes against our values for the sake of short-term profits — eventually disappointing and losing the exceptional team that brought us here, or with slower growth caused by disengagement and turnover later on.\nI simply don’t think we’d be anywhere close to what we have become today if we had not been intentional about the type of company we wanted to build. That helped us attract and empower the talent that led us to who we are today. It’s the same team that takes pride in innovating and making everything we do first-class — because we are engaged, vocal and accountable to our values day in, day out.\nMake culture a strength in your growth, not something that slows you down.\nI have a philosophy… if you hire the right people and empower them, then magic will happen.\n\nLet’s not become complacent\nWe’re proud and humbled by the recognition we’ve received from Glassdoor and Inc., but we have so much more to do.\nWe have not made it yet, and we know it. It feels good to celebrate with the team, but we’re continuing to look forward, not backward. Many more challenges await. The faster we grow, the faster these challenges fall on our shoulders.\nCulture is never “finished.” We want to continue building a culture of empowerment, diversity and inclusion. Whatever our future size, we want team members to have a sense of meaning in their work lives.\nToday, we have a strong… set of core values, shared and cherished by all in the company. It’s a great foundation, but we need more. We are creating operating principles to streamline our day-to-day decisions, as well as launching new diversity and inclusion initiatives.\nOur goal as we continue growing is simple — we want to make sure we are building the company we want to work for tomorrow and in years to come.\nOriginally appeared in Algolia Stories on May 23, 2018."
  },
  {
    "id": "7676-0",
    "title": "Inflect + Algolia: Modernizing the Buying of Infrastructure Products",
    "author": "Ryan Chen",
    "text": "Inflect is the only global, neutral online marketplace for the buying and selling of internet infrastructure products. Inflect’s goal is to make the process as easy as it is to book a hotel or airline ticket. \nInflect attributes its ability to deliver the buying experience the industry needs in large part to Algolia’s search-as-a-service platform. Algolia allows the Inflect engineering team to focus their development efforts on the back end, data normalization efforts and overall user experience. \nWe talked to Jason Barry, Inflect’s front-end lead, to learn how his team went about finding a search solution, their Algolia implementation, and how various search features serve their users. \nTell us a bit about Inflect: what does the company do and what problems does it solve? \nThe internet infrastructure industry is comprised of two big buckets of service providers - Infrastructure as a Service (IaaS) and Network Service Providers (NSP). The IaaS market’s current estimated size is $65B and growing at 21% CAGR, and the NSP market is projected to reach $20B by 2020. \nInflect’s main customers today are either hyperscale companies or enterprises looking for colocation and interconnection services globally. Today, a majority of these services are purchased through antiquated, manual sales methodologies. Customers have no easy way to search across multiple vendors’ offerings, and are forced to look at each service provider individually and try to piece together their own solution. \nThis means that they are making very important decisions based on old data and assumptions. Inflects aims to modernize this sales process by creating a system of record for the industry and creating a common marketplace for everyone to transact. \nWhat led you to needing search? How did you discover Algolia? \nSearch is the focal point of our user experience. The first thing a user sees on the home page is a search field. After logging in, the user is redirected to the search page. Our… goal is for users to find the solutions that fit their needs in the shortest amount of time possible.\nWe like to use our engineer-hours wisely: we didn’t want to build search from scratch. Instead, we decided to investigate services that had a feature set that met our technical requirements. We wanted a client-side search library with robust performance connecting to a distributed network, thorough online documentation, and flexibility and customizability. As front-end engineers, we didn’t want to spend time tweaking column indexes or optimizing SQL queries—we wanted to build our product without search getting in the way.\nWe looked at Swiftype initially, but I recommended Algolia after working with it at a previous role. \nTell us a bit about the implementation process. \nImplementation has been a breeze thanks to Algolia's documentation and dedication to open source. We decided to go with the algoliasearch npm package because it was lightweight and straightforward.\n\nHow about implementing specific features - any interesting details? \nAutocomplete \nWe recently added autocomplete to our search field. Customer response has been very positive. Every type of thing that we track gets returned by autocomplete: locations, data centers, service providers, peering networks, and internet exchanges. To do so, we created two indices that get queried at each keystroke: one index for locations, and one index for everything else. \nOur locations index is composed of cities, states, countries, continents, colloquial regions (e.g., Silicon Valley), and airports. We wrote a script to grab the bounding box of items of each of these types and threw them in an Algolia index. We use a custom weight attribute in our ranking formula to control rank in the case of tie-breaks so that popular locations are always displayed first.\nOur other index contains information about search facets from our datacenter_search index. Selecting an item from this autocomplete index would trigger the same… action as selecting a checkbox on the datacenter_search index: for example, choosing “InterXion” would display all the data centers where InterXion is the colocation provider.\nOur 99th percentile autocomplete response time is 14ms. \nThe response is so fast that we don't have to debounce text input—debouncing actually made the perceived response time slower. \nGeolocation\nWe're using Mapbox to display search results from Algolia on an interactive map. Algolia makes this very easy, supporting geolocation search out of the box. By setting a _geoloc key on each record, you can add the aroundLatLng search parameter to query by a given lat/lng pair. On the end of a pan or zoom, we can create a new Algolia query with the current search query and filters coupled with the bounding box of the map. This helps us avoid massive responses of data only visible outside of the viewport. \nImplementing this functionality on our own would have taken a non-trivial amount of time, but with Algolia, it was as easy as passing an argument to a function.\nSynonyms for mergers/acquisitions and common names\nIn our industry, mergers and acquisitions are common. Bigger service providers buy up the smaller players, and change the marketing name of the data centers they own. The problem is that users still refer to them by their original names, even though the new names are under new ownership.\nAlgolia’s Synonyms feature allows us to address this problem gracefully. We have an array of known mergers and acquisitions set as one-way synonyms so that users can find data centers and their providers under their former names. Setting this up is as easy as filling out a web form on the index level — everything else happens automagically. \nThe icing on the cake is that a hit’s highlightResult has the markup properly noted. For example, if a user searches for “Telx” (a company bought by Digital Realty), Digital Realty shows up first with the resulted bolded around the text of the alternative… synonym. This is also useful to find products or data centers with a commonly spoken name whose marketing name is actually different.\n\nAbove, you can see that searching for “zcolo” (a phrase known to refer to Zayo products and services) returns information about Zayo. \nConjunctive and disjunctive filtering\nWe have a set of filters containing thousands of different service providers, peering information, compliance, etc. Most of the time when selecting filters, users are looking for the intersection of the checkbox options they choose—they want to see what matches this AND that. \nThis works in all cases except where items of a facet are mutually exclusive. Colocation providers tend to fall into this category, because for many the relationship between them and data centers are one to one.\nWe wanted the filtering of colocation providers to be the union of checkbox options (an OR), and everything else be an intersection. Algolia makes this very easy by giving developers control of which facets should be searched conjunctively versus disjunctively. This way, a complex query like “show me data centers owned by Equinix or Digital Realty that have Level 3 and Comcast on-net” can be supported just by checking a few checkboxes.\nZero-result queries\nA feature we’re making more use of is being able to see search queries that return zero results. Looking at this data helps us fill in the missing pieces that our customers expect. From a business perspective, this gives us leverage when asking companies for their data—we can approach them with the frequency of these zero-result queries as an incentive to join our platform.\nWhat have been the benefits of using Algolia either for your team or your users?\n\n\nA key benefit Algolia has given us is the ability to iterate without getting in our way. Our back-end engineers were able to upload the data in the structure we wanted without a hitch, and afterwards we were able to query and configure the indices to our heart’s… content without asking for help. The time we saved using Algolia allowed us to focus on building our actual product.\nOur customers are happy with the speed, relevance, and our ability to solve their business problems through the search functionality."
  },
  {
    "id": "7663-0",
    "title": "The Faces of Algolia: Meet Steven Merlino",
    "author": "Steven Merlino",
    "text": "Today’s Faces of Algolia post features Steven Merlino, a business development representative on our Sales team. After moving to San Francisco and joining Algolia as one of our first SF team members, he dove into the local community and has been raising money for people affected by life threatening diseases such as HIV/AIDS in the LGBTQ community. \nHear his story!\nSteven Merlino, Business Development\nWhat is your role here at Algolia?\nBeing an early team member in our San Francisco office, I’ve had the opportunity to wear quite a few different hats, but my primary role now is in Business Development. When I joined the team in November 2016, I was brought in to kick off our inbound sales strategy by creating the processes for managing our inbound lead program. Once that role was stable, I trained new people to take over our inbound business.\nAs I started to look at new opportunities within the company, I wanted to challenge myself a bit and try something new, so I joined our newly-formed outbound team. Outbound sales at Algolia requires us to dive deep into problems we can solve for businesses. This involves evaluating the performance of their websites, and educating them in areas within their search experience that could be improved — typically around speed, relevance, UI, and UX. This process takes many hours to find the right company to target, the right people to reach out to, and to perform the evaluation itself. While this role has been challenging, I still wanted to push myself further and that’s when I started to get involved in other activities outside of work.\nWhy did you decide to get involved in the local community?\nUnlike the majority of the team in the San Francisco office, I was new to the Bay Area — I moved here from Atlanta. While the move was exhilarating and being in a new city was amazing, I wanted to feel connected to the city I lived in, similar to the way I felt back home. That’s when I decided that I should pick up one of my old… activities from back home - Cheerleading. \nThe first thing I typed into Google was Cheer San Francisco and boom — there was a team called Cheer San Francisco! I emailed the team saying I wanted to join, so they invited me to one of their invitationals and that’s when I discovered that this wasn’t an ordinary cheer team. These cheerleaders were all adults, all volunteering their time to raise money for charities in the Bay Area that helped the LGBTQ community. I was truly blown away and from that moment I knew that I had to be on the team. \nWhy is this organization meaningful to you?\nSimple — the people I’ve had a chance to meet and lives I can impact. While at events raising money, countless people that have had a friend, loved one, or even themselves affected by HIV/AIDS have thanked us for bringing awareness to illnesses that impact the LGBTQ community. With charitable donations, it’s easy to forget at times that there are real people directly benefiting from your efforts — for me, the interaction with the community that we impact keeps me coming back. \nCheer San Francisco started back in 1980 as a means to raise spirits when the AIDS epidemic was significantly impacting the LGBTQ community. We were established as a 501(c)(3) nonprofit organization in 1993 and in 2004 we started the Cheer For Life Foundation to help other cities across the country do what we have done in the Bay Area. Since 2004 we have raised over $350,000! \nHow can we get involved? \nThat is actually a great question. As a fully volunteer organization, we are always looking for help 😉 Information for how to get involved can be found on the Cheer SF Facebook page and on their website. On these pages, you will find the local organization we are supporting, like the Shanti Project, videos of our performances, where to find us next, and a donate option if you would like to support our cause. \nIf you are interested in joining the team, we are having tryout clinics on June 26th, June… 30th, and July 1st, with tryout being held on July 3rd. It doesn’t matter if you are old, young, have experience cheering, or no experience at all — we would love to see you! You can find out more information on our Facebook page about tryouts and if you want to apply you can do so here!\nI hope you enjoyed learning a little more about me and Cheer San Francisco 🙂 With San Francisco Pride coming up this weekend (June 23rd and 24th), you can come say hi and meet the rest of the team! We will be in the parade on Sunday and performing on the main stage in front of City Hall on Saturday at 2:10pm and Sunday at 1:45pm. Hope to see you there! \n-Be Proud. Be Loud.\nCatch Cheer SF at Pride Weekend!"
  },
  {
    "id": "7646-0",
    "title": "Optimizing search performance with A/B testing",
    "author": "Nicolas Baissas",
    "text": "To date, businesses have been testing search relevance in the dark—with little to no performance data or post-query metrics to inform optimizing search results for engagement. Algolia’s Analytics API and Click Analytics solutions started solving this challenge by providing you a complete view into the entire search lifecycle—from query to click-through to conversion.\nThe A/B testing feature rolled out today on our Enterprise plans empowers you to test performance impact of a change in the configuration of your search settings. The best part: you can create A/B tests entirely from the dashboard, without a single line of code (and of course, it is also doable entirely from the API).\nWhy A/B testing?\nSearch and discovery are core to the digital experience, and relevance is the one aspect of search that can’t be overlooked. Nothing will have as much impact on the quality of your users’ experience than the relevance of results returned by your search engine.\n\nLet’s take e-commerce as an example.\n\nOptimize for conversions: there are numerous criteria an e-commerce website could use to improve relevance so that users can easily find what they are looking for and convert faster; for example, promoting product sales, popularity, ratings, in search results. But it can quickly get overwhelming. Where to start? How to make sure those new criteria aren’t counterproductive? A/B Testing allows continuous, iterative optimization, criterion by criterion.\nValidate business decisions: there are various parameters that define an e-commerce business. Maybe your operational costs require you to favorise high-margin products. But how to make sure doing so does not have  negative impact on sales? A/B Testing adds confidence to the decision making process.\n\nFrom the parsing of textual tokens, to the impact of your business metrics on the ranking formula, to the importance of the proximity of words in the query, Algolia offers dozens of features and settings allowing you to… fine-tune the relevance to achieve great search results.\nFor Algolia users, here is how\nLet's take an example where you have an index currently used in production. Let's call it indexA.\nThe ranking strategy on this index currently relies on the number_of_views of each result. You'd like to know if changing the ranking strategy to rely on a number_of_likes instead of the number_of_views would lead to better search conversions.\nA/B testing allows you to do exactly that, by following these steps:\n\nCreate a replica of the indexA—we’ll call it indexB (make sure beforehand that you have enough space to duplicate the data)\nChange the custom ranking of indexB to use number_of_likes instead of number_of_views\nIf you haven't done it yet, implement click analytics. It will be used to measure performance improvements caused by your configuration change in index B\nGo to the A/B testing feature on the dashboard, and enable an A/B test between indexA and indexB\nWait to get enough traffic until the significance score of the AB test reaches 95%, and that's it!\n\nDepending on the result, you'll know if your new ranking strategy leads to more clicks and conversions on your search, so you can proceed with the changes with confidence!\nWe hope this feature will help you iterate faster on your configuration and achieve the best relevance for your search. And we certainly plan to guide you along the way and help you make the most of it!\nWe invite you join our upcoming webinar: “Optimizing search performance with A/B testing” and read the documentation or visit the product page to learn more\nDon’t hesitate to get in touch and share your questions and feedback: shoot us an email, tweet to us, comment on this post."
  },
  {
    "id": "7624-0",
    "title": "The Faces of Algolia: Meet Marie-Laure Sin",
    "author": "Marie-Laure Sin",
    "text": "Introducing The Faces of Algolia: our brand new series connecting you to the people who make magic happen. As we build our team across multiple continents, five cities, and countless cultures and backgrounds, we’re excited to give you a behind-the-scenes peek into our world. \nToday’s post features Marie-Laure Sin, a software engineer on our Intelligence Squad, who began her career in marketing in the Big Apple, and made the big leap to a life in engineering. Hear her story!\nMarie-Laure Sin, Engineer at Algolia\nMy job in marketing: a tech startup in New York organizing events\nLike many of us, I was one of those people who never really knew for sure what they wanted to do. Sure, I knew I liked being part of an interesting project, organizing things, preferably in a challenging environment, but it was not a calling like some of my friends had. So I kept trying: a few months in a luxury brand as a press attaché, an apprenticeship in a big French corporation in the international marketing department. Then, when I graduated, I had this opportunity to join a tech startup in New York.\nI was in charge of the events' strategy coordination. Being a tech startup, attending events to demo the product was one of the main sources of leads for the sales team. The first few months were intense: settling down in a new country, working with very different and smart people. The change with the big bank I just left was brutal: instead of waiting for several rounds of approvals, I was in charge of half a dozen events, handling things from negotiation, to marketing collateral, last minute issues with the booth and all the suppliers but also all the logistics around manning our booths. I had to constantly be on top of everything, while travelling most of the time. I liked the fact that it was a very hands-on job which was impactful, that was a nice change from the comfortable environment of the bank.\nWhy this decision?\nHowever, after a few months, the adrenaline started fading away:… the startup was doing great, and we had committed to sponsor a lot of events. It started to be a bit repetitive and I started to get a bit bored: all the events would be the same, and I had no prospects to do something else.\nI still liked spending time with engineers. Engineers intrigued me; although I didn't understand everything they were talking about, they all seemed to be very excited and proud of their work. They would always chat very enthusiastically about a new technology, and then they would debate over the way to use it, or help each other, quoting articles that they had read or liked.\nThey seemed very passionate about their work, and what I really liked was how they would help each other out when something wasn't working. Of course, they would sometimes get stuck on something, but one of them once told me: \"sometimes, I don't know the answer and I can spend hours or even days working on something, but it is like a police investigation where you're trying to find the culprit, and at the end of the day, I would always learn something new\". Some of them were really encouraging, telling me about how anyone could become a developer with some motivation and a lot of hard work.\nI really wanted to do something that challenged me intellectually, so, in my free time, I took a bootcamp about Computer Sciences: the CS50 by Harvard (which I really recommend!). That bootcamp was a game changer: it manages to make Computer Sciences entertaining and teaches you concepts ranging from algorithms to data structure, security, web development and much more! I started to see things differently and even caught myself asking how certain things worked behind the scenes: it really opened a whole new perspective!\nTaking the leap\nAt that point, I knew I wanted to pursue this path, so I quit my job and took a 5-month full-time coding bootcamp in Paris. Starting all over again wasn't easy: there were so many concepts that I've had never heard of, each one of them implying a lot of… other (also unknown) concepts, but the feeling you experience when the thing you've been working on for hours is finally working is incredible!\nSometimes, you can get lost along the way, or you can feel lonely. I decided to join a bootcamp called 'Women On Rails', where we would meet every week and get teamed up with a mentor. From time to time they would do a session on a particular topic (\"the basics of Git\" for instance), otherwise, we could follow their  track, or work on our own project. The mentors are very supportive and friendly and it has helped me tremendously! Being able to share my experience with people who were in the same place, but also people who've been through that always gave me a motivation boost.\nAnd so the job hunt began…\nI had a pretty clear idea of what I was looking for:\n\nI wanted to work in a startup, because, well after experiencing it, felt the vibe of it, there is just no going back. An English-speaking one was a huge plus\nA product that I could relate to: coming from marketing, my 'product' side didn't disappear in the process, and I knew I needed to really like the product and be able to talk about it\nA supportive environment with people willing to share their knowledge and a real team spirit!\n\nA few disappointing interviews in, one of the mentors at Women On Rails told us about this job offer for a junior developer for a startup that \"really seems committed to source a real junior developer\". At that point, I had read hundreds of job offers, and I wasn't sure what he meant. But when I read it, I understood. Where everybody else was listing all the technologies that I needed to master, how frequently I had to ship brilliant code, this job posting was focusing on motivation, making each other grow. \nSure, at this point it could have been only good advertising, but it was already a good start. Interestingly enough, it turned out that Algolia was hosting a Women On Rails meetup a couple of weeks later, so I was going to be able to see… for myself how it was! And I wasn't disappointed: as soon as I walked in, somebody welcomed me, sat with me and asked me what I was doing and told me how happy she was about this special meetup. I could see people talking openly and exchanging  ideas in a very open and constructive way. There is a special feeling that is hard to describe, but something resembling genuine friendliness, and I instantly felt at ease. That's when I knew I had to apply!\nJoining the team and on-boarding process\nA non-negligible number of interviews later, I finally got the answer I was hoping for: I was going to be a member of Algolia's InstantSearch team!\nI remember being very impressed with the on-boarding process: everything is done to make you feel comfortable (I cannot recall how many times people have checked on me to make sure everything was going smoothly). During the first couple of weeks, I had an overview planned with a member of every team in the company! While it seems like a lot, it is definitely a great way to learn how Algolia works and really helps getting to know everybody.\nInstantSearch on-boarding with Marie\nI began working with Marie, a well seasoned developer who was then in charge of the React InstantSearch library. The on-boarding went smoothly since it was gradual: first of all we did an overview of how the project was built (something as simple as: where to find this or that). Then I did my first project that consisted of doing a demo with pretty much all of the widgets from the React InstantSearch library.\nThen we took on the project of developing my first feature: the breadcrumb. I first sat down with Marie and we defined the different steps of the process. From one time to the other, I would know exactly what I had to do (that is to say, what I should achieve in terms of output/results) and I always had some time to look into it and sometimes get a bit lost.\nEvery step of the way, Marie was there if I had any questions, or even checking on me… regularly. Every other day, we would meet to define new goals. Sometimes we would even work together an entire afternoon on a more difficult part. \nIt was great in terms of motivation but also very interesting since I would learn a lot (debugging skills, how to find info) etc. \nKatas\nAt the same time, Marie suggested to do a Kata (it is a short exercise which helps programmers hone their skills through practice and repetition) twice a week. We would usually choose a theme (manipulating arrays, learning regex). The idea is to work on a concept isolated from a big project, proving useful because then I'd feel more comfortable working on it \"in real life\" since I've already done that.  Upon discovering the exercise (that usually come with handy things such as tests that you need to make pass), the goal is to get the job done regardless of any style/optimization. Then we would see what could be improved. During the next session, we would do them again until it becomes natural.\nI really liked those exercises, since the complicated side of getting to know how a project is articulated goes away. And during one hour, I can focus and I would always learn something new...it's also a great way to mingle and do team building! \nAnd now what?\nLooking back, one of the first things that comes to mind is that in just over a year, I've come a long way! If someone had told me I would have contributed to our InstantSearch libraries with not only small improvement but also features, and done support for the company, I wouldn't have believed it!\nAfter a few months, I got the opportunity to switch teams. I am now part of the internal tools team. Our mission is to help people within the company be more efficient. The work is very different from what I was doing in the InstantSearch team: we focus on aggregating data from various sources, but also automating things. For instance, the team built a Slack bot that reminds people when they have interviews, then they will be pinged to send in… their feedback, etc. It is very rewarding to be able to help solve our team's day-to-day problems, and also get their feedback right away in order to improve our tools!\nThe team is comprised of senior and talented developers, so we have to learn how to work together and it can take time to adjust (and we are still learning). It is not always easy, but I am lucky to be learning in a supportive environment, where it is okay to not know something and to voice your concerns, and other people will care and try to help you.\nI don't know what tomorrow will bring: sometimes the learning process can be overwhelming and frustrating. But no matter how hard it is, there is something that I am certain of: at the end of the day, I've learnt a thing or two...and that feeling is just priceless!"
  },
  {
    "id": "7539-0",
    "title": "Supporting the Open Source Software that Supports our Success",
    "author": "Martyn Davies",
    "text": "At Algolia many of us are working with, contributing to, or benefitting from open source projects on a daily basis. We’re all about the tools that help us to get the job done, and get it done right.\nYet the majority of the software that we’re using to create the fastest search experiences on the internet is not coming from large megacorps. It's coming straight from the laptops of talented developers all over the world who are passionate about solving a particular problem and sharing their solution with the world.\nMany open source projects are kept alive and iterated on by just a small group of developers, who often claim no pay for their efforts despite their work being used by many other developers to create awesome apps.\nBuilding up an open source project that finds a decent level of adoption in 2018 requires much more time, consideration, planning, and general day-to-day management than it has ever needed before. Luckily, the mindset of “just because it’s open source doesn’t mean it’s free” sits well with many developers and larger companies alike.\nThe rise in adoption of services like OpenCollective, has made it incredibly easy to donate to your favourite, or most used, open source projects as well as gain insight from the project owners around how that money is put to use; very transparent, but very useful too.\nWe love our community and support it whenever we can; including helping with open source projects, but it was time to take things further. So, we thought that we’d share a little about the projects we’ll be donating to over the next 12 months.\nFirstly, a little background on how we arrived at the list below.\nIt all started with the suggestion that we take a monthly budget for OSS contributions and split it up between the 13 different squads (the micro teams that form the engineering department here at Algolia) and let them choose which projects they would like to support.\n\nOnce the votes were counted we arrived at a list of 18 projects… that were used extensively throughout the company. Some projects received votes from multiple squads, so they were weighted higher and allocated a larger amount.\nOthers were more squad specific, so were allocated lesser amounts, but only so we could support as many as possible and still provide a little financial support to every project that got a vote rather than push more money at the most used.\nOpen source projects we're supporting in 2018\n\nBabel - Helps to make sure our JS libraries are forwards and backward compatible.\nPreact - Lighter and more performant React rendering library used in the front-end of InstantSearch.js.\nStorybook - Allows you to browse a component library, view the different states of each component, and interactively develop and test components. Also used on InstantSearch websites.\nOpenStreetMap - This is where we get most of the data for Algolia Places.\nCheerio - A superb library that makes parsing and crawling websites in NodeJS a breeze.\nCodeSandbox - An online IDE with awesome niche features like two way GitHub editing capabilities. Used extensively throughout the company for demos but also supported via InstantSearch templates.\nRubocop - Because good code quality is important for the health of any project. This helps our Ruby projects maintain their code standards as per the Ruby project guidelines.\nVueJS - The incredible front-end library that we all love and cherish.\nScala - A great language that enables us to ease our development process, thanks to its amazing compiler.\nWebpack - Build solution to bundle JS files for deployment to just about anything that’s also running JavaScript, from the browser, to the server, to desktop apps. There isn’t a single JavaScript project at Algolia that doesn’t use Webpack today.\nMaterialUI - A strong set of React components that implement the design standard set out by Google’s Material Design specification.\nRuby Together - Donate to this and you catch the maintainers for RubyGems, Bundler… and other great shared tools within the Ruby community.\nRollup - A JS module bundler that helps you create a tiny payload from your original source code.\nMobx - A state management library for modern JS frameworks.\nFaker.js - Great testing requires great testing data, and Faker is one of the best at making sure you get what you need from its support of 30+ localities and a large number of API endpoints.\nPug - A templating language with powerful features, written in JavaScript and designed to work in node.js and on the browser. We use it for all the website that make up the Community side of Algolia.\nNodemon - Monitors for any changes in your source and automatically restarts your server. Oh, the hours we save by using this!\nShields.io - Living in a Readme near you. We use Shields to identify build statuses, code coverage, version numbers and much more across all our own OSS projects.\nWe aren't just giving support to these projects, we're also giving our time to others. For example, some projects that we wanted to support did not have a donation page and said, \"We'd prefer you to spend time on issues and code rather than donating money\".\nThat's a very valid ask, and that's exactly what we'll do. By working with those projects, and by providing speedy documentation search to many others via our own DocSearch project, Algolia has been giving dedicated engineering time (which means money for any company) to many other developer-driven projects as well.\nWe’re very much looking forward to adding to this list in 2019. If you’re donating to OSS projects at the moment, or simply have your own list of awesome tools you’d like to share,  then feel free to reach out to us on Twitter, or in our Discourse, and let us know about it.\n "
  },
  {
    "id": "7595-0",
    "title": "How to Hack User Research",
    "author": "Alexandra Prokhorova",
    "text": "In this article, we’re sharing how we’re getting creative with conducting user research to show its value quickly and dispel notions that it’s an expensive bottleneck.\nIn theory, most of us in tech know that we need to listen to the user, understand their needs and test our products early and often. In practice, it’s hard to do when we’re running by the seat of our pants trying to launch features. User research is commonly associated with prolonged timelines, additional expenses, and results that are interesting, but hard to act on (e.g., “our users don’t immediately see the value of our product”).\nYour colleagues may be tempted to skip research altogether and “learn and iterate after it’s live.” That approach can create the illusion of saving time in the short term, because the product is out sooner. However, in the long term, and I’ve seen it, it can lead to wastefulness. If you release a product whose most important hypotheses are not validated even a little, you may have to spend a lot of effort redesigning and rebuilding the product (potentially incurring tech and design debt in the process).\nBy working with several start-ups and now at Algolia, a hyper-growth, developer-led organization, I’ve learned that I can be creative and agile in order to balance research with moving quickly. It means spending less time on recruitment and picture-perfect deliverables. It means sometimes talking to a not-perfect-fit-target-user, but still getting extremely valuable information.\nEvery organization is different and sets its own expectations for deliverables and project outcomes. This is what has worked at Algolia, and I am not sure it would have the same positive reception at a large multinational corporation. Still, you will pick up a few tips here that may work for you.\nKnow what you’re trying to do\nLet’s go back to the basics for a moment. What you want to learn will determine whom you will recruit, how you will test and how much time you… will need. If you want to test usability of your feature, you’ll run a usability test. If you want to conduct exploratory research to generate new feature ideas, you may run an observational study or in-depth interviews. Defining your goals as a first step in your project will help you to pick the right research tools and to keep the project scope focused and manageable.\nSave time on recruitment, not on research\nRecruitment, in my experience, is the most painful part of user research. You can pay recruitment companies to do it for you, but if you have a modest budget, you can be creative to economize time.\nEven if you can’t recruit your ideal target user, it is still better to have someone walk through the prototype than no one. You can recruit in coffee shops: offer people free coffee for 5 minutes of their time (pro tip: avoid the morning rush). You can try posting an ad on Craigslist. You can go to a co-working space and set up a research station. Ask your friends to take a look when you see them over the weekend.\nLet users come to you. If there a company event/meetup coming up that will attract your target users, set up a research station and give out swag or gift cards as compensation. People will be there because they are already interested in your company. Capitalize on this! Make them feel like they’re helping shape your company’s future. Train others within your company to help so you can test with even more people.\nA company I know has Support Days, where once a week, users can come in and get their questions answered in person. Next to the support folks, there is always a researcher recruiting participants and running usability sessions\nUse tools like Calendly to minimize the back-and-forth of scheduling, which can add up time.\nTest with fewer people\nI usually go for 5–7 users (within each user type category). Most of the time, you will have enough data with this number to see trends. If you don’t see trends or the patterns are weak, then you… can add a few more participants. Some research is always better than no research.\nApproximate your user\nWhen you need results quickly, something is better than nothing. This is especially true if you are testing a prototype and are looking for general usability. Instead of spending time trying to find the ideal user profile, you can approximate your user and test with people of similar profiles. Let’s say you’re building a web app that will help sales folks in the solar industry draw up preliminary plans for a solar project. You want to test the usability of your drawing tools, which they will use to place a project on the map and generate a solar panel layout. Do you need solar sales people to test this? No! You need people of the same general comfort level with technology to see how they would use the tools. On the other hand, if you need to test the usability of the industry-specific input fields, then you definitely need to test with your target user and there isn’t a way around that.\nOne important note: this technique does not work for generative research, in which you need to get as close to your exact user as possible. If you don’t understand your actual potential users’ problems, you may not build the right products.\nABC — Always Be Conducting\nFinally, it is much easier to do research if you have target users in the pipeline. It takes a bit of organization up front (more here), but can help prevent scenarios where you need to test, but don’t have participants. If you are a product designer and research is not your full-time job, dedicating even 2-4 hours each week to planning user research upfront may save you time when a research need arises in the future. Constantly recruit and set up conversations, testing sessions, etc. Try to create a pipeline of research calls and activities that you can later adapt to your needs.\nUse all the tools\nSpeaking of tools… When you’re short on time, you should use every opportunity and tool to get the… data you need, which means stepping outside of the one-on-one interview comfort zone.\n\nI will start by mentioning the myriad of user research tools out there: Validately, UserTesting.com, UserZoom, and so many others. You can read about pros and cons here, and here is an oldie, but goodie on how to select these tools from Norman Nieman Group.\nJoin sales and implementation calls with your colleagues. These can provide a lot of information for you.\nDoes your company use Intercom? Set up mini-research questionnaires using the “Engage” feature to gather data and to recruit for research.\nDo you have a support team or a call center? Listen in on the calls if you can. Go through the support tickets and conversations and follow up with users for further details. Speak to the support team members about what they’re hearing as well. (Keep in mind that they may speak to you in terms of solutions: “Our customer would be happier if we built X.” Try to get to the bottom of the problem rather than focusing on the solution first.)\nThink about implementing an analytics tool like Kissmetrics, Google Analytics, Hotjar or FullStory on your site to get some insights about how it’s being used. While these tools tell only a part of the story, they can give you some indication about quick wins on your site. For example, we recently learned through Hotjar heatmaps that the “Search UI” section on our docs page is too hidden. As a result, we will make it more prominent in the next redesign to ensure we share our Search UI expertise in addition to our technical know-how.\nTalk to the subject matter experts. Sometimes, they will tell you more than ten users will. Recognize their biases (and tendency to suggest a lot of solutions), and use their wealth of knowledge.\n\nThere are many, many tools at your disposal to get insights into your users’ needs and thoughts. Learn more about the different tools here.\nShow results quickly, often — and don’t overdo it\nAs tempting as it… is, don’t wait for the grand finale results presentation. Synthesize results as you go along and whet your stakeholders’ appetite. I usually do a synthesis session at the end of each day and then a larger one at the end of the week. I like to send out a research update each day or every few days to my team (that includes PMs, developers and other interested parties). This creates a sense of progress and my stakeholders are happy to see some early insights.\nWhen hacking research, try not to fall into the deliverables trap. I usually don’t spend a lot of time on picture-perfect research reports. I compile key findings and recommendations focusing on the “so what” rather than spending time making pretty persona sheets for the sake of creating attractive deliverables. They take time and may impress people at first glance, but if they walk away not knowing what to do with these deliverables, then my time wasn’t well spent. Focus on impact of your research. What action should your stakeholders want to take right after they read your results?\nAs I mentioned before, it is important to consider your organization’s expectations and culture while implementing these methods. One time, I consulted with a large, traditional company, where presentation was essential to selling the idea. Our program manager hired a video crew to make a short film about our project. To me it seemed excessive, but the project would not have succeeded without it. So keep in mind what type of delivery will ensure your project’s success, but don’t be afraid to go leaner on the presentation if you can get away with it.\nMost people in my company love hearing what our customers experience when they use our product. They are hungry for insights. It is our responsibility as advocates of user research to adapt our techniques to the organizations we serve so that we can keep giving our users a voice."
  },
  {
    "id": "7556-0",
    "title": "Building Real Time Analytics APIs at Scale",
    "author": "Sylvain Friquet",
    "text": "We recently redesigned our analytics API from the ground up, in order to provide near real-time analytics to our customers on billions of search queries per day. Here's how we did it.\nIntroduction\nOur first analytics system started as a small side project in the early days of Algolia.\nIt consisted of batches of compressed log files being sent from all our search servers to a central storage service, before being pushed to an Elasticsearch cluster, from which we served the analytics queries.\nNaturally, this system had its limitations. Firstly, we needed to pave the way for our new Click Analytics feature, where we wanted to leverage a more relational model. Doing so with Elasticsearch, where documents are independent, would have been too cumbersome.\nSecondly, with dozens of billions of records to handle across many nodes, managing such cluster was becoming a full-time job. Being a SaaS company, we often talk about the value of focusing on your core business rather than on maintaining tools. As such, we were ready to migrate to something easier to manage so we could focus on building features.\nOur search analytics provides our customers with insights about how their search is being used.\nThese insights range from overviews, like the number of searches or the number of unique users, to actionable business insights such as ‘What categories are users most interested in?’ or ‘Which queries return no results?’. All of these can also be done over specific time ranges which, in some instances, can be very very large.\nWhat follows is an explanation of the tech choices we made when rebuilding our analytics from the ground up, as well as an overview of some of the design principles we put to use.\nChoosing the datastore\nThe top level requirements we needed to meet were:\n\nHandle ingestion, storage, and deletion of billions of events per day.\nReturn results in sub-second for most queries, regardless of the time range being requested.\nScale with us. We're doubling our scale… every year, so the selected solution should be able to keep up with that growth painlessly. Ideally by just adding more machines.\n\nWe started by evaluating powerhouses like RedShift, BigQuery and ClickHouse. While they are certainly good options for data warehousing, we did not find them good enough for real-time analytics workflow.\nIn our case, the focus is on performing sub-second analytics queries rather than long running analysis.\nHowever, achieving sub-second aggregation performances on very large datasets is prohibitively expensive with RedShift, and not possible with BigQuery.\nAdditionally, another issue we found with BigQuery was that the pricing is driven by usage, not storage. As such, we didn't consider it a safe choice to power a public API.\nFor ClickHouse, we were curious to try it out but ultimately found that there would be a lot of extra engineering overhead required to host, maintain and fine tune it.\nFinally, our search led us to Citus Data and their Citus extension for PostgreSQL, that makes it seamless to scale Postgres by distributing tables and queries across multiple nodes.\nCitus Data also provides several extensions that are well suited to real-time analytics such as HLL (HyperLogLog) and TopN. The former is an algorithm for fast approximative distinct count, and the latter acts like a heap, which allows keeping top items sorted by frequency in a JSONB field.\nIf we opted for this solution, we could spread our customers' data across many nodes, leverage collocation, and precompute metrics. As a bonus, we would still get the benefit of having a relational database running up-to-date Postgres instances.\n\nWith the data store decision made, here’s how the rest of our analytics solution works in practice.\nCreating near real-time analytics\nAchieving sub-second analytical queries doesn't come out of the box though. We achieve this both by distributing data across shards and by using a roll-up approach.\nTo achieve this we have to perform the… following steps:\n\nIngest raw events into the system.\nPeriodically aggregate events into dedicated rollup tables.\nQuery the roll-up tables and get the result in milliseconds.\n\nIngestion\nWe take advantage of the Postgres COPY command to insert batch events into Citus. As we'll see below, we never query the raw data directly so those tables can stay very simple schema-wise.\nWithout a lot of indices to update, the performance of insertion operations is quite impressive. Citus advertises ingestion of up to 7M rows per second (that’s 600B rows per day!) and our own benchmarks showed similar trends.\nWe distribute our data by customer. A single customer’s data lives on the same shard so we can take advantage of collocation. A request targeting a single customer app will only ever need to target a single Postgres instance.\nRollup\nWe don’t serve metrics from raw events. At some point, datasets, even for a single customer, can become too large to process on the fly if you’re looking for sub-second response time.\nAs a rule of thumb, you can expect to aggregate 1M rows per second per core with PostgreSQL.\nWe instead use roll-up tables. Rollup tables hold pre-computed metrics for a given time range.\nAs we've seen in the introduction, we often need to return tops and distinct count. This is made easy thanks to the TOPN and HLL extensions mentioned above.\nHere's a simplified rollup function that covers several use cases:\n\nThis function aggregates all queries received in a given time range into buckets of 5 minutes, and computes their count, the unique number of users (using HLL), and keeps the top queries, along with their respective count (using TOPN).\nThis function will be executed concurrently across all nodes.\nFor our analytics solution we have several levels of rollups. We aggregate events every 5 minutes, and further aggregate them by day. We do this for several reasons:\n\nIt means the metrics returned by the API are updated frequently. Roughly every 5 minutes, the… data is refreshed, so customers don't have to wait long until their queries are reflected in the dashboard.\nTo compress our dataset even more, we keep daily aggregates. This means that at some point the granularity is reduced to the day level, but this is acceptable for us. We could even consider rolling our dataset by month after a point, which would allow us to keep offering close to unlimited retention, at the cost of coarser granularity. The opposite is also true, adding intermediate hourly rollup could also be an option in the future.\nSince we're precomputing tops, we can't incrementally append queries to them throughout the day. Doing so would yield very incorrect tops. Instead, we build many tops across the day, and we merge them all at once by EOD. We're still doing a top of tops, but considering their fine granularity and the fact that we merge them all at once, in the end, we only saw a minimal difference compared to a more exhaustive (and much more computation intensive) approach.\n\n \n\n \nA consequence of such an approach is that once the data is rolled up, we can delete it. We don’t need to keep terabytes of raw events in Citus to serve metrics and deleting data is made easy thanks to Citus’ ability to parallelize deletes across nodes.\nAnalytics Queries\nThe API targets the roll-up tables, never the raw tables. If we compare the number of rows from the raw tables and the roll-up tables, we see a compression ratio ranging from 50,000 to 150 on average (this of course varies based on the dimension picked for the aggregation).\nThis is what makes this approach work so well. Since ultimately our metrics are pre-computed per day, we can easily understand why we get results in milliseconds across virtually any time range: the amount of data to scan at query-time is trivial.\nFor instance, to get the count of queries for a given customer for the past week, it’s a quick index scan to fetch 7 rows living on the same instance followed by a sum.\nTo get the top… 1000 searches for the past month, we fetch 30 rows, unnest the top items, and further aggregate them into our final top 1000.\nHere are a few simplified example queries to demonstrate:\nTo get the count of queries:\n\nSince our metrics first live in the 5min rollup table before being merged into the daily roll-up, we query both tables and union their result at query time.\nTo get a distinct count of users, we take advantage of the HLL type which can compute the intersection of several HLL field.\n\nAnd last, but not least, here's how fetching the top 10 queries would look:\n\nAll our analytics queries follow this pattern, where only a minimal aggregation logic is performed at query time.\nThe big picture\nLet's take a step back and have a look at the whole system:\n\nBefore actually inserting our queries into Citus, we first aggregate keystrokes into queries. Most of our customers have an InstantSearch implementation. With InstantSearch, results are obtained as you type. This is a great user experience that under the hood generates many API requests for a given search. For example, searching for ‘Algolia’ may result in up to 7 queries (A, Al, Alg… etc). When it comes to analytics, we only want to provide insights on full queries (someone searched for Algolia once), not the intermediate keystrokes.\nTo ensure this is the case, we have a pipeline that processes our logs to aggregate the sequence of keystrokes into searches, before pushing them straight into Citus.\nWe rebuilt this pipeline using Go as our language of choice. We’ve always had a microservice approach at Algolia, and over the past year we started building most our services (with the exception of our search engine) in Go and have been very satisfied so far, both in terms of performance and productivity.\nTo complete the picture, we rely on Kubernetes on GKE for orchestration, and Google Pub/Sub for communication across services.\nConclusion\nThanks to Citus and a roll-up based approach, our analytics handles… billions of searches per day across thousands of customers, a number that is rapidly growing.\nSo far, we’re very pleased by the performance and scalability of the system and are looking forward to building more products on top of it. Indeed, our analytics infrastructure is also becoming the foundation of new products and features such as our recently released Click Analytics API.\nIf working on such scale and infrastructure is of interest to you, we’re hiring!"
  },
  {
    "id": "7490-0",
    "title": "Three Best Practices for Search Autocomplete on Mobile",
    "author": "Lucas Cerdan",
    "text": "Using Search Suggestions is a familiar pattern that has been around for years, and after reviewing thousands of implementations, we want to share with you some best practices. Here’s what you need to know.\nAutocomplete or autosuggest? Several names, same pattern\nWhether you are calling it an autocomplete or an autosuggest, a pattern used to display queries suggestions or search predictions, it is likely that you’re speaking about the same kind of a search experience. This pattern has been around for years and you are probably using it every day on websites such as Google or Amazon:\nExample of an autocomplete that provides search suggestions as you type\nThe concept is pretty straightforward: the search engine suggests several predictions in order to autocomplete your query as you type. Search suggestions are particularly powerful on mobile devices and for e-commerce marketplaces. Instant search experiences that display actual results instead of completing the user queries are a good example of the right pattern in other use cases.\nExample of a rich autocomplete that goes beyond search suggestions\nAn effort-saving pattern on mobile\nThere are a couple of constraints on mobile that you need to design for: limited screen real estate, and the “fat finger” effect (touch interfaces are even more prone to users’ mistakes!).\nThese constraints dictate a few reasons search suggestions are a better option than directly displaying search results on mobile. Using autocomplete will minimize the number of characters one has to type, thus reducing potential for typos (even though your search technology should handle them anyway). It is also easier to fit 3-5 suggestions on a screen, especially since you can expect the keyboard to be opened. Displaying results tends to require more space (pictures + 2-3 attributes on average).\nSuggestions are also helpful for extending queries. For example, you probably wouldn’t have typed “iphone car mount magnetic” on your own, but… having it suggested might help the user find the exact words to describe what they're after, thus making it more precise and leading to more relevant results. It also guides your users and sets their expectations: if a suggestion exists, it means there are actual results behind it (otherwise, why would it even be suggested?).\nFor all these reasons, suggestions might help your users find the right content faster. However, a prerequisite is getting the implementation right.\nImplementation—doing it well\nSpeed: as-you-type experience\nDisplaying suggestions should be blazing fast. Several studies already demonstrated that there’s a 100ms threshold between an experience that a user feels in control of and an experience that makes them feel like they are waiting for a machine to answer.\nSuggestions should start with the first character. It’s not that the user will expect perfect suggestions with just one letter, but they will see right away that an autocomplete is there to help. Waiting for a few characters to kick in might (unpleasantly) surprise them, and they might ignore them entirely.\nHighlighting the differences\nHighlighting is crucial in search, because it helps the user understand why the results match their request, and allows them to quickly decide which result to choose first. Without highlighting, the chances of the user choosing a bad result are higher, which can lead to a disappointing experience with your product.\nA common practice is to highlight the part that matches the user's input; however when offering search suggestions, the inverse is true: it is important to highlight the part that is being suggested. This approach visually aids the user in distinguishing between suggestions as it highlights the differences.\nOn the left, Ali Express. On the right, a better implementation by eBay - even though the contrast could be improved.\nTap-ahead pattern\nHave you ever noticed this icon next to suggestions? \nTake a good look at the apps you have installed… on your phone, and you will start seeing it everywhere.\nMany apps are using the tap-ahead pattern. From left to right: eBay, Youtube and Google Maps.\nDid you know this was not only an icon, but is used for a specific action? If not, don’t worry—our own studies showed that 80% of people on our panels had no idea this existed—but those who did use it profusely.\n \nAmazon on iOS: a perfect implementation of the tap-ahead pattern\nWhen taping on the icon, the suggestion will replace the initial query in the search box and the user can continue typing. This is a great way to reduce the amount of typing while having very precise queries. In this example, I only typed “i”, and found exactly what I needed in four taps.\nA good implementation requires:\n1. Use of the arrow icon that points to the search box\n2. On tap, filling the search box with the suggestion\n3. Staying on the same view while instantly updating the other suggestions according to the new query\nPretty simple, yet even Amazon struggles to keep consistency between its platforms (here, failing with point 3):\n\n \nIf you would like to learn more about the tap-ahead pattern, the earliest mention I could find is in a 2011 Smashing Magazine article.\nBuilding relevant suggestions\nSuggestions are only helpful if they’re relevant. We already mentioned how each suggestion should lead to actual results. Most of these suggestions are being built by analyzing what your users are searching for on your app (popular or trending queries).\nHowever, be aware: never blindly trust user-generated content. Castorama, a French home improvement tools and supplies retailer, learned this the hard way in 2016. Some of their online visitors realized that typing the same query multiple times in a row would be enough for it to start appearing in the autocomplete for everyone. It only took a few minutes for the website to be flooded with inappropriate suggestions that I won’t share in this article. Castorama had to shut down their… website for weeks and missed many sales opportunities.\nGoing beyond suggestions\nSearch suggestions is only one interesting pattern in the world of search. There’s a lot more you can do to create a great search experience.\nPinterest is doing a fabulous job of mixing suggestions with top results for People and Boards\nCombining suggestions and instant search is something worth experimenting with, and not just on mobile.\nAt Algolia, we like to push the limits of search from technology and UX perspectives. Lately, we've been exploring the pattern below, which shows results and suggestions at the same time.\n\n \nWe have yet to prove or disprove it, but we're working on it. Feel free to try the demo yourself, and please send us feedback in the comments below or via Twitter"
  },
  {
    "id": "7504-0",
    "title": "Zendesk + Algolia: A Discovery Experience to Delight Your Customers",
    "author": "Maxime Prades",
    "text": "Forrester Research tells us that 76% of consumers use self-service Help Centers to find answers to their questions. Your support team spends hours creating those answers—what good is all the work if they can’t be found? At Algolia, we are all about accelerating the find and improving the customer experience.\nA little under two years ago, we released an extension for Zendesk, our first integration with a Help Center solution. Today, we are proud to become an official Zendesk Guide and Zendesk Guide Enterprise partner.\nBuilding better customer experiences\nZendesk and Algolia have a shared goal of empowering businesses to be more reliable, flexible, and scalable—and, above all, empowering them to build best-in-class customer experiences.  Our partnership makes the goal easily attainable.\nThe concept of frustration and wasted time in seeking support is fast becoming obsolete. To provide great customer experiences, we must meet or exceed user expectations previously set by big players like Google and Amazon. Search is the primary mode of discovery across the web: making it powerful is critical to reducing the friction for users to find the right answers.\nHere is what Algolia brings to your Zendesk Help Center:\n\nInstant answers (even to mistyped questions). You can direct customers to the right content with results that update with each keystroke, regardless of typos. An advanced search reduces the number of support tickets and elevates customer experience at the same time.\n\n\n\nFederated search. It’s not just about the support center. Break down content silos and empower your customers to find answers across all of your content.\nAnalytics & insights. Truly understanding your users’ intent is key to improving your support center content. Most companies build their support center content based on recurring requests from their customers. That’s a nice way to prioritize which content is most relevant to your users —but here’s a better one: a powerful… analytics tool. Looking at your search analytics will help prioritize new Help Center content, based on what your customers are searching for—including queries returning no results.\n\n\n\nFully customizable. Our code is open source. Make it your own and design the experience that best fits your brand.\n\nWins from the frontlines of customer support\nFor Derek Daniels at Handshake, a self-service knowledge base was a must-have to enable their three different customer constituencies to find the right information quickly and easily. In looking for a solution to connect customers with the content to solve their issues, the Handshake team discovered the Algolia for Zendesk Guide integration. Since enabling the integration, Derek found significant drop in browsing category pages “as people started using search to find the articles they needed to resolve their issues instead of wandering aimlessly through the Help Center.”\n“Algolia enhances Handshake’s Help Center search, provides the Handshake team with better analytics on what people are searching for, and helps customers by surfacing more relevant articles.”\nOur customers have seen results including a 27% decrease in basic support questions, lower numbers of tickets submitted to support, decrease in ticket resolution time, improved federated multilingual search, and enhanced relevancy for all queries.\nGetting started\nFor Algolia and Zendesk customers, we made it a simple 3-step process; details here.\nIf you haven’t tried us out yet, sign up for a free trial with Zendesk and Algolia. We promise to improve your Help Center experience for you and your customers."
  },
  {
    "id": "7492-0",
    "title": "Magento 2: Our New Extension is Here, and It’s Full of New Features",
    "author": "Jan Petr",
    "text": "Search on an e-commerce website is a tricky subject: if you want to win and keep customers, you need to stay ahead of the digital experience game. Our Magento customers trust us to help with the job. Here is how the new Algolia extension for Magento 2 goes further.\nInfinite scrolling\nYou called for it and now you have it! No more annoying paginating for your customers. Offer them a smooth browsing of your catalog while staying on the same page. Just turn it on in Algolia configuration in Magento and you’re all set!\nStandard pagination:\n\nInfinite scrolling:\n\nAdaptive images\nHave your users been searching for a black jacket, blue t-shirt, or a pair of orange shorts — and all they got was a results page full of images of products in different colors? It won’t happen again. The extension now indexes images of all colors, and whether you’re searching for an item in a certain color or filtering by color, the right images will be displayed on the results page.\nNo more confusion and frustration for your customers.\n\nPrevent backend rendering\nDon’t let your customers wait while you load and generate all pages on a server. Algolia extension re-renders the page on the front end anyway. The extension now comes with an experimental feature to disable HTML generation of a catalog on your server.\nIt removes all HTML of the catalog from generated HTML code. This means that search crawlers like Googlebot, Bingbot and others might not find all your products and remove some pages from their indices.\nBefore enabling this feature, please read the documentation and make sure you watch your search engine traffic.\nSearch for facet values\nIf you have a large store with a lot of attributes and their values, you’ll definitely benefit from the search for facet values feature. The feature adds a search bar on top of your filters and lets your customers search for the right value. Search for facet values includes highlighting to make it as smooth as possible for the… customers.\n\nDeveloper-friendly\nThe extension comes with many ways to extend it, modify its functionality, and handle every edge case. This way, your developers don’t have to spend their precious time on writing a boilerplate code — we did it for them.\nHere’s a custom Algolia Magento 2 extension that helps write event listeners, override templates, and modify the frontend. All events you can use can be found here:\n\nFront-end events\nBack-end events\n\nWhat comes next?\nWe’ve heard you’re missing custom sorting of products on a category basis, and that’s exactly what you can expect from the next version. If you use the extension for category listings, in the next version it’ll respect the order of products in categories as you set it in category configuration.\nStay in touch\nThe most important part of the extension is... you! — the users of the extension. Whether you are a store manager, designer or developer, feedback is welcome!  Drop us a line, create an issue, send a pull request or show us what you build on the Algolia forum.\nWe’ll be happy to see you around 🙂 ."
  },
  {
    "id": "7448-0",
    "title": "Algolia + SeaUrchin.IO",
    "author": "Nicolas Dessaigne",
    "text": "Building our Next Wave of Analytics \nWe’re introducing an exciting addition to our product family and rolling out new analytics solutions for our customers \nI’m thrilled to announce today our first acquisition: SeaUrchin.IO — a real-time analytics platform that zeroes in on search insights. This is a key milestone on our path to help our customers improve their user engagement. We’ve been focused on identifying new opportunities for them to create more intuitive, relevant and rewarding experiences. Today’s announcements bring us one step closer on that journey.\nWhy SeaUrchin.IO?\nWe first came across the SeaUrchin.IO team as admirers of their technology. They built a unique platform that surfaced granular insights about how users were engaging with search. They were targeting the exact need we were trying to solve for our customers. We quickly saw that together, we could accomplish so much more for our customers… and accomplish it faster!\nThanks to the work we’ve already been doing to integrate SeaUrchin’s technology, the acquisition has enabled us to immediately accelerate the development of our analytics solutions. Read more about our new Analytics here.\nWe’re just getting started\nWe’ve had a quite a year of milestones — we crossed the 40 billion mark for search queries processed monthly, doubled our revenue, team and customers. But as excited as I am about what the team has accomplished, I’m even more excited about the future. Bringing new technology to our team gives us the ability to innovate faster and bring new solutions to our community.\nWe’re on a mission to give product builders the tools to create amazing experiences, so stay tuned for what’s next!"
  },
  {
    "id": "7439-0",
    "title": "Search Analytics: Gain Insights from User Search Data",
    "author": "Nicolas Baissas",
    "text": "By analyzing what your users search for, you can understand what they want and which keywords they use. By looking at how they interact with the search results, you can understand how well your service answers their aspirations.\nMost importantly, through search, you can get invaluable feedback about your users’ intent: feedback that accelerates their digital journey, and therefore your revenue. But that’s only true if you’re actively using the data that they’re giving you through search —if you collect, analyze, and surface this data.\nOur new Analytics\nFor the first time, our Analytics feature will track the search flow in its entirety — from beginning to end. We’re not only analyzing the queries that are typed and the filters used, but also the results that are retrieved and how users interact with them. Do people click on the results list? Do they add a product to their cart, or read the article, or watch the video?\nToday, we’re releasing a new Analytics functionality, available via API as well as the newly updated front-end dashboard. It is designed to track click and conversion events following a search.\nWhat’s new \nAnalytics API \nSearches.  Improve visibility into queries and keywords users type, as well as queries that return zero results. Set up Query Rules for one-off problematic queries — or adjust your searchable attributes to address systemic issues. \nFilters. Create faster paths to conversion by predefining filters on specific keywords based on the most popular filters users are selecting when searching.\nResults. Improve relevance on every query by using insights into results users see when performing a search. Identify and analyze low quality search results and find problematic queries such as common typos, obscure queries and query reformulations in order to improve content and relevance.\nClick Analytics API\nClicks. Gain insights into what results users are clicking on, what position those results appear in, and the average… click position of specific search queries \nConversions. Join search and conversion data in order to understand which search results are converting, and improve search queries that lead to users abandoning \nWhy it’s useful \n\nThere are two main ways search analytics provide value: by giving you actionable business insights, and by helping you improve the relevance of your search engine.\n\nBusiness insights\nBy analyzing the queries that your users are typing, you can get insights that will be useful across your business:\n\n\nImprove your catalog of product or content. For example, an e-commerce platform could realize that a specific product is often asked by customers and that they should add it to their catalogue.  A news outlet can learn more about the topics that their audience wants to read more about.\nImprove SEO. If your users are searching for certain words on your website, chances are these words are also the ones they type into Google.\nFuel your merchandising and marketing initiatives. Noticing the popularity of thematic words like “healthy”, “cheap”, or “revolutionary” can drive your future campaigns.\n\nImprove the relevance of the search\n\nAnalytics surfaces easy paths to improving your search relevance:\n\n\nThe query “couch” returns no result? Perhaps you’re missing the “sofa” synonym for “couch”.\n\nThe Click Rate on the query “tomato” is lower than you’d like? Perhaps your relevance configuration retrieves “tomato soup” higher than actual tomatoes, and you need to improve your relevance strategy.\n\nThe articles most surfaced by your search don't correspond to your best articles? You could update your ranking to showcase these more.\n\nIt’s only the beginning!\nEver since we've started Algolia, our goal has been to focus on making the best possible search engine, and our new Analytics feature is another step on that path. It adds a new dimension to the strong foundation of our engine: textual and business relevance, speed and… reliability. \n\nWe can’t wait to see how you use Analytics to make meaningful improvements to your search experience and to your business. And we certainly plan to guide you along the way and help you make the most of the insights surfaced by your analytics. \n\nJoin our webinar on April 5 to discover how Search Analytics help drive conversion rates. Also, check out the following resources or contact us for a demo:\n\nInteractive tour\nAnalytics documentation\nClick Analytics documentation\n\nDon’t hesitate to get in touch and share your questions and feedback: shoot us an email, tweet to us, comment on this post."
  },
  {
    "id": "7420-0",
    "title": "Mobile Search UX – Part Three: Displaying the Hard Work",
    "author": "Guy Daher",
    "text": "In part 1, we’ve depicted the many obstacles around search mobile UX. In part 2, we took a look at the anatomy of mobile search. We tackled the search bar, the search screen, and the search result screen. In this last part of the series, we will focus on the actual results page, specifically looking at what it needs to look like when building a great search experience for your mobile users.\nDesign: real estate is key\nDisplay results the right way\nWhen displaying search results, the end user should be able to pick the results that are right for her. It is therefore important to visually display the information in a way that will resonate with her, while making efficient use of the limited screen space.\nStart by understanding how your users browse your catalog. For items that they select visually —  like shoes or clothes — you could present search results as enlarged pictures.\n\nOn the other hand, price, distance and ratings can be used to display results for entries that are heavy on specifications, like restaurants.\n\nWhile it is possible to use a mix of both, understanding how your users search will guide you in choosing the right search interface.\nShowing feedback when filtering\nProviding filtering options is omnipresent in advanced search experiences. The issue at hand is mobile screen real estate since it is hard to include both filters and search results in the same view without the screen getting cluttered. Sometimes, we are able to squeeze a few important filtering buttons in a top bar:\n\nBut most of the time, we’ll have to show a dedicated view with all possible filters. Good filtering user experiences have one thing in common: they show some sort of feedback when the user applies a filter. For example, Amazon displays a panel that occupies 2/3 of the screen width. When a filter is applied, the background search result view will update on the fly to showcase the new results:\n\n \nAnother example is the Airbnb app, where there is a button at the end of… the screen displaying the number of results every time a filter changes.\n\nShow relevant results\nRelevance is of utmost importance on mobile because of its tiny screen which allows for a very limited number of results to be visible. Without relevance, all best practices are for nothing.\nWhile on the desktop you may afford a luxury of not having the most relevant result at the top — since the user will eventually find it — this is not the case on mobile. Because of the limited real estate, only the top search results are visible to the user, so they’d better be relevant.\n\n \nRelevance is best evaluated by taking into considerations both textual and business ranking rules; you can learn more about it here. \nUsability: respect user's effort\nRecognize the typos\nIt is easy for the user to do typos. This is even more true when typing on mobile due to the limited space between keys. It is therefore important for your search results to have some sort of a typo tolerant mechanism. Anticipating user’s intent and correcting their mistakes will substantially reduce their frustrations.\n\nLet them scan the results\nIn general, people don’t like to read everything that is on the screen, especially when they are browsing vs. knowing exactly what they’re looking for. What they prefer to do is scan for information. It is thus our duty to help them achieve this, and what better way to do this than highlighting.\nHighlighting helps the user understand why they are obtaining specific results or suggestions.  There are two effective uses of highlighting.\nThe first one is a standard highlighting that matches the query exactly. This is great for the user to understand why her query matched a result, and is best used when showing instant results.\n\nThe second one is inverted highlighting, which highlights everything but the search query. A great scenario for this is query suggestions. In there, the user can easily identify the differences between the different… suggestions.\n\nEnvironment: speed and connectivity\nThis is an obvious one, but we have to mention it since a good search experience has to meet user’s expectations of speed. Google has set the bar high, and apps must follow.\nOn mobile, users are on the go, often with bad connectivity. The search experience has to react well to these circumstances in the best possible way in order to ensure a smooth user experience.\n\nWhen connectivity is slow, a good idea is to show some progress indicator: spinning wheels, loading icons or progress bars. Another way to manage user’s expectations is to show a skeleton screen with placeholders, as shown in the screenshot above. This gives the user a sense of progress and feedback. A final way to improve the search experience is to implement lazy loading, where you would give priorities to certain types of content over others, and fetch them separately.\nNevertheless, the dreaded case of no network availability will surface at some point to the user, and this needs to be handled properly, beyond just showing a “retry” button. One straightforward way to solve this is to cache the top results offline, and then offer a basic local search experience. A better way is to provide a light full-fledged search engine on the device that is capable of doing most of what the online engine would do. However, this is not for the faint of heart, and requires a lot of skills, resources and time, as we learned building it with Algolia. \nWith this series, we tried to share everything we learned through many years of building search user experiences on mobile. Let us know if we missed anything or if you have any questions or feedback: @guydaher, @algolia\nThis blog post relies heavily on the research done by Lucas Cerdan, Product Manager at Algolia. Big thanks, Lucas!"
  },
  {
    "id": "7391-0",
    "title": "Bringing Search to Angular",
    "author": "Marie-Laure Thuret",
    "text": "Today, we are excited to launch Angular InstantSearch — our newest library for easily creating search UIs in Angular applications.\nOur mission at Algolia is to make sure any website or application has the best search experience. But search is its own domain of expertise, with specifics concepts, specialized coding skills and UX/UI best practices. This knowledge is hard to gain and as search is only one part of a project, that's why we want to make sure any developer can easily create a search UI even when the rest of the search puzzle is solved by someone else.\nFrameworks and search UI\nIn 2015 we released our first solution to provide a search UI/UX library: InstantSearch.js. The goal of InstantSearch.js was to let developers build their search UI faster by providing a set of ready-to-use UI widgets. Since the web framework war was roaring back then, we built a generic (Vanilla JavaScript) solution instead of one tailored to a specific framework.\nBut soon it became clear that we should go a step further if we wanted to provide the best developer experience. When using a specific web framework, a developer expects a certain kind of an API that we can only provide with dedicated InstantSearch versions. That’s why in 2016 we started releasing other “flavors” of InstantSearch like React InstantSearch and Vue InstantSearch.\nAfter several requests from our users, we thought that Angular developers deserved a dedicated InstantSearch library. This InstantSearch flavor, like all our products, has been built in collaboration with our users. We contacted all our current and potential users interested in Algolia + Angular and offered them to join a private chat where they can continually give us feedback on the library. Based on this, we built a first beta; today, Angular InstantSearch is already used in production with great success.\nLet's look at some of the main features of the library.\nBuilt for Angular 4 and 5\nAngular InstantSearch is compatible with Angular 4 and… 5 which means that, even if you are still on Angular 4, you can use it and be sure it will work when you upgrade to Angular 5. We are not compatible with Angular 1 and 2 because the changes between Angular 2 and Angular 4 or 5 were too broad. We also thought it was better to provide great features (like server side rendering) to the latest instead of being blocked by older version incompatibility.\n20 ready-to-use widgets\nFrom a good search box to a proper pagination system, we got you covered. This release includes all the widgets you need to build a complete search UI. The widgets can be completely customized, from the DOM they generate to their style and behavior.\n\n API\nWith Angular InstantSearch, widgets can be used easily with Angular directives. It's just a matter of writing some HTML-like code to obtain a working search:\nView the code on Gist.\nAll the options are attributes to apply on the widgets directives. Building a search UI can now be achieved in a few minutes while still remaining highly customizable.\n\nCustomization API\nWhile we identified the most common use cases and best practices when designing InstantSearch widgets, some needs just can't be achieved with the sole use of options. That's OK.\nAngular InstantSearch provides an API letting you choose the rendering you want without having to rewrite all the search business logic code. For example, transforming the menu widget that by default renders as a list into a dropdown menu becomes simple.\nFirst-class SPA support\nSingle page applications are able to deliver performant and dynamic experiences to their users by loading and displaying only relevant elements of the UI based on user needs and actions.\nWith Angular InstantSearch, widgets are nothing more than regular DOM elements. They can be dynamically added or removed from the page. For example, you could decide to display only certain type of filters depending of the current query.\nServer-side rendering, of course\nAngular 5 comes with nice utilities… that ease the creation of server-side rendered applications.\nCreating a server-side rendered app is a must-have to improve SEO score on search engines by facilitating web crawlers, especially if you're running an e-commerce business. It's also a good way to speed up the first page display for users.\nThat's why we developed Angular InstantSearch with service-side rendering available from day one.\nTry it out\nTry Angular InstantSearch! We have a getting started tutorial, and also built some demos so you can quickly see how it looks and behaves.\nThis is the first release of Angular InstantSearch, based on your feedback we will enrich it in the coming weeks and months.\nWhat can we do better? What's missing? Reply in comments here, reach out on our community forum or tweet us!\nYou can also comment on Product Hunt and Hacker News.\nCredits\nThis release would not have been possible without some good team work:\n\nMaxime Janton, our Angular champion, coded 90% of Angular InstantSearch\nMarie Thuret, our product manager, ensured that the release is on time and meets our InstantSearch feature requirements\nMatthieu Blandineau, our product marketing manager, made sure we reach every possible user with the news of the release\nThe whole InstantSearch team was here to support Maxime with code and discussions\nSébastien Navizet, Alexandra Prokhorova and Tiphaine Gillet designed the Angular InstantSearch logo and website and made the illustrations on this blog post and Product Hunt\nLucas Bonomi took ^ that design and transformed it to HTML and CSS compatible with desktop and mobile\nIvana Ivanovic helped us write this blog post\n\nWanna be a part of our next release? We're hiring!"
  },
  {
    "id": "7369-0",
    "title": "What Can't React Do?",
    "author": "Tim Carry",
    "text": "Algolia Search Party is a monthly Meetup we organize around a common theme that makes fellow devs’ lives easier. Topics have included Laravel, static websites and, this month, React and some cool things you can do with it.\nSince the talks were useful and well received, we are sharing summaries and videos with you.\nCodeSandbox: your IDE in a browser\nIves van Hoorne flew in from the Netherlands to share his work on CodeSandbox. The idea for CodeSandbox came when Yves was trying to help some friends with code, but had no easy way to share projects. Because mentally parsing and running code in their heads was cumbersome and error-prone, Ives decided to invent a collaboration tool.\n\nCodeSandbox UI is a split view. On the one side you have your code, and on the other you have a live preview of the code. Whenever you update the code, the preview automatically reloads. The nice thing about this view is that you can load a full project, require any `npm` dependency and run it directly in the browser.\n\nTechnically, CodeSandbox is written in React and is made of two different apps. One is for handling the code editor, the other is for running the preview. Both apps are running on different subdomains so a live preview cannot hijack your CodeSandbox credentials. Whenever code is updated in the editor, CodeSandbox will compile/transpile it and send it to the live preview for execution. Any errors generated are then sent back and displayed in a more human-friendly way. As Ives put it: “Humans will be reading the code and humans are visually focused.” UI is just one way that CodeSandbox is improving the developer experience.\nAn example: the Webpack visualization UI built on top of webpack-dashboard. It gives you a clear visual representation of your bundles and their respective sizes. CodeSandbox also goes further than displaying what the error is; it also explains what went wrong and how to fix it. For simple errors like a missing dependency, it even provides a one-click… button to add the dependency.\n\nAlgolia plays a role in the quest to provide the best DX, as we provide the search into all the `npm` packages via the index we built and maintain for Yarn (available for anyone to use). CodeSandbox uses it for discovery of packages as well as autocomplete when adding a `require`. We can’t help but quote Yves: \"I've seen search. Algolia is not search, it's instant results\". Because of that speed, Algolia can be used directly in the UI, and it took Yves only one day to integrate it.\nIves finished his talk with more ideas about how he could use Algolia to improve the DX of CodeSandbox—further including the re-use of the indices in DocSearch to provide contextual documentation help directly from the editor. This would remove one more step for users and allow them to stay in the zone for longer. Another idea would be to improve the error display by automatically searching for the error and displaying the documentation about it. We love the ideas and will work with Ives to make them happen!\nCodeSandbox is open source and the code is available at https://github.com/CompuIves/codesandbox-client.\nLet's code Redux in 15 minutes\nAurore Malherbes, a mobile developer specialized in React Native presented an exercise she does with any new member joining her team: recoding Redux from scratch to understand how it works.\nAurore’s talk was inspired by Dan Abramov’s Redux course on egghead.io. Redux itself is not hard, but to use it correctly one needs to actually understand it — and the best way to understand it is to build it.\nAurore’s talk was clear,explaining step-by-step in 15 minutes how the state, subscribe and dispatch all fit together to form the Redux event loop. I won’t be repeating everything Aurore explained in her talk and would instead suggest watching the video with explanations so clear it would do them injustice repeating them “on paper”.\n\nSearch UX on React Native\nLast but not the least, our own Marie Thuret talked… about unique challenges mobile search presents: screen size, connection speed, low connectivity and more, as well as how to overcome them with our React InstantSearch library\n\n \nInstantSearch packages a set of UI components on top of the Algolia API, that include all the best practices you see above. All those components are open source and extensible, letting you add your own custom logic.\nMarie ended her talk with a live demo of implementing a mobile search using our React-flavored version of InstantSearch. Step by step, from the search bar to the infinite scrolling of results, including the highlight. In less than 5 minutes, you too can have a working search. Check it out!\n\nSpecial thanks …\n...to our speakers and the audience; both made the evening super enjoyable and informative. We hope to see you at one of our community events next time, and meanwhile, stay tuned to this space for interesting talk summaries and more."
  },
  {
    "id": "7351-0",
    "title": "Travis Encrypted Variables and External Contributions",
    "author": "Julien Bourdeau",
    "text": "In this article, I’ll explain how we handle community pull requests while avoiding storing Algolia credentials in Travis's secured environment variables. It requires a bit of time, but for a SaaS company maintaining open source projects, we believe it's worth it.\nFirst, it's important to understand that the only way to have a green build for community pull request is to make the credentials publicly available. It sounds like a bad idea, but I will show you how to do it safely.\nNote that this post uses Travis CI as an example because it is what we primarily use at Algolia. The problem and solution are also valid with other build systems, like CircleCi for instance.\nThe problem\nBefore we start, let's summarize the issue we want to solve.\nAlgolia maintains more than 10 open source API clients and most of them run tests against our API. It requires credentials to be able to contact the service, so following Travis's best practices, we store them as encrypted environment variables in our .travis.yml. To test our API clients, we use Algolia API keys, which are not just read-only but will also be used to test insertions in an Algolia index. Thus they can't be made available outside of our own builds (otherwise people might use the keys to create a lot of objects and indexes in our API).\nThe way encrypted environment variables works is that you use the travis gem to add encrypted variables to your .travis.yml. Those keys are then automatically decoded by Travis at build time, but only for builds triggered by contributors with write access to the repository — not for forks, meaning not for external contributors’ PRs.\nTravis tried to solve this issue by providing a JWT addon that some companies (like Sauce Labs) implemented, but recently, they took a step back and will deprecate the addon in April 2018.\nPending PRs on the Symfony bundle \nThe screenshot above illustrates the problem: maintainers' PRs are green and community PRs are red —not because they're doing… something bad, but because in this context, Travis is not able to call Algolia.\nTerrible experience for contributors\nAt Algolia we care a lot about developer experience. Everything — from our rest API to all the ecosystems we built on top of it — is focused on providing the best developer experience for our users. So when our users are willing to contribute to our open source projects and their experience is less than ideal, it can be...frustrating.\nImagine you took some of your free time to fix a bug or refactor something in a library you use and the first feedback you get from the build system is more or less:  \"Tests are failing for some reason, figure it out yourself\".\n\nYou might look at it and spend some time understanding what's going on. Then, you’d realize there is nothing you can do and you just wasted some of your precious time. In short: this is terrible for contributors and will definitely not encourage them to come back to us again.\nPainful for maintainers\nNot only do we want to provide the best developer experience for our users, but we also want our own developers to have a great maintainer experience.\nAs a maintainer, I'm always hesitant to merge a contribution if I don't see the results of the tests. Sure enough, if only the README.md was modified, it's safe to merge. But in general, it leaves you with a feeling of doing something wrong. What I am used to is to pull the contributor's branch and run the tests locally, but this is really time consuming.\nAnother solution would be to re-submit the same PR by pushing the branch to the original repository, either manually or using a script. But this means that the original PR gets closed, which complicates the process and is seen by contributors as a strange way to handle PR.\nOverall, this was so painful that we thought it was time to invest some time to find a better solution, once and for all.\nFirst try: using temporary keys with limited capabilities\nIn the end, the solution was pretty… straightforward: every build will use temporary credentials that will expire. We still want to avoid malicious developers using these keys, so we added a set of limitations to each key. It means that even if the key is publicly exposed and has write access to Algolia, there is so little one can do in such a limited time that it is simply not worth the effort.\nOn Algolia side, these limitations are different for each repository, but each set of keys:\n\nHas a subset of all the available ACLs\nExpires automatically after a few minutes\nOnly has access to some indices\nHas a limited number of operations per hour\nHas a small nbHits that you can't override\nCan only be used from Travis IP addresses\n\nThis is how we protect our API keys given Algolia API key features. Of course, it will be different for your own API.\nBut wait...we realized that, even with such keys, there's one drawback: in the case of Algolia, you're still able to create API keys, which means that an attacker could fork one API client repository, change the testing code to create keys using our restricted keys, and still escape the protection. Back to square one: we need another layer of protection.\nA proper solution: using an API key dealer\nThe best way we found to solve this \"API keys can create other API keys\" issue was to build a little server that would act as an API key dealer. It's open (no authentication) and holds one configuration per repository (what restrictions to apply to the key). This server is responsible for creating keys on the Algolia side, and giving them back to the build system.\nWe plan to open source the API Key Dealer we built in the coming weeks, and will let you know when it's out.\nBecause it's a publicly available server, anyone is able to generate keys, so we check if the call is legit and originates from Travis's IP ranges. Not only this, but when a key is requested, we sends the original TRAVIS_JOB_ID and we use it to verify that the repository is part of the Algolia organization… and that a job is currently running.\nThe whole process is described in this schema:\n\nThe client\nTo be able to call such an API key dealer from Travis, we needed a small client that could be reused in all our repositories.\nTo do so, we built a small Go script that compiles into one binary file.\nThis binary is downloaded before each build, so we can easily update it on all the repositories.\n\nIf you want more details, read the whole .travis.yml file.\nThe small binary will:\n\nAssess if temporary credentials are necessary\nCall the API key dealer\nPrint a comment (for help and debug purposes)\nExport env\n\nChallenge: testing advanced admin permissions\nDespite having a temporary key with limited capabilities, there might be permissions you cannot give to a public key: managing team members or changing account credentials for instance. The only solution for this is to skip the tests requiring admin permissions in external contributions.\nThe client is then responsible for determining if the credentials should be grabbed from the env variables (pull request from organization member) or from the API key dealer (pull request from external contributor). So yes, you still need to leave the master credentials in the encrypted env vars of Travis.\n\nIs it really worth it?\nAs you can see, we spent some time designing this system so finding out if this was time well invested is a healthy question to ask.\nThe answer: it all depends on your own ecosystem and what experience you want to offer to your contributors. If one day someone decides that they want to take some of their free time to help improve our API clients, we believe they should have the best possible developer experience. Our libraries don't yet have hundreds of external contributors but we want to reach that milestone and the only way to get there is to show respect to every contributor, no matter their level of engagement or skill set.\nI hope you enjoyed this blog post and that it encourages you to, if you get inspired,… contribute to our open source projects.\nThanks to Vincent Voyer, Josh Dzielak, Ivana Ivanovic and Tiphaine Gillet for helping me with this article."
  },
  {
    "id": "7323-0",
    "title": "Introducing Query Suggestions: Making Autocomplete Search Experiences Right",
    "author": "Lucas Cerdan",
    "text": "As-you-type autocomplete is a well known search pattern that many of the best websites offer. Providing suggestions helps the user enter longer queries. For mobile users, it reduces the amount of typing required to execute a search. But, it's not just about typing less. It’s far more powerful — it helps users formulate the best search. Good suggestions are proven queries that ensure the best results. The quality of the suggestions is as important as the results they generate.\nToday, we’re releasing Query Suggestions, a new feature that extends the conversational search experience. It is as if the user, with every keystroke, is conversing with the search bar, discussing with it exactly what to send to the search engine to obtain the best results. For this to work, suggestions need to be instantly responsive and intuitively/accurately relevant to be useful. If either one of these is missing, the conversation is over.\n\nHarnessing analytics\nWhile generating suggestions might appear trivial, it comes with its own set of challenges. Suggestions often derive from popular queries: pick what your users have been searching for most, and suggest it next time so they don’t have to type it.\nThe popularity of each query, extracted from analytics, can also be used as a great way to rank suggestions. If “iphone charger” is the most searched term, it might make sense to make it the first suggestion when your users are typing “iph” — but not always.\nShowing only relevant suggestions\nNot everything your users are searching for should end up as a suggestion. As is often the case when it comes to data, it needs to be processed and cleaned. For example, it would be a mistake to suggest a query that leads to no results. This is where our Query Suggestions tool comes into play. You are now able to configure a set of rules to create the most relevant suggestions for your business.\nPreventing spam &amp; cheaters\nOne caveat to using analytics as a source of suggestions is… relying on user-generated content: user input should never be trusted blindly.\nCastorama, a French home improvement tools and supplies retailer, made that mistake last year. Typing any kind of a query multiple times was the only requirement for it to start appearing in the autocomplete for everyone. Their system got flooded with inappropriate suggestions and Castorama had to shut down their website for weeks, missing many of sales opportunities.\nFor marketplaces, it is equally important that vendors cannot influence how suggestions are ranked; otherwise, they might be tempted to promote suggestions that favor their own products.\nIntroducing Query Suggestions\nOur Query Suggestions tool has been created to address these issues. Popular queries will be extracted from our Analytics API, and then processed and cleaned. While we have systems in place to limit inappropriate suggestions and prevent fraud, suggestions can also be moderated by the user with an easy-to-use blacklist.\nWe also support external analytics data, which will be particularly useful for new customers who can upload any historical data from previous search solutions, or Google analytics. And for more complex use cases, combining facets is also a possibility: for example a brand “nike” + a category “shoes” can generate “nike shoes”. While this is a simple example, our early users have used the feature extensively to guide their users to very precise content.\nScoping suggestions\nGuiding your user to the right content is paramount, and including a scope in the autocomplete is the perfect moment to do that. Simply indicate which attribute should be taken into account, and our Query Suggestions tool will generate the top related categories for each suggestion.\nExample of a scoped search\nEasy to implement\nAfter extracting and processing the data, the Query Suggestions tool will push all valid suggestions to a new index, and refresh it every 24 hours. Just like any other index, you can query it… with the same API and libraries we provide. Here’s a tutorial with InstantSearch iOS.\nPricing &amp; availability\nQuery Suggestions is available today for our Business and Enterprise customers. To enable this feature directly in your dashboard, reach out to your product specialist or customer success manager. All settings and blacklists are managed within the Algolia dashboard."
  },
  {
    "id": "7243-0",
    "title": "New Features, New Docs and a New Foundation for Algolia’s Jekyll Plugin",
    "author": "Tim Carry",
    "text": "We’re proud to announce the new release of our Jekyll plugin, jekyll-algolia. Jekyll is the best-known static website generator, converting raw markdown content to beautiful HTML pages. This plugin helps developers make their Jekyll sites searchable.\nThe main strength of static websites like those built with Jekyll is that you don’t need any database or backend server to run them. Just deploy a bunch of HTML, JS, CSS and images to a hosting provider and you’re good to go.\nThe downside was that the only way you had to find content was to struggle with  or head back to Google and refine your query. With , that’s a thing of the past. You can now search all your content directly from the static website itself, using the Algolia API.\nIntegration into the Jekyll ecosystem\nWe released the first version of the plugin in 2015, but a lot has happened since then. Jekyll got a major version update. Static websites are blooming everywhere. On our side we also indexed a large number of blogs and documentation websites. Everything we learned with these projects has now been distilled into this new version.\nThe plugin has been renamed from  to . This is one of many small changes to better follow the Jekyll conventions. We wanted  to be a good citizen of the ecosystem, and that comes by speaking the Jekyll language.\nBut plugins do not live in a vacuum. You will surely use this plugin with other plugins, and we had to make sure they were all playing along nicely with each other. For example, we tested it alongside the popular  plugin because they both manipulate the same objects (tags), and made sure our plugin was not interfering with the other and vice-versa. We also ensured it was compatible with the de-facto static hosting platforms: Netlify and GitHub pages.\nUsing the plugin\nTo use the plugin, you need to first add it to your , under the  group (like every other Jekyll plugin). You then need to define your Algolia credentials in the  file, and run the  command.\nView the… code on Gist.\nView the code on Gist.\n\nThis will read all your content (pages, posts and custom collections) and push them to your Algolia index. Each record pushed that way will contain metadata about the original page (, , , ,  and any custom data you defined in your front-matter). All this data will then be available to your front-end, allowing rendering of rich search results.\nCheck out the live demo and the code repository.\nThe documentation explains in great detail how to integrate search into , the default Jekyll theme. Applying the same principles to your own theme is straightforward using our InstantSearch.js front-end library and its UI widgets.\nSmarter indexing\nThe previous version had a naive approach to indexing. It used to delete all records in the Algolia index, then push everything again. It meant that even the smallest typo fix could delete and recreate thousands of records.\nIn the new version, the implementation was replaced with a smarter diff algorithm, reducing the number of operations used by orders of magnitude. Everyone, including people on our forever-free Community plan, should be able to use the plugin without worrying about hitting their monthly quota limit.\nBetter documentation\nOne of our mottos at Algolia is that documentation is part of the code. If it's not documented, it's not usable. We worked hard on making the documentation clear and exhaustive, not only explaining the various options but also why, when and how they should be used.\nWith the Getting Started guide, you’ll be up and running in a matter of minutes. Configuration options will let you customize which files should be indexed or excluded, as well as add your own custom Algolia settings. For those of you who have more advanced implementations, a hook system will let you write your own Ruby methods to alter the records in any way you’d want before pushing them. If you’re coming from the previous plugin version, we also have a migration guide to help you transition.\nBut… documentation also comes in the form of error messages. We all know that even with the best documentation and code, errors will still happen. Maybe it will come from a misconfiguration on your side or a bug in the plugin. Either way, this should not stop you. This is why we made sure the error messages explain clearly what failed, why, and more importantly, how to solve it.\nSome errors are pretty common (badly copy-pasted credentials or malformed HTML) and can be easily identified and avoided before they ever happen. The plugin does both pre-emptive checks and smart error handling to catch those common errors and display a message explaining what happened and the steps required to fix the issue.\n\nHopefully you’ll never have to see those error messages. But if you do, I think you’ll be glad they explain what went wrong, and how to fix it.\nStart your search journey\nEasy to use, yet powerful and extensible — that’s our new jekyll-algolia plugin. Whether you have a large (or a small) blog, a documentation website or a custom display of your own collections, give it a try. You’ll have a fast and relevant search across all your content and your users will thank you for that.\nQuestions? Feedback? We'd love to hear it: @pixelastic, @algolia."
  },
  {
    "id": "7267-0",
    "title": "Handling Natural Languages in Search",
    "author": "Léo Ercolanelli",
    "text": "Most of world’s languages feature a wide range of unique quirks that make search engines stumble when processing queries. In this article, we’ll present features we implemented to solve these issues, and go over their rationale and technical implementations.\nOur engine is language agnostic, which means it’s designed to work with every written language. This is no easy feat, considering all the little and big differences languages have, from being based on symbols from different alphabets like Japanese or using many variants of the same stem like in German.\nThere is no silver-bullet or magical way to handle each of these specificities. Some of the most common approaches rely on statistical models, which are by default close to impossible to debug and iterate upon. Our goal was to design a system that is not only simple to iterate upon, but runs fast and works transparently.\nWe believe that we’ve found ways to achieve a great relevance in many of the world’s languages. Today, we’re explaining our approach, and how to best use it to configure the relevance of your Algolia index for every language.\nNormalizing characters\nA well known low-level step while doing NLP is normalization. The goal of this step is to depend more on the letter than on the way it was typed, to reduce friction between what the user wants and what has been indexed in the first place.\nThis step is actually composed of two steps:\n\nUsing the Unicode standard and Unicode supplements to find the canonical form of the letter: À → A or ß -> ss, for example. For additional detail on how we handle Unicode, you can refer to this post.\nUsing the Unicode standard to find the lowercase form of the canonical form: À → A → a.\n\nThose two steps are applied on the text while being indexed, and on the query, when a search is performed.\nThe normalization into lower case characters is easier to grasp: requiring an exact capitalization of the query is too strict to allow for a pleasant search… experience.\nLately, however, we have been questioning our strategy of indiscriminately removing the diacritics (´, ¸, ¨, etc.).\nThe strategy was designed for the languages we started with — including the French —  and it brought a couple of benefits:\n\nBe resilient to missing/extra diacritics when users are typing on mobile.\nAllow users that can't or don't know how to type some characters to still be able to search for them.\n\nHowever, our bias led to some languages being harmed by this mandatory step of our search engine: mostly Nordic languages (Swedish, Norwegian, etc.) and Turkish. In French, e and é are considered by native speaker as more or less the same letter; for a Norwegian speaker, o and ø are not interchangeable at all.\nInterchanging two such letters can create an issue by making the search unable to discern between two completely different words, like in this example in Turkish: cam → glass, çam → pine (pretty annoying if you are focused on selling furniture).\nAs we already mentioned, this step is today mandatory in Algolia’s search engine and can't be disabled. We are thinking on improving this to allow a more granular handling of languages for which diacritics play an important role.\nTypo tolerance\nTypo tolerance is an important part of a search engine, and one of the earliest features Algolia brought to the table. Its role is really simple: be resilient to input mistakes, or plain misspelling.\nThe rules for the typo tolerance are as follows:\n\nif a letter is missing in a word, we consider it a typo: “hllo” → “hello”\nif there is a extraneous letter, we consider it a typo: “heello” → “hello”\nif two consecutive letters are inverted, we consider it a typo: “hlelo” → “hello”\n\nBy default, Algolia accepts 1 typo for words that are at least 4 letters long, and 2 typos for words that are at least 8 letters long. Those thresholds are configurable by the user.\nOn top of those rules, we added some subtleties, such as… the fact that a typo on the first letter of a word is “penalized” double, because it is much less common to mistype the first letter of a word than any other letter.\nThose rules are easily applied to languages using Latin alphabets: while some languages tend to have shorter or longer words on average (which could be translated to increasing or decreasing the minimum number of letters before accepting typos), they are all similar in the way typo tolerance can be configured.\nLanguages using logograms, such as Chinese or Japanese are, however, a whole different story. Because of the way those languages are input, the typos as we know them in Latin alphabet languages don’t really occur.\nHowever, those languages are subject to the presence of alternative logograms for the same word. Those alternatives can exist for two reasons:\n\nthe presence of a traditional and a simplified drawing for the same character\nthe presence of two logograms that can be used interchangeably\n\nRemoving stop words\nIn the last couple of decades, while browsing the internet, most of us adopted the habit of transforming natural language questions such as: \"What is the best search engine?\" to full-text queries we knew would be better interpreted by search engines, such as: \"best search engine\". While those requests used to yield better results thanks to the decrease of noise in the query, this is less and less true because of two major factors:\n\nSearch engines adapted to allow more casual users to find their results without having to pick up this habit\nThe advent of voice search is favoring the use of fully formed sentences\n\nBy default, Algolia is requiring all the words in the query to be present in a record for it to be considered a match. While it is possible to remove words when no results are found, all the words will still be considered equal, and \"the\" won't be considered as less important than \"search\".\nTo handle this case, we introduced pretty early a removeStopWords setting that… removes all the “unnecessary” words from the query string, and eventually keeps only \"best search engine\".\nTo accomplish this, we are using a really simple dictionary-based approach. We parsed several sources (Wiktionary and ranks.nl) in order to constitute a list of words commonly used as stop words, not only in English, but in ~50 languages available.\nThe first implementation of the setting was suffering from a major flaw: it was a single boolean value, and one could only activate this feature for all languages simultaneously or not at all. A lot of corner cases were observed: like \"thé\" (the French word for \"tea\") being removed because of \"the\" being an English stop word, thus transforming a really specialised \"magasin de thé\" (tea shop) into a regular \"Magasin\" (shop).\nSince then, we changed the setting; the feature can now be set to a list of languages, allowing one to remove stop words from one language at a time.\nWhile this feature doesn't work well in itself with languages not using spaces (such as Japanese), once coupled with other pre-processing steps (such as word segmentation) it behaves just the way you would expect it to.\nWhile the list of stop words for a given language is not likely to change, we still have a few ideas in mind in order to improve this feature:\n\nAdd support for more languages\nIntroduce an option to make those stop words optional; meaning that they won’t prevent results from showing but will influence the ranking if they are inside a record. As of today, they are simply removed from the query.\n\nHandling alternative forms\nIn English, words have a rather limited number of alternative forms:\n\nnouns have a plural\nverbs have a few conjugated forms\n\nHowever, even if \"car\" and \"cars\" are not so different from each other, there are use cases where you still want to consider them as one and the same (“feet massage” and “foot massage”).\nTo tackle this issue we created a parameter called ignorePlurals which has over the years… undergone several transformations.\nThe problem can be solved in two different fashions:\n1. When indexing words, strip them to their singular form: \"feet\" → \"foot\", “massages”  → \"massage”. Whenever a query is sent, do the same to each words of the query: \"feet massages\" → \"foot massage\", then search for those radical forms.\n2. Index the words the way they are given (\"feet\" → \"feet\"), but generate all possible alternatives for all the words in the query: \"feet massages\" → \"(foot|feet) (massage|massages)“ and search for any of these forms.\nAt Algolia we chose to go for the second method. The reason is simple: typo tolerance. If the word is not indexed exactly as it was given, then you can't accurately compute the number of typos afterwards: that's why it is important for us to not alter the words that are indexed.\nThere are two Natural Language Processing (NLP) techniques to reduce a word to its singular form or find its alternative forms: Stemming and Lemmatization.\n\nStemming is relying on rules such as: if your word is ending with \"ing\", strip it. The most popular implementation of stemming is called Snowball.\n\nWe have mixed feeling towards stemming, as a single rule could cause a lot of harm if you consider the following points:\n\nDifferent languages could be present in a single index, meaning that English stemming rules could be applied to French text, which is likely to create a ton of non-valid words, and yield to noisy, confusing results.\nSeveral of our use cases make heavy use of brand names, proper nouns, street addresses, etc., and for those, using arbitrary rules designed for common nouns degrades a lot of the relevance\nLemmatization, on the contrary, doesn't try to create general rules, and relies on a dictionary giving for each word its radical : \"eating\" → \"eat\", \"ate\" → \"eat\", ...\n\nWe make heavy use of lemmatization. The only thing that lemmatization efficiency depends on is the accuracy and completeness of the dictionary used. We… are continually working on improving our dictionary, primarily based on Wiktionary.\nWiktionary relies heavily on templates such as {en-noun|s} allowing Wiktionary contributors to declare alternative forms when writing a page on a word. For example, {en-noun|s}, would show up like this on the “car” page of Wiktionary:\nNoun[edit]\ncar (plural cars)\nBy using those templates found inside the Wiktionary data, we are able to build our dictionary of alternative forms. Almost every language has its own template syntax, and many languages have multiple templates to do so. Because implementing everything at once is extremely tedious, we chose to improve our dictionary gradually, mostly upon user feedback about missing words/languages.\nWe spent a fair amount of time on this, and improved a lot of the features over time:\n\nAs mentioned above, for removeStopWords, the feature started being a simple boolean switch, allowing only an \"all-languages-or-nothing\" behavior. It is now possible to specify which languages the lemmatization should be used with.\nLanguages like German also have alternative forms depending on the grammatical role of the word in the sentence. This is not a plural per se, but we have had a customer reaching out to us in order to support this. And so we did! That makes today's name a bit misleading as it's not about plural anymore, but also declensions, diminutives, etc.\n\nignorePlurals evolves a lot: words are constantly added, new types of alternatives are supported (e.g., most recently, Dutch diminutives). The hardest part is actually not adding words, but finding the right balance between improving relevancy and harming it with noise.\nSegmenting CJK text\nCJK stands for Chinese, Japanese, Korean. The acronym is pretty popular and we use it a tad inaccurately by using it to refer to any language that doesn't use spaces to separate their words (Thai and Lao, for example).\nFinding the words in a query is a capital step of a search engine: when you are searching… for something, you want documents containing the words of your query, but not necessarily in the same order. You want the engine to be less strict about whether words are consecutive. For example, if you are searching for \"chocolate cookies\", you don't want only results with \"chocolate” and “cookies\" back to back — you also want \"chocolate chip cookies\" and \"cookies with chocolate chips\" to show up in results.\nWhen a language delimits its words with spaces, things are relatively simple; when it doesn't, you roughly have 3 choices:\n1. Take each character separately. At first, from the point of view of someone not familiar with CJK languages, this makes sense, especially with Chinese or Japanese. Those languages heavily use logograms, and we have the false impression that 1 character = 1 word. This, however is not the case, as many notions are expressed as logogram compounds: a concatenation of several logograms that make up for a new word. Besides, as mentioned, Chinese and Japanese are not the only ones not using spaces. That leaves this approach out of question.\n2. Require the exact query to be inside the matches, verbatim. This greatly constraints the search, but it at least keeps the meaning of the query. While far from being perfect, it's an acceptable fallback if the method I will introduce next fails. Example: 寿司とラーメン (sushi and ramen). While you would want to be able to also find ラーメンと寿司 (ramen and sushi), with this method we would constraint the results to documents containing only 寿司とラーメン (sushi and ramen).\n3. Find by yourself the words inside the query. The segmentation of a sentence into words is a problem CJK experts have always faced, and several satisfactory techniques have been elaborated to solve the issue. None of them guarantee a 100% accuracy, and like everything else in engineering, it's only a matter of compromise between speed/cost and accuracy/quality. This is the behavior we implemented inside… the engine.\nThere are many ways to implement the segmentation, ranging from statistical to dictionary-based methods.\nWe strive for an as-you-type experience, which gives us roughly 100ms (threshold below which our brain doesn't really register difference in speed of response) to answer a query, network latency included. Because of this requirement, we settled for the fastest method in order to find words inside a query: dictionary-based, as implemented in the ICU library, using the MECAB dictionary enriched with data from Wiktionary.\nThe idea is simple: starting from the left of the query, we will try to decompose the query with words that are present in our dictionary. In case of ambiguity we are prioritizing the solution that maximizes the length of the words and minimizes the number of characters not belonging to a known word.\nAs of today, we already have a couple of ideas about what we could do to improve the segmentation of CJK languages. First and foremost, the segmentation is based on a dictionary, which means that any improvement made to the dictionary is sure to improve the overall segmentation:\n\nImproving the dictionary means removing words that are extremely rare, as much as adding common ones.\nWe also heard the feedback from customers about the segmentation underperforming when named entities (such as brand name, people, etc.) are present inside the query. Those entities are not always found inside our dictionary, and the segmentation step is often not able to accurately separate it from the other words. Introducing a custom dictionary that the user herself can improve with custom names is something we are thinking about.\n\nIndexing agglutinated words\nThere is another issue that is in fact really close to the segmentation of CJK characters. In some languages (German, Dutch, Finnish, etc.) several nouns can be concatenated without space to form new words. For example, Baumhaus is a German word composed of Baum (tree), and Haus (house) which designates a… “tree house”.\nA person searching for “Baumhaus” (tree house) may very well be interested in results containing something like “Haus in einem Baum” (house in a tree), but the default Algolia behavior is to only search for Baumhaus. This issue is what triggered us to spend some time working on handling agglutinated words, or compounds, better.\nThe very first step that we implemented was a naive splitting of the words in the query. The idea was simple: in the case of “Baumhaus”, if the index was to contain “Baum Haus” consecutively, a match was possible. This feature was more akin to a typo tolerance on spaces than a real split of compound words, as the words still had to be adjacent to one another, which is a limitation we got rid of in the new version.\nSince then, we settled for full fledged processing steps that look individually at each word. If the word happens to be a compound word (we are here again relying on static dictionaries to avoid false positives), its various parts are indexed separately.\nAs of today, this setting can be activated using the decompoundedAttributes setting. The languages we wish to attempt a decompounding for must be specified; three languages are implemented: Finnish, German and Dutch.\nWe designed this feature to interact with all the other goodies coming with Algolia: synonyms, ignorePlurals, etc.\nThis setting is still rather new, and we expect to allocate time improving the dictionaries used by the decomposition process in order to increase the accuracy of the feature. While today we don’t have a strong demand for languages other than those implemented, additional languages may show up in the future.\nAutomatic concatenation\nFor languages which use spaces to delimit words, there are still subtleties around separating words, for example with acronyms or words separated by hyphens or single quotes. We will review two cases to better illustrate how Algolia handles this.\nAcronyms: D.N.A.. There are two ways of… approaching this:\n1. considering that we have three words: D and N and A\n2. considering that we have a single word: DNA\nIn this case, the second solution makes more sense:\n\nPeople may search both with D.N.A. or DNA. Considering it always as DNA makes it searchable whatever its form inside the query or the index.\nAlgolia uses prefix search if configured to do so, which means that “D N A” as three separate words can match every record that has 1 word starting with D, 1 word starting with N and 1 word starting with A. This is likely to match a lot of results that have nothing to do with DNA.\n\nIn this case, we consider D.N.A. the same as DNA.\nCompounds: off-campus or a.to_json. If we were to apply the rules previously established for acronyms, we would index and search for offcampus and classmethod. In those cases, this is not the ideal behaviour. In the case of compounds, words can be searched individually, or even written without the punctuation in between: both “off campus” and “off-campus” are valid spellings. Transforming “off-campus” into “offcampus” would prevent a search for “off campus” to retrieve results containing “off-campus”.\nIn that case, “off-campus” will be considered as “off” “campus” and  “offcampus”.\nIn the case of “a.to_json”, we will consider “ato_json” and “to_json”.\nThis behavior follows two simple rules:\n\nIf letters are separated by separators, consider the concatenation of those letters without the separators in the middle (D.N.A. →  DNA)\nIf each separated component is three or more letters, also consider it as a standalone word (off-campus → off + campus + offcampus).\n\nThose rules allow for enough flexibility on the presence or absence of punctuation, and avoiding unnecessary small words like “a” for “a.to_json” that could hurt any prefix search.\nSummary\nBelow is a table roughly summarizing the different pre-processing steps we are applying on some of the most popular… languages found across the Internet:\n\nWhile we are always striving to address languages specificities, we know that we are only at the very beginning of the journey, and we already have a long list of language-related issues we want to address further.\nThose issues relate to various parts of the processing of a query:\n\nat the character level: transliteration between hiragana to katakana (two of the three Japanese alphabets) and vice-versa, transliteration between the digits of different languages, better handling of diacritics for languages that suffer from our current method, etc.\nat the grammatical level: improving our dictionaries to even better handle stop words, plurals, declensions, and maybe conjugations\nat the typo tolerance level: a way to handle the Arabic habit of omitting vowels, a deeper look into languages such as Thai that are different from both Indo-European and CJK languages, etc.\nlast but not least, sustaining the effort of improving our numerous dictionaries, in order to improve the efficiency of our current processing steps.\n\nIf you have ideas or feedback, we'd love to hear it: comment below or tweet to us @algolia."
  },
  {
    "id": "7258-0",
    "title": "The Meltdown and Spectre impact on Algolia infrastructure",
    "author": "Julien Lemoine",
    "text": "On January 3rd, several vulnerabilities against modern CPU microarchitectures made news headlines. Those vulnerabilities expose a risk of an information leak. A software could potentially exploit the vulnerabilities to get access to the data of another software stored in memory. This is a major security incident.\nIn total, two attack vectors have been disclosed to the public:\n\nSpectre: two vulnerabilities available in nearly all processors on the market. Those two vulnerabilities are known as \"bounds check bypass\" (CVE-2017-5753)  and “branch target injection” (CVE-2017-5715)\nMeltdown: one vulnerability affecting mainly Intel CPUs, known as “rogue data cache load” (CVE-2017-5754)\n\nImpact for Algolia\nOur infrastructure is a mix of bare-metal and cloud infrastructure. We have three parts in our infrastructure that are impacted by these security vulnerabilities.\nOur API servers\nThose servers are hosting our users’ data and power the indexing/search API. They are distributed worldwide in more than 50 data centers with a similar hardware configuration using Intel CPUs (mainly Intel E5-1650v4). The servers are configured and tuned for performance. We have no virtualization layer and only run our own software while applying security best practices.\nThe CPUs we are using are vulnerable, but the impact is mitigated because we do not expose any way to run custom code on our machines. The only way to exploit those vulnerabilities would be to get access to the machine that already gives access to privileged information. Our security efforts remain oriented to making this impossible, and we are working on integrating the KPTI kernel patch and reducing/testing the performance impact it introduces.\nOur website and dashboard\nWe are using AWS to run our website and dashboard, and this is the place where we have our database listing users. We, of course, consider it a critical part of our infrastructure.\nWe followed closely the AWS actions to protect all instances and they… completed their patch deployment to protect them.\nHowever, we decided to move all our website and dashboard virtual machines to dedicated instances to make sure we do not share our hardware with any other AWS customers. This action was not required to be protected but our general security posture is one of extreme caution.\nAnalytics\nOur analytics stack is computing statistics on your search usage, analyzing query trends.\nWe are in the process of migrating our analytics stack to Google Compute Platform and we already have several customers running on this stack (our current stack is on bare-metal machines, so the status is similar to our API servers).\nLike AWS, Google was working on the fix for a long time and their infrastructure is already protected against those vulnerabilities. Our stack also relies on several systems, including Pub/Sub and DataFlow which are protected against the vulnerabilities.\nSecurity at Algolia\nOur security team is constantly monitoring services running on our own machines, as well as those hosted on cloud platforms to ensure that we're protected against the latest security vulnerabilities. If you have any questions about our process or want to share any information feel free to reach out to the team directly at security@algolia.com."
  },
  {
    "id": "7200-0",
    "title": "Start Your Open Source Career",
    "author": "Vincent Voyer",
    "text": "This year I gave a talk about how to make open source projects successful by ensuring everything is in place to attract all kinds of contributions: issues, documentation or code updates. After the talk, the feedback I got was \"It’s nice, you showed how to make projects successful, but how do I even start doing open source?\". This blog post is an answer to that question; it explains how and where to start contributing to projects and then how to create your own projects.\nThe knowledge shared here is based on our experience: at Algolia, we have released and maintained multiple open source projects that proved to be successful over time, and I have spent a good amount of time practicing and creating open source projects too.\nGetting your feet wet\n\nA key moment for my career was six years ago at Fasterize (a website performance accelerator). We faced an important memory leak on our Node.js workers. After searching everywhere except inside the actual Node.js codebase, we found nothing that could cause it. Our workaround was to restart the workers every day (this reset the memory usage to zero) and just live with it, but we knew this was not a very elegant solution and so I wanted to understand the problem as a whole.\nWhen my co-founder Stéphane suggested I have a look at the Node.js codebase, I almost laughed. I thought to myself: \"If there's a bug, it's most probably our code, not the code from the developers who created a revolutionary server-side framework. But, OK, I'll have a look\". Two days later my two character fix to the http layer of Node.js was merged, and solved our own memory leak.\nDoing this was a major confidence boost for me. Amongst the thirty other people who had contributed to the http.js file were folks I admired, like isaacs (npm creator)— making me realize that code is just... code, regardless of who wrote it.\nAre you experiencing a bug with an open source project? Dig in and don't stop at your local workaround. Your solution can benefit… others and lead you to more open source contributions. Read other people's code. You might not fix your issue right away, it might take some time to understand the code base, but you will learn new modules, new syntax and different ways to code that will make you grow as a developer.\nOpportunistic contributions\nFirst contributions labels on the the Node.js repository\n\"I don't have an idea\" is a common complaint by developers who want to contribute to open source but think they don't have any good ideas or good projects to share. Well, to that I say: that’s OK. There are opportunistic ways to contribute to open source. Many projects have started to list good contributions for first-timers via labels or tags.\nYou can find contribution ideas by going through these websites: Open Source Friday, First Timers Only, Your First PR, CodeTriage, 24 Pull Requests, Up For Grabs, Contributor-ninja and First Contributions.\nBuild some tooling\nTooling is a nice way to publish something useful to others without having to think too much about complex problems or API design. You could publish a boilerplate for your favorite framework or platform that would gather the knowledge of many blog posts and tools into a nicely explained project, ready with live reload and publishing features. create-react-app is one good example of such tooling.\nThere are 58K boilerplate repositories on GitHub, it's easy and rewarding to publish one\nToday you can also build pure JavaScript plugins for Atom and Visual Studio Code like we did with our Atom autocomplete module import plugin. Is there a very good plugin for Atom or Sublime Text that does not yet exist in your favourite editor? Go build it.\nFinally, you could also create plugins for webpack or babel that are solving a particular use case of your JavaScript stack.\nThe good thing is that most platforms will explain how to create and publish plugins so you won’t have to think too much about how to do it.\nBe the new maintainer\nWhen browsing… through projects on GitHub, you might sometimes find and use projects that are abandoned by their creator. They are still valuable, but many issues and pull requests are sitting in the repository without any answer from the maintainer. What are your options?\n\nPublish a fork under a new name\nBe the new maintainer\n\nI recommend you do both at the same time. The former will help you move forward with your project while the latter will benefit you and the community.\nHow to become the new maintainer, you ask? Drop an email or a tweet to the maintainer and say \"Hey, I want to maintain this project, what do you think?\". This usually works well and is a great way to start your open source career with a project that is already known and useful to others.\n\nExample tweet sent to revive an abandoned project\nCreating your own projects\nThe best way to find your own project is to look at problems that today have no good solutions. If you find yourself browsing the web for a particular library solving one of your problems and you don't find it, then that's the right time to create an open source library.\nHere's another key moment for my own career. At Fasterize we needed a fast and lightweight image lazy loader for our website performance accelerator —not a jQuery plugin but a standalone project that would be injected and must work on any website, on every browser. I spent hours searching the whole web for the perfect already-existing library and I failed at it. So I said: \"We're doomed. I can’t find a good project, we can't do our startup\".\nTo this, Stéphane replied: \"Well, just create it\". Hmm.. ok then! I started by copy pasting a StackOverflow answer in a JavaScript file and ultimately built an image lazy loader that ended up being used on websites like Flipkart.com (~200M visits per month, #9 website in India). After this success, my mind was wired to open source. I suddenly understood that open source could be just another part of my developer career, instead of a field… that only legends and mythical 10x programmers fit into.\n\n&nbsp;\nA problem without any good solution: solve it in a reusable way!\nTiming is important. If you decide not to build a reusable library but rather inline some workaround code in your own application, then that's a missed opportunity. At some point, someone will create the project you might have created. Instead, extract and publish reusable modules from your application as soon as possible.\nPublish it, market it and share it\nTo be sure anyone willing to find your module will indeed find it, you must:\n\nCreate a good README with badges and vanity metrics\nCreate a dedicated website with a nice design and online playground. Want some inspiration? Have a look at Prettier.\nPost your project as answers to StackOverflow and GitHub issues related to the problem you are solving\nPost your project on HackerNews, reddit, ProductHunt, Hashnode and any other community-specific aggregation website\nPropose your new project to the newsletters about your platform\nGo to meetups or give talks about your project\n\n\nShow your new project to the world\nDon't fear posting to many websites; as long as you truly believe what you have made will be valuable, there is no such thing as too much information. In general, communities are really happy to have something to share!\nBe patient and iterate\nIn term of “vanity metrics” (number of stars or downloads), some projects will skyrocket on day one but then have their growth stopped very early. Others will wait one year before being ready for HN frontpage. Trust that your project will be at some point noticed by other users, and if it never does, then you have learned something: it's probably no use to anyone but you  — and that is one more learning for your next project.\nI have many projects that have 0 stars (like mocha-browse), but I am never disappointed because I don’t have high expectations. That's how I always think at the beginning of a project: I found a good problem, I… solved it the best way I could, maybe some people will use it, maybe not. Not a big deal.\nTwo projects for a single solution\nThis is my favourite part of doing open source. At Algolia in 2015 we were looking at solutions to unit test and freeze the html output of our JSX written React components for InstantSearch.js, our React UI library.\nSince JSX is translated to function calls, our solution at that time was to write expect(&lt;Component /&gt;).toDeepEqual(&lt;div&gt;&lt;span/&gt;&lt;/div). That's just comparing two function calls output.But the output of those calls are complex object trees: when run, it would show \"Expected {-type: 'span', …}\". The input and output comparison was impossible and developers were getting mad when writing tests.\nTo solve this problem, we created algolia/expect-jsx that allowed us to have JSX string diffs in our unit tests output instead of unreadable object trees. Input and output of the test would be using the same semantics. We did not stop there. Instead of publishing one library, we extracted another one out of it and published two libraries:\n\nalgolia/react-element-to-jsx-string transforms JSX function calls back to JSX strings\nalgolia/expect-jsx does the linking between react-element-to-jsx-string and mjackson/expect, the expectation library\n\nBy publishing two modules that are tackling one problem together, you can make the community benefit from your low-level solutions that can be reused on a lot of different projects, even in ways you never thought your module would be used.\nFor example, react-element-to-jsx-string is used in a lot of other test expectations frameworks along with being used on documentation plugins like storybooks/addon-jsx.Today, to test the output of your React components, use Jest and snapshots testing, there's no more the need for expect-jsx in those situations.\nFeedback and contributions\n\nThat's a lot of issues. Also, it's faked just to have a nice picture 🙂\nOnce you start getting feedback and… contributions, be prepared to be open minded and optimistic. You will get enthusiastic feedback, but also negative comments. Remember that any interaction with a user is a contribution, even when it seems like just complaining.\nFor one thing, it is never easy to convey intentions/tone in written conversations. You could be interpreting \"This is strange...\" as: it's awesome/it's really bad/I don't understand/I am happy/I am sad.  Ask for more details and try to rephrase the issue to better understand where it’s coming from.\nA few tips to avoid genuine complaints:\n\nTo better guide users giving feedback, provide them with an ISSUE_TEMPLATE that is displayed when they create a new issue.\nTry to reduce the friction for new contributors to a minimum.Keep in mind that they may not yet be into testing and would gladly learn from you. Don't hold Pull Requests for new contributors because there's a missing semicolon;, help them feel safe. You can gently ask them to add them, and if that doesn’t work, you can also merge as-is and then write the tests and documentation yourself.\nProvide a good developer experience environment in terms of automated tests, linting and formatting code or livereload examples.\n\nThat's it\nThanks for reading, I hope you liked this article to the point where you want to help or build projects. Contributing to open source is a great way to expand your skillset, it's not a mandatory experience for every developer, but a good opportunity to get out of your comfort zone.\nI am now looking forward to your first or next open source project, tweet it to me @vvoyer and I'll be happy to give you advice.\nIf you love open source and would like to practice it in a company instead than doing it on your free time, Algolia has open positions for open source JavaScript developers.\nOther resources you might like:\n\nopensource.guide, Learn how to launch and grow your project.\nOctobox, your GitHub notifications as an email. Awesome way to avoid the \"too many issues\"… effect by focusing on the ones that matter\nProbot, GitHub Apps to automate and improve your workflow like closing very old issues\nRefined GitHub provides an awesome maintainer experience for GitHub UI at many levels\nOctoLinker makes browsing other people's code on GitHub a great experience\n\nThanks to Ivana, Tiphaine, Adrien, Josh, Peter and Raymond for their help, review and contributions on this blog post.\nJanuary 2018 update: \nCheck out discussions about this post on Hacker News and Reddit."
  },
  {
    "id": "7203-0",
    "title": "Tips for Reducing the Cost of Your Infrastructure",
    "author": "Eran Chetzroni",
    "text": "In today’s complex world, ops engineers and SREs get to worry not just about quality of infrastructure, but about cost reduction. Here, I’ll share a few tips on reducing cost of servers with popular cloud providers, as well as a general approach for those to whom this type of work is new.\nWhen you get hired as an Ops engineer or an SRE, you probably know what you are getting into and what you are supposed to do: things like maintaining servers, developing build and deployment pipelines, provisioning and initiating cloud servers, monitoring services and the likes come to mind. One thing that does not usually come to mind when discussing the role of the operations engineer are finance-related activities such as cost analysis and cost reduction.\nThis, as it turns out, can sometimes be quite a big part of our jobs in companies.\nWho would have thought, for example, that I’d become quite familiar with a term like COGS, or “cost of goods sold”, defined by Investopedia as “the direct costs attributable to the production of the goods sold by a company”. In plain English: “how much does it cost to create a product before selling it”, or, if you prefer plain engineering English: “how much do we spend to have our services running in production”. Basically, this is about cost reduction: you start off with creating a budget in order to understand how much you are spending, how much you are going to spend, and whether your costs are going up or down. Let’s look step by step on how to do this with cloud services.\nSteps to success with (cloud) cost reduction\nHandling cost in today’s cloud is very similar to other analysis tasks such as monitoring. You have to:\n\n1. Collect billing data\n2. Analyze and visualize the data\n3. Alert on issues\n4. Act\n\nCollect\nAmazon Web Services (AWS) has a very nice billing dashboard, They basically collect the data for you, so you can just skip to the analyze part \\o/. Google cloud platform (GCP) does not currently provide a… very convenient way to see your billing data, but they do provide a way to export your billing data to BigQuery.\nAnalyze\nThere are all kinds of ways to analyze your data, but a simple way you can start with is looking at 2 types of analyses:\n\n1. Monthly Analysis\n\nMonth-by-month analysis over a period of a year (or even less than a year) can give you interesting insights, such as: is the cost of the infrastructure increasing at the same rate as the growth of the business?\n2. Daily Analysis\nTrack the changes you have made this week, and see if they have the effect on the monthly budget that you expected.\nAWS has a great tool for cost analysis — the Cost Explorer:\n\nIt is a part of the billing dashboard and it has very strong analytical abilities: you can analyze cost by date (yearly/monthly/daily), and it has hundreds of other dimensions that can be configured — such as what machines and databases you are spending on, when things start going up or down, etc. A good practice here is to tag assets; for example, you can tag an instance by its principal user so you can quickly track back to the person over-utilizing or under-utilizing a machine. Another good practice is to tag by product which will make it easy to know at a certain point in time how much a project costs.\n\nIn GCP — once your data is in BigQuery — you can use Google’s Data Studio, or Redash, an open source tool to do quite sophisticated analyses. Check out this elaborate blog post from Google. \nAlert\nBoth AWS and GCP let you set up billing alerts, like this one:\n\nIn AWS, you can set up billing alerts (such as notifications of costs exceeding the free tier) in a granular way.\nAct\n\nCleaning. The first and easiest thing to do is start cleaning stuff up — by that, I mean removing machines that are not used. We all start test machines that we plan deleting at the end of the day, but they often stay behind and incur cost for...a while later. I suggest a continuous cleaning plan for machines,… instances, IP addresses, files from S3 (here, you can use the lifecycle management, which automatically moves files from hot storage to cold storage, and then deletes them per the schedule you set), EBS snapshots (if you backup your data)...and — very important if you are using AWS — you should not forget to check all regions.\nData transfer. In cloud services, data transfer is quite complicated. Here is an example:\n\n\nYou can see that cost of traffic between different regions and AWS services differs greatly, so it can be very important to understand where you are transferring data to and from.\nOne tip: communication between regions is relatively expensive, but can be cheaper between regions that are close, or if one of them is new. If you want to be multiregional, one way to save is by finding cheaper connections between regions.\nAnother tip: you can reduce the pricing of load balancers by simply asking for a discount: cloudfront and CDN pricing are quite negotiable at high volumes.\n\nCompute is more simple and there are quite a few ways to save quite a bit of money. Here are a few:\n\n- AWS: Switch to a new generation of instances. For example, the M3 instance had an inferior CPU, less memory, and cost more than the new M4. The recently released C5 instance family has faster CPU than C3 and C4 and once again costs less.\n- AWS: Reserved instances allows you to get significant discounts on EC2 compute hours in return for a commitment to paying for instance hours of a specific instance type in a specific AWS region and availability zone for a pre-established time frame (1 or 3 years). Further discounts can be realized through “partial” or “all upfront” payment options.\n- AWS: EC2 Spot instances are a way to get EC2 resources at a significant fluctuating discount — often many times cheaper than standard on-demand prices — if you’re willing to accept the possibility that they be terminated with little to no warning if you underbid. I highly recommend a… company by the name of Spotinst that can manage the hard work and uncertainty for you.\n- GCP: if your workload is stable and predictable, you can purchase a specific amount of vCPUs and memory for up to a 57% discount off of normal prices in return for committing to a usage term of 1 year or 3 years.\n- GCP preemptible VM is an instance that you can create and run at a much lower fixed price than normal instances on GCP. However, Compute Engine might terminate (preempt) these instances if it requires access to those resources for other tasks. They also last a maximum of 24 hours.\n\nTagging for cost visibility. As mentioned above, as the infrastructure grows, a key part of managing costs is understanding where they lie. It’s strongly advisable to tag resources, and as complexity grows, group them effectively.\nHuman \"resources\". Don’t be shy about asking your account manager for guidance in reducing your bill. It’s their job to keep you happily using their service.\nServerless. I am not going to advise moving all your servers to a serverless architecture. It’s not great for everything, but let’s say you have a cron machine, and it’s running something every hour – perhaps moving it to serverless makes more sense. You pay per function run, and save additional money by saving on operation costs: looking at logs, maintaining the server, etc.\nUsing multi-cloud. This is complicated and not for the faint of heart. You need to deeply think about your architecture before you start, and the real downside is that sometimes you can’t do all the cool stuff that each cloud provider gives you (e.g., cloud formation which works on Amazon but not Google). There are some tools that can help you be multi-cloud, like Ansible or Terraform, or Dockerizing everything to run everywhere.However, you can get credits from Google, then credits from Amazon (then you ask for a few more); you have account managers in both companies and can negotiate with the leverage of actually using… the competing service (this of course does require having some volume in production).\nGo bare metal. Many past disadvantages of bare metal are going away; bare metal companies like LeaseWeb and OVH now have APIs — you can create a server or replace a broken hard drive on your machine without talking to anyone. The prices are significantly lower.\nWe heavily use bare-metal servers and you can read more about it here.\n\nBonus tips\n\nEc2instances.info This is a great tool that helps you see all your instances in AWS, including prices for different regions; you can compare selected services. It helps you see things clearly in one place. Note: while it updates regularly, I would always double check the information.\nKeep learning: the Open Guides for AWS is updated daily and a fantastic place to learn; including their billing and cost section that has many of the things I talked about and more.\n\nCompute price comparison\nLet’s take the example of a pretty strong machine: AWS EC2 M4.10XLARGE - [40vCPU, 160GiB RAM, 10Gig Network], and compare its prices with other possibilities and different optimizations (the table is sorted by provider).\n\n\n\n\nProvider Machine Type\nPrice / Month\nComment\n\n\nAWS - OnDemand\nm4.10xlarge\n$1,460.00\n\n\n\nAWS - Reserved 1 Year\nm4.10xlarge\n$904.47\n\n\n\nAWS - Reserved 3 Years\nm4.10xlarge\n$630.72\n\n\n\nAWS - Spotinstnce\n\n~$447.696\nPrices change very frequently\n\n\n\n\n\n\n\n\nGCP - Sustained Price\nCustom Instance\n$1,054.00\n\n\n\nGCP - Upfront\nCustom Instance\n$873.77\n\n\n\nGCP - Preemptible\nCustom Instance\n$314.40\n\n\n\nLeaseWeb\nR730XD\n$374.99\n2x 10 cores\n256GB DDR3 RAM\n2x480GB SSD\n10 TB traffic\n\n\nOVH\nMG-256\n$365.99\n20 cores\n256GB\nDisks 2x2TB\n\n\n\nThe first thing you can easily spot is that bare metal providers such as leaseweb and ovh will provide the best value for the buck, and they also include storage and traffic in the same package. You can also see that those bare metal providers are much less flexible in terms of machine types.\nAnother thing we need to consider is that… in a cloud environment we can pay for only what we use, so if we need a machine for an hour a day, we actually don’t have to pay a monthly fee, and this might reduce costs dramatically, especially if we use spot instances or preemptible instances.\nHere at Algolia we actually chose a mix of providers. Using bare metal for the Algolia engine and API was the best decision for us, but we also use Google Cloud Platform for our log processing and analytics, and AWS for many different production and internal services.\nThe bottom line is that, as always with building and maintaining a robust infrastructure, you need to choose what’s best for your company and your use case. Hopefully, tips above will help you make the right choices at the right price. Have other tips? We’d love to hear them: @eranchetz, @algolia."
  },
  {
    "id": "7166-0",
    "title": "Introducing TalkSearch — making videos searchable, one conference at a time",
    "author": "Peter Villani",
    "text": "It’s time for one of our favorite annual traditions: revealing the holiday gift we built for the developer community. This year, we’ve made a tool that helps users discover key moments at conferences by searching into video transcripts. It’s called TalkSearch, and we’d like to share a few details about why we built it and how it works.\nWhat is TalkSearch?\nTalkSearch is an interactive search and video experience that any conference or meetup organizer can offer to their community. All the event needs is a YouTube channel or playlist to get started. TalkSearch indexes video titles, descriptions and transcripts and serves up an instant search experience that plays the right video at the right time based on the search results.\nConference and meetup organizers are invited to fill out the TalkSearch request form and get the process started. Once the indexing is complete, a standalone video search page will be built just for that conference. The search experience can then be embedded on the conference’s website (coming soon).\n\nPast community holiday gifts\nEvery year since 2014, Algolia has built something to address a pain point or opportunity in the dev community. Here are a few past examples:\n\n2014: GitHub Awesome Autocomplete, with recently improved UX\n2015:  DocSearch, now used by 450+ open source documentation sites and API portals, including Stripe, webpack and React\n2016: Yarn package search, now at 700,000 user searches per month!\n\nThe inspiration for TalkSearch\nWe wanted to give back to an especially important community for Algolia — the organizers of events. We at Algolia go to over a hundred events every year, whether as sponsors, speakers, or participants. We benefit greatly from the communities that conference organizers bring together and the opportunities they afford us.\nWe’re also scratching our own itch here. When we return from a conference, we often want to watch talks again or see the ones that we missed. Or we want to leap right to the… moments that were the most relevant to us. We might also want to share with our co-workers a particular moment in a talk or a particular slide. TalkSearch was conceived and built to make this process easier, and to help unlock more value from all of the time and energy that goes into writing talks and producing videos.\nHow does it work?\nAs with any Algolia implementation, TalkSearch can be broken down into two main parts: the indexing process (crawling or scraping the data) and the search interface that users will interact with.\nIndexing - Youtube API\nTalkSearch uses the YouTube API to loop through a conference’s channel or playlist of videos. It extracts the essential information from each video — title, author, transcript (YouTube “captions”), tags — and pushes that data into the index.\nEach conference’s TalkSearch data is structured as follows:\n\nThere is one record per “phrase” of the talk, all stored in one index.\nEach phrase contains between 5-10 words on average.\nEach phrase has a start time and duration.\nThe title, author, and description of the video are added to each record.\n\nHere’s what an example record looks like:\nView the code on Gist.\nSearch - standalone or embedded; single or multiple videos\nThe TalkSearch user interface is hosted automatically on a standalone page by Algolia when a new conference is indexed. We do recommend, however, that conferences use an embeddable version to put it directly inside of their site, which will create a more seamless experience for their users.\n\nWhen the user first lands on the standalone page or embedded widget, they can search through all videos at once. Up to 3 locations in each video will be shown that match the search query. When the user clicks a search result, a full-size panel will appear containing just the video selected, which will start playing at the right start time according to the transcript data. Within this view, the user can search into just the current video to find other moments… of interest.\nOpen source on GitHub\nThe search is built with the React InstantSearch library. React InstantSearch and the family of InstantSearch libraries provide a set of components and building blocks that make it easy to build dynamic, full-page search experiences like TalkSearch.\nAll of the TalkSearch code is open source and you can also run it on your own. See algolia/talksearch-scraper and algolia/talksearch on GitHub. Whether you’re a new developer or an experienced pro, we welcome your feedback and contribution in the form of issues and PRs.\nEnjoy!\nWe sincerely hope you’ll find TalkSearch useful as a conference-goer or organizer. If you’d like to create a TalkSearch experience for your event, please start by filling out the request form.\nWe would love to get your feedback to make TalkSearch the best tool that it can be! Say hello on the TalkSearch page on ProductHunt or tweet us at @algolia.\nHappy holidays!"
  },
  {
    "id": "7176-0",
    "title": "Geo-Spatial Search on Mobile: Quick but Not Dirty",
    "author": "Robert Mogos",
    "text": "Geo-spatial search or geo-search is no longer a buzz word or a nice to have in your service or app. If you take a look on the AppStore / PlayStore, more than half of the apps will ask your permission for location.\nAnd here's why.\nLocation matters\nAccording to Google's 2011 \"The Mobile Movement Study\", 77% of smartphone users use their smartphone for search.\nWhen it comes to location, things are getting even more interesting. 95% of smartphone users have looked for local information. After finding the information:\n\n77% contacted the business after — 61% called and 59% visited the location\n44% made a purchase, 26% being online and 36% in the store\n\nWithin a day, 88% would take an action (visit the place, buy, call, etc.)\nLocation matters! And not only for buying food or dining but for the way we interact with friends and even strangers. Apps for dating, transportation, social networks and media all leverage location for better content.\nDo all those apps really need our location for their services? Maybe not, and I am not suggesting we all start draining the battery and fetching the location without purpose, especially with GDPR in Europe. Rather, I am suggesting to devs developing apps to think twice about whether they could improve their search by taking location into consideration.\nHopefully, by this point I convinced you that location matters. Let's see in the next parts how to build geo-search interfaces leveraging user's location for better mobile experiences.\nMy use case\nBefore joining Algolia, I tried my luck and built several startups. One of those startup’s goals was to imagine a new way of interacting with people: putting more emotions into the photos or videos we are taking, and, like Hansel And Gretel, leaving a trace of our moments all over the world.\nThe use case was simple: take a picture or video. Instead of sending it to your friends, leave it in the same physical place you took it. The message could be public and hence seen by anyone around… the place, or private, visible to selected friends.\nAs an example, let’s say you are in a bar and took some very cool selfies (or at least that's what you thought after those five beers). You open the app and post them, so now, everyone around that bar can see your awesome selfies. When your friend, George, who skipped this Friday’s beers comes around next time, he can see how happy you were.\n\nChoosing the dev stack\nAs the tech person in the startup, I was responsible for building an MVP (Minimum Viable Product), which was an iOS app. My use case was simple: being able to search for messages around a geo-location; given latitude / longitude and a radius, show the messages around the place in question.\nSo, I started a Google search for the following terms: geo-search database, geolocation databases, geospatial database. There weren’t many options popping up, but I did find a couple of solutions: MySQL would work, and PostGIS was a more powerful solution.\nSince I had some experience with Firebase, I learned that I could use GeoFire on top of it, which allows to store and query a set of keys based on their geographic location. In case you know nothing about Firebase, it is basically a backend as a service. They have a lot of cool stuff integrated, like authentication, realtime database, storage and so on. In a nutshell, for every message sent, I would save an ID and its location with GeoFire.\nWith this in mind, and the fact that I was building an MVP, using Firebase + GeoFire was much faster than building an entire back end with PostGIS.\nFirebase and GeoFire\nAt its heart, GeoFire stores locations with string keys. Its main benefit, however, is the possibility of querying keys within a given geographic area. GeoFire stores data in its own format and its own location within your Firebase database.\nSo, in my case, for every message I would store in Firebase, GeoFire would store another object containing the ID of the message and its coordinates.\nFor our app,… every time the user was opening the app or moving around, we would fetch his location and start a search. Given the latitude, longitude and a radius, we would listen to each of the messages that was in that area. Because Firebase is implemented as an Observable, each message would come one by one, every time matching the query. So, in the end, we would have a list of messages IDs with their specific location.\n\nFirebase + Geofire limitations\nEven though Firebase is super fast and reliable, we encountered some limitations with GeoFire.\nDisplaying rich messages on a map or list\nGeoFire will only store the ID of a message and its coordinates; you cannot store any other metadata. When displaying your data on a map you will then have to:\n\nGet all the messages and coordinates in a single request\nMake other requests to Firebase to get the other metadata, like title, image url...\n\nThis approach might be OK when you have 5-10 messages around you, but when you have hundreds, it will kill your network.\nFiltering\nOne of the biggest pain points was filtering. On the map, you can see public messages and messages from your friends. Since GeoFire does not support any kind of filtering, we had to filter the results on the client side. If a user only wants to see messages from her friends, we would have to download all the messages around and filter out the ones that are public. Since the ratio was 1 to 100, we were downloading 99 messages for nothing.\nOnly latitude and longitude queries\nOne of our use cases was to show only messages around a certain area. Let's say you only want to see the messages at your school and nothing more —queries in a polygon were out of question.\nTo sum it up, Firebase + GeoFire can be a very useful and quick solution to your geo-spatial search problem. You might have some limitations, but overall, it works.\nAlgolia and geo-search\nLess than three months ago I started working at Algolia, on the mobile team. My ramp-up project was developing an… Airbnb-like search experience using Algolia’s geo features (here's the code) which would display available rooms around you, on a map:\n\nI start reading the documentation and dove into the features — to be completely taken by surprise. Where was this when I needed it eight months ago?\nHere's what I found out, compared to the limitations I encountered with GeoFire.\nDisplaying rich messages on a map or list\nAfter indexing your object and in order to search based on a location, all you need is to add a field _geoloc with given latitude and longitude. That's it!\nSo, when querying for objects around a location, you will get the entire object. This means that, when displaying it on the map, you can leverage all the attributes of the object. For my use case, I could have, for example, displayed the image and name of the person who left a message.\nOf course, keep in mind your customer. Downloading large datasets will impact the speed and network consumption. To offer the best experience, you could:\n\nlimit the amount of results you are displaying at each iteration\nuse attributesToRetrieve to get only the attributes you are interested in. This is very helpful when your object contains attributes that are not useful in your search context\n\nIn my case, I would limit the attributes to only the location, the user's image URL and his name. Anything else, like the content of the message, comments, and so on, would be lazy-loaded if needed.\nFiltering\nFiltering was my biggest pain point with GeoFire. Not being able to search for specific type of messages was almost a deal-breaker. With Algolia's system, the location is only one of the ways you can filter. You can add additional filters like whether the message is public or private, if it is a video or image, and so on.\nSo, instead of downloading 100 messages for only one relevant message, I was able to only fetch what was truly relevant.\nLimit the result set\nThis feature applies to Algolia's search in general: you can set up the… limit and download batches of that limit: check out infinite scrolling.\nNot only latitude and longitude queries\nIn some cases you need more than just results around a position. Maybe you want to display results in a certain area. Instead of using the latitude and longitude you can define a rectangle or a polygon. This way your results are bound to that region.\n\nOther features worth mentioning\nMultiple locations for the same object\nYou can add a list of locations for the same object. If it makes sense for your use case, instead of replicating the same object, add the locations where the object is available.\nAutomatic radius\nAutomatic radius is very useful when displaying results in areas that have too many results or too few.\nBy default, Algolia will expand the search radius around the specified latitude and longitude until approximately 1000 hits are found. If the area is dense, the radius will be smaller. If it’s not dense, the radius will be bigger. The benefit of this feature is that it increases the chance of finding more results in low density areas. If a fixed radius is set, there could be fewer/no results returned.\nResults around user's IP address\nSometimes the GPS is not an option. Maybe the user blocked the access or it is just not working. You can always fall back to displaying results based on their IP address. Algolia will associate a location based on the user’s IP address and search around that location. Here's how.\nConclusion\nWhether you are working on the next big thing or want to improve your product, keep an open mind about the user’s location.\nThe goal of this article was to:\n\nmake you aware of the importance of geo-search\nsince Firebase and GeoFire was the top suggestion I got when looking for a solution, I wanted to give you an overview of features and tradeoffs\npresent Algolia’s approach when tackling geo-search\n\nBy the way, in case you didn’t know, Algolia and Firebase play nicely together. In case you have geo-aware data in… Firebase, you can sync it to Algolia to get these more advanced geo-search features.\nHave you played with geo search? Have cool tips or feedback on this article? We want to hear it: @robertmogos, @algolia."
  },
  {
    "id": "7150-0",
    "title": "Master the Rebase (and the Other Way Around)",
    "author": "Anthony Seure",
    "text": "Git `rebase` is one of those commands that you may have heard of as a substitute for `merge`. Truth is, it’s more than that — `rebase` is a completely different set of tools that intersect with the goals achieved by `merge`. Confused? Don’t worry! This blog post is about different uses of `rebase`. First, as a way to integrate and exchange your work between two branches (like `merge` does), but also as a history rewriting tool.\nFrom merge to rebase\nMerging branch is the most common way to integrate changes between two Git branches. A Git workflow common to services such as GitHub or Gitlab is as follows:\n\nCreate a new “feature” branch called `my-new-feature` from a base branch, such as `master` or `develop`\nDo some work and commit the changes to the feature branch\nPush the feature branch to the centralized shared repo\nOpen a new Pull Request for `my-new-feature`\nWait for your tests to pass and to gather feedback from your peers\n\nFrom there, everything is great. You end up with a nice, clean branch, such as:\n\nHowever, in an imperfect world, here’s what might come next:\n\nCode reviewers have found a few bugs and typos in your first commits and your tests are not passing\nYou make some changes and commit the fixes locally\nYou push your updated feature branch to the centralized shared repo  (C6 and C7 on the following schema)\nMeanwhile, other commits are merged to the base branch (C8 and C9)\nYour pull request finally gets accepted and is merged into the base branch (C10)\n\nAnd from there, your history becomes a bit more complex:\n\nThere’s nothing wrong with this workflow; in particular, you don’t have to bother about what your coworkers are doing so you can focus on your own work. Since the critical part where Git combines (or merges) your changes with the ones from the base branch only happens once, you will only need to deal with eventual conflicts once — at the merge step.\nHowever, a few things are also a bit off here. First of all, if you’re working… on your branch long enough, you may end up out-of-sync with the base branch for days or weeks. This may not be an issue, but sometimes you would have really appreciated including that specific fix that your team merged, or getting rid of that huge dependency that slows you down every time you compile. Secondly, the history may become too complex to understand once all your coworkers have merged  their own branches to the base branch. Lastly — and this one may be a bit more subjective — you probably kept a logical breakdown between the commits of your branch. Having a single merge commit containing all the changes to all your files probably isn’t what you want to expose in the end.\nLet’s see how rebasing may help you address all those issues.\nRebasing on the base branch\nIn September 2016, GitHub introduced a new way to merge pull requests: the “Rebase and merge” button. Also available for other repository managers such as GitLab, it’s the “rebase front door”. It lets you perform a single rebase operation of your Pull Request commits on top of your base branch and then perform a merge. It is very important to observe that those two operations are performed in order, and that the rebase is not a substitution of the merge. Hence rebase is not used to replace the merge, but it completes it.\nConsider the previous example. Before the final merge, we were in this situation:\n\nYou can simulate what happens when you click on the “Rebase and merge” (when there’s no conflict) by performing the following commands:\nView the code on Gist.\nBy doing so, you finally end up with a “linear history”:\n\nAs you see, rebasing is not a substitution for the merging step. As explained before, the two operations are not performed on the same branch: `rebase` is used on the feature branch whereas `merge` is performed of the base branch. For now, this operation just prevents having a single merge commit with all the changes in it, and it’s still a single operation… that happens at the last step of your contribution (i.e., when you want to share your work).\nUntil now, we have only interacted with `master` as our base branch. To stay in sync with the changes of the base branch, it’s just a matter of performing the rebase step with the up-to-date base branch. The longer you wait to do this, the more out of sync you’ll be.\nThe up-to-date version of your base branch is hidden in plain sight. It’s a read-only version of the base branch, prefixed with the name of the remote to which you’re connected, or more simply put: it’s a read-only copy of the branch from your remote instance (such as GitHub or GitLab). The default prefix when you are cloning the repository for the first time is `origin`. More concretely, your `master` branch is the local version of master, whereas `origin/master` is the remote version of this branch, copied on your computer the last time you performed a `git fetch` operation.\nWe've stepped through a lot of theoretical material, but as it turns out, the end result is relatively straightforward; here's how to sync with changes happening on the remote:\nView the code on Gist.\nThe first step retrieves the latest changes from the distant copy of `master` into your local `origin/master` branch. The second one checks out your feature branch. The last one  performs the `rebase` so that all your commits are now added on top of the latest changes that happened parallel to your own work. By applying those commands on our very first example, here is what would have happened:\nAs you can see, the feature branch now includes all the latest changes, so you’re able to work in sync with the rest of your team. By working with the above workflow, you’ll be able to deal with potential conflicts earlier and progressively instead of at the very last moment (when you want to merge your work within the base branch). People often disregard the “Rebase and merge” button because they expect too many conflicts at the very… last step of the process (so they prefer to perform a regular merge commit instead). Ultimately, it takes a small active effort to stay in sync with the latest changes.\nRebasing your own work\nUntil now, we’ve only used `rebase` to apply commits from one branch onto another. This is pretty much the basic use-case of `rebase`: just default options, actions, and results. Furthermore, we are only using `rebase` to integrate changes from a different branch into our own branch. But — it can be used to add/change/remove your commits directly from your own branch too! The “base” on which you rebase can be virtually any commit — even a direct ancestor.\nIn fact, if you wanted to see what was happening during the rebase we did, you could have used the “interactive mode” of rebase by adding the `-i` or `--interactive` argument. By doing so, Git will open your editor of choice (the one defined in your `EDITOR` environment variable) and list all of the commits that will be affected by the rebase operation, and what should be done with every single one of them. This is where the true power of `rebase` lies.\nFrom your editor, Git lets you reorder, rename, or remove commits, but you can also split single commits into multiples, merge two or more commits together, or change their commit messages at the same time! Pretty much everything you’d like to do with your history is possible with `rebase`. And the awesome thing is that it’s relatively straightforward to tell Git what to do. Every commit is presented on its own line, in a sequential order, prefixed by the command that will get applied to it. Reordering commits is as easy as reordering lines, with the most recent commits at the bottom of the list. Removing commits is just a matter of removing the corresponding line, or specifying the `d` or `drop` command as a prefix. One of your messages contained a typo? Just use the `r` or `reword` command to keep the commit, but change the associated commit message.\nTo sum… up, `rebase` is just a Git command that lets you:\n\nSelect one or multiple sequential commits\nBase them on any commit of your repository\nApply changes to this commit sequence as they are added on top of the new base commit\n\nTo better illustrate this, consider the following series of commits:\nView the code on Gist.\nAs you see here, we have a first “root commit” — which will serve as our base commit — followed by 4 commits adding a total of 5 files to the repository. For the sake of the exercise, let’s say this series of commits is your Pull Request, and you aren’t satisfied with it for the following reasons:\n\nThe first commit message is wrong: it should be “add A file”, not “add A”\nFile B and C were added in the wrong order\nFile D should have been added at the same time as file C, not with file E\nFinally, file E should be added in its own separate commit\n\nAll of those changes can be performed with a single rebase. The final history would look like this:\nView the code on Gist.\nNote that, except for our base commit, all commit hashes have changed. This is due to the way Git generates those commit hashes, which are not only based on the changes themselves, but also on the parent commit hash and other metadata.\nAnyway, let’s rebase!\nLet’s start with a `git rebase -i HEAD~4`. This tells Git to interactively rebase the last 4 commits from HEAD inclusive. `HEAD~4` points to the “root commit” which is the commit upon which we will rebase. After hitting ENTER, your editor of choice will open (or `vi` by default on Unix-style systems). Here, Git is simply asking you what you want to do with the commit you performed.\nView the code on Gist.\nAs explained earlier, every line represents a single commit, prefixed by the corresponding rebase command that will get applied. All the commented lines are ignored during the rebase and are here to remind you of what to do now. In our case, we will go with the following commands:\nView the code on Gist.\nHere, we… told Git to perform three tasks for us during the rebase:\n\nStop at the first commit to let us change the commit message\nReorder the second and third commit to have them in the correct order\nStop at the last commit to let us do some manual amending\n\nUpon saving the file and quitting your editor, you’ll be presented with your editor again, and the first commit message will be in front of you. The rebase is happening and you are being prompted to change the first commit message. Let’s change it from “add A” to “add A file”, then save and quit.\nThe reordering of the second and third commits is done transparently by Git. This leaves us with the last amend we asked to perform. Here, we’re stopped after the “add D and E files” commit. As we wanted to create a single commit with C and D files and a new one only for E, we need to perform the following steps as if we were amending additional commits on the top of our branch:\nView the code on Gist.\nThese commands (except the last one) make the “add C file” and “add D and E files” commits become the “add C and D files” and “add E file” commit we wanted to have. The last command, though, is just to inform Git that we’re done with the `edit` step. After that, Git will happily tell you that the rebase finished successfully. Great!\nWe’ve covered pretty much everything you might like to do with your commit history. A few more commands are available and may help you better depending on your use cases.\nHandling conflicts\nWhen it comes to using `rebase`, people are often confused about the way to fix conflicts that may happen when rebasing a branch on top of another one. It’s actually convenient to fix conflicts when they do arise with Git, for multiple reasons.\nFirst, when a conflict arises, Git doesn’t try to be smarter than you — it stops the current `rebase` and asks you to fix the conflict. The conflicting files will be marked  as “both modified” and the conflicting sections will… have some markup to help you find what differs. When you’re done with the modifications, you then `git add` the modified files and run `git rebase --continue` to let the rebase continue.\nSecond, there are two tools that are very powerful when you are not confident with an on-going rebase, or with a rebase that went wrong. Consider `git rebase --abort` which  rewinds history to just before the current Git rebase operation.\nWith these techniques, changes made using `rebase` can be undone, so the risk of impact of making mistakes is minimal.\nFinally, you might find yourself dealing with a long and boring conflict to resolve and even then, it’s possible that the same conflict happens again at a different time. For example, it is common, unfortunately, for the base branch to change while you were working on your own branch. Another scenario is that you aborted a rebase and are now attempting to redo that rebase. To avoid resolving the same conflict again, Git provides a solution which is disabled by default. This feature is named “reuse recorded resolution” or “rerere” and can be enabled with: `git config --global rerere.enabled true`. This way, Git will keep track of all the conflict resolutions you perform. When the exact same conflict happens again, you should see from Git outputs that a recorded resolution was used.\nGoing further\nI hope this article helped you to see what’s possible with the `rebase` command. Of course, the best way to learn Git is to use it, but the second best way is to read about it. If you’d like to read more, I highly recommend the Pro Git book - specifically the section about rebase itself. And, because once in a while we all end up in a bad situation, you should probably take a look at Data Recovery in the Maintenance and Data Recovery section. If you’re not into reading the whole documentation, maybe you’d prefer these Git Flight Rules.\nHave additional rebase tips or feedback on this post? We’d love to hear them:… @aseure or @Algolia. Thanks for reading - and happy Gitting!"
  },
  {
    "id": "7111-0",
    "title": "API Documentation 2.0: From Browsing to Search",
    "author": "Peter Villani",
    "text": "A challenge that documentation writers face is how to encourage developer readers to stay in the documentation and not reach out to support until absolutely necessary. To meet this challenge, writers of course focus on the text — its clarity, coherence, and exhaustivity. They also focus on form, devising ways to guide developers to the relevant text — structuring the information, highlighting key words, hyperlinking, creating menus and submenus, thinking up cool titles. Algolia adds search to that effort.\nAlgolia's new integrated search panel\nFrom browse to search\nDevelopers come to documentation seeking answers, hoping to find what they are looking for. They browse, read, and browse a bit more. Often they find what they need, but not always. Some will eventually contact support for more personalized guidance, which may send them back to the documentation, but this time to the exact paragraph or code sample they were looking for. \nAlgolia’s documentation is about search — to wit: how to use our search API. So, we thought: if our users can’t always find what they need using our own search bar — and worse, to learn later that what they were looking for was actually present in the documentation— what sort of message were we sending about our API?\nSo we’ve faced this challenge head-on with an example of Algolia’s powerful search engine — by expanding our current search bar into a fully-integrated search panel.\n\nThe new search panel is designed to be prominent and conversational, so that whenever our developers ask themselves What is or How to or Why, they simply type the remaining part of their question in the search bar, and our new panel lights up with the answers they are looking for.\nAdopting best practices\nOverall, our UI/UX model was Google. We adopted a Google-like feel, but used our own search engine + the knowledge of our own documentation to drive the whole user journey from question(s) to answer(s).\nWe also believe that search is a… bridge between customer support and documentation. That’s why we included support discussions from our public forum in the new search panel. Now, when you search our documentation you’ll also be looking into our support history. That way, you get a side-by-side view of all relevant information about our API — relevant texts, code snippets, and all support tickets.\nThis time our model was Google + Stack Overflow, that well-known dynamic duo that has saved every developer from the great unknown. Stack Overflow, and more generally community-driven support, have become essential to the developer experience. By integrating our own developer community into our documentation, we will be giving our developers that same standard — and maybe even more, given that we know our own support data and can therefore fine-tune the search results.\n\nFinally, taking this Google/Stack Overflow model a bit further, we decided to display code samples in the search results. Many developers come to our docs with a very specific question in mind; for them, finding a well-written line of code is often the best, most direct answer. So we added a toggle button to switch between text and code, allowing developers to search only for code.\n\nWith these features in place — a prominent search panel, integrated support, and code searching — we hope to extend the trust with our readers, so that they keep coming to our documentation expecting a useful experience. \nWe are also backing up our efforts with analytics: real metrics that will help us follow developers from query to query, page to page, and even from support to documentation. That kind of feedback loop will tell us how we can shorten the reading process and make it more pleasant, and it can also indicate how we can encourage our doc readers to use more advanced features, to push our API to its limits, which benefits everybody. \nAnd we won’t stop at analytics. Because the challenges — to write clear, coherent, exhaustive, and… easy-to-find information — will never go away, we will need to keep improving by focusing on different kinds of search strategies that work particularly well for natural language documentation.\nSearch in more detail\n...or more specifically — what strategies did we use to ensure that our readers find what they are looking for?\nIn a nutshell: a successful document-based search relies in large part on how you organize your content. Global sections, individual pages, headers / sub-headers, and paragraphs — these are only some of the textual elements that, when done consistently and ordered logically, matter a lot. In our case, with a well-thought and cohesive collection of texts, Algolia’s speed and relevance work out of the box. \nAnother focus is on the query itself. The search engine can, for example, behave differently depending on the specificity of the query: for simple, general queries (like “index” or “install”), the content can stay high-level. For longer or more precise queries (like method names, or longer sentences), we can switch the results into more low-level API references and concepts.\nSearching structured data\nLet’s look at what Algolia does best — searching structured data. Here is an example of a T-shirt inventory. If the user is looking for a “slim red T-shirt with nothing on it”, you can help them find it by filtering:\nType: T-shirt\nColor: red\nDesign: blank\nType: slim\nIf the user types in “T-shirt”, they get the whole inventory (1M records). If they add “red”, you divide the 1M t-shirts by 5 (let’s say there are 5 colors). If you add “slim”, you divide by 3 (there are 3 types: slim, wide, and stretch). If you start adding other criteria - like “midriff”, “sleeveless”, “multi-colored”, and so on, you could conceivably reduce the relevant stock to 25 t-shirts. Not bad, from 1M to 25! And a good UI would make this process as easy as possible for the user.\nAll this works as described when the content in… which you are looking contains very clearly defined items. The discrete clarity of commercial products is what lies behind the success of structured data searches. \nBut not everything is so discrete. English language has an unlimited number of ambiguities, so creating categories for natural language is not a scaleable solution.\nUnstructured text — from search to research?\nLet’s now take a look at two queries which make for a difficult search structuring as described above. \nExample 1— legal text\nLet’s switch subjects to better illustrate the point. Let’s say a lawyer types “out of order” in a legal database that contains court cases, laws, and legal journals. For this query, there are at least 4 relevant categories of documents, with each category containing 1000s of documents:\n\nA database can be “out of order”, causing system failure (computer law)\nAn elevator is “out of order”, causing monetary loss (commercial law) or personal injury or death (torts law)\nA lawyer is “out of order” in a courtroom (procedural law)\nA factory produces widgets “out of order”, breaking contractual terms (contract law)\n\nThe lawyer clearly needs to signal to the search engine which of these categories is relevant.\nExample 2 — API documentation\nIt would be the same if a developer were to come to Algolia’s documentation and search for “indexing filters” and find two categories of documents: : \n\nthe back end (creating filters in your data) \nthe front end (filtering at query time)\n\nand four formats:\n\nconcepts, tutorials, code snippets, API reference\n\nThe developer will want to have control over both the subject and format of the documents retrieved. I’ll use the term \"researchers\" for our confused lawyers and developers above.\nAs-you-think\nLet’s go back to the T-shirt example to see if that can help here. That example was about one item: the consumer is searching for one thing, and the quicker they find it the better.\nThe other extreme are… researchers: researchers are often not looking for one thing. Their query is to think about a subject, to get a better understanding and to construct and support an argument. They have to be patient. If they are searching a site with 1M documents, they are ready to scan through 1000s of results (in say one or two hours, or days, or longer), and to read 100s of documents. We are clearly not talking about consumers. \nDevelopers fall somewhere between these extremes. Sometimes they know more or less what to look for and so are searching for one thing — for example, a method or a specific setting. Other times they don’t really know what they are looking for: they might be onboarding, or trying to solve a difficult problem, or looking to enhance their current solution. In this case, they are more researcher than searcher.\nBut even here, we don’t want to waste a researcher’s time with irrelevant results. And we surely don’t want them to fail by not presenting them with important results (this is the difficult balance of precision and recall). \nEssentially, we want researchers to have the same quality results — and the same confidence in Algolia — that our consumer clients have. \nAnd so the challenge is clear. How do we structure our “unstructured” documentation to come up with consumer-grade results?\nSearch strategies for documents\nBehind our new search panel — what we’ve done\nAlgolia’s power lies in structuring the data before searching. To put this into action, we focused on four key areas:\n\nOrganizing content — the way that our documentation is organized (within the page as well as the combination of all the pages) is probably the most important step\nIndexing — structuring the text for search and relevance\nRelevance — testing out numerous queries and engine configurations, to ensure a consistently good relevance\nUI/UX — the developer experience: how to encourage our readers to use and to keep using our integrated search panel.… (Although this is of equal importance, we do not describe how to implement the InstantSearch UI)\n\nOur indexing and relevance strategies follow our DocSearch methodology, which has been well documented by our CTO in a previous blog post on The Laravel Example. There he describes:\n\nHow to structure your texts with sections, headers / subheaders, and small paragraphs\nHow to order the overall content so that that some information is more important than other\nHow to tweak our tie-breaking algorithm using customer ranking (again, relevance)\nHow to configure Algolia with filters, facets, synonyms, and many other settings. \n\nA recent feature not mentioned in the post is our extensive use of Query Rules to handle special use cases like specific products or coding language queries. \nThere is, of course, the matter of documentation tools. We have written about that in a separate post.\nExploring what’s next\nSearching through thousands of documents is not an exact science. There are many pitfalls, and though we’ve solved many of them, it’s hard not to wonder: what happens when there are 1,000,000+ documents? Here are some interesting features not yet implemented.\nA word cloud filter\nAlgolia offers complete filtering out of the box, but we rely on our users to define the most effective filters for their information. One way to do that is to use word clouds. Word clouds, in this context, are a set of filters that act as one global filter. For document-intensive searching, word clouds can be quite powerful. \nFor example, we can help resolve the above lawyer-researcher’s “out of order” ambiguity by using word-cloud filtering: \n\nAs you can see, the four word clouds above match the four distinct areas of law mentioned in the “out of order” example. Normally, a filter is one word: by presenting a set of related keywords within a single frame/word cloud, we offer the user more information to help choose the best filter. And by making these word clouds clickable (as seen… below), the user can think-by-clicking, to test which set of words most closely matches his or her train of thought.\n\nThere are many ways to build word clouds, one of which is to scan each document using specialized dictionaries, to pick out keywords that make the document unique — and to do this before indexing them. For the example above, you would use different specialized legal dictionaries. For our API docs, we would use our own API reference pages as dictionaries for each new document added to our documentation.\nThematic frequency\nSome documents are so similar in terms of relevance that it is impossible to know which should be presented first. At this point, the engine needs help. With structured data, such as shopping items, this is achieved through custom ranking, using metrics like “most popular” or “most sold”. However, using metrics is not always relevant for texts. For example, we can use “most cited” or “most read”, but these metrics are often irrelevant to a researcher. \nSo, why not create front end tools that help researchers — documentation readers themselves — choose between different ways to break ties? \nBelow is one such tool, which implements thematic frequency — a shortcut term to refer to the classification of documents by theme. Each document can be placed in one or more themes based on how close (or far) its content is from the theme. The themes are represented by word clouds. Documents can be scored using the thematic word clouds by matching the document’s content with the keywords contained in the word clouds. Later, filtering can use that scoring to both find and order the documents. \nFor example, here’s a subset of results for the theme “server-side indexing and filtering”, in the order returned by Algolia:\n\nThe UI can offer the ability to switch between rankings:\n\nKeep the ranking returned by Algolia’s tie-breaking algorithm. \nAdjust the ranking with Algolia’s custom ranking feature, ordering the list… by “most read” or “most cited”.\nAdd thematic frequency on top of Algolia’s ranking algorithm, reordering the results according to the strength of the document’s relationship to the active theme. \n\nBy choosing the last option - thematic frequency - the researcher could reorder the results from (1, 5, 20) to (20, 1, 5), because record 20 contains the largest number of thematic keywords. In other words, document 20 goes to the top because it is more consistent with the theme of “server-side indexing and filtering” than documents 1 and 5.\nThese bonus strategies, as well as many others, will keep us - and hopefully our readers - confidently within the conversational search powered by Algolia. \nWe look forward to your feedback on the effort we’ve put in so far, and on future ideas: @algolia, @codeharmonics."
  },
  {
    "id": "7089-0",
    "title": "Cyber Monday Search Review: Walmart, Target, Best Buy, Amazon & eBay",
    "author": "Liam Boogar",
    "text": "Black Friday & Cyber Monday are single-handedly the two most important days of the year for retail & e-commerce players alike. On Friday, American shoppers spent $5 billion dollars online  - that’s 17% more than in 2016 - on a selection of deals that were too good to refuse. Whether in store or online, shoppers looking for flash deals are going to rely heavily on search & discovery in order to proactively find the best deal on headphones, for example, as well as to discover an item, brand or category that they might not have thought of initially.\nToday I’m taking a look at search experience of five of the most popular retailers and e-commerce pure players. Starting with the Algolia Search Grader, I’ll dive into the fundamentals of each site’s search speed, relevance & design. Afterwards, I'll take a look at some of the interesting aspects of the search bar placement, the initial search experience, the results page & the search refinement experience.\nFor this review, I’ve chosen in advance three keywords that I’ll use (with and without typos): my queries can be categorized as Vague 'headphones', Long 'iphone case blue with red stripe', and Branded 'bose headphones'.\nBest Buy\nBest Buy knocked the Search Grader out of the park with a 100% score (here), meaning they have all the fundamental features we would expect from a great search experience - there are a few places where it was a close call, though. Best Buy is typo-tolerant, but it’s not immediately visible in the drop-down menu. Instead, when you hit enter to see the results page for ‘haedphones,’ the page automatically displays results for ‘headphones.’ This may seem small, but if a user starts typing and see the dropdown menu showing nothing, they may get the impression that what they're looking for isn't available here (even if they've misspelled their query).Best Buy’s search bar is a bit small, and the suggested queries before you start typing give you the impression that you’re in… for a text-only drop-down menu, but once you start typing you immediately get both suggested queries and results - this is a great use of the drop-down menu providing both discovery (queries you might not have thought of) and finding what you’re looking for (you can skip the search results page altogether).\nBrand queries like “Bose” take you straight to the dedicated brand page, while Vague queries like “headphones” give you a search results page that empowers you both to discover categories like “true wireless” that you may not have thought of (or thought existed). In addition, refinement of the query is very easy via facets on the left-hand side, which have counts and are searchable by Brand.\nThat said, Best Buy doesn’t seem to handle my Long Query too well - I didn’t get blue or red iphone cases, let alone a blue iphone case with a red stripe. Color identification is a great way to deliver more relevance to longer queries - no need to go full machine learning to interpret ‘blue’ and the color blue on a product.\nLike many retailers, Best Buy leverages Click & Collect with it’s “Pick up Today” tab, a different way of displaying a facet than in the left-hand bar. Best Buy’s lack of slider for pricing isn’t unique; however, we find that a slider for pricing adjustment, such as you might see on Airbnb, is a much more intuitive way to filter by price.\neBay\neBay came in at a 73% on the Search Grader (results here). Not providing results as-you-type, but simply query suggestions in the drop-down menu, makes a big difference in the UX when compared to Best Buy. The results are not instant, and displaying results for anything other than products, like Brands, is not nearly as visible as it could be.\nThe white search bar on a grey header is notably less visible than on most ecommerce sites, and eBay’s dropdown menu only provides two suggestions: query suggestions and categories to search my given query inside of. With so much blank space,… eBay could display top results like BestBuy does and help shoppers who know exactly what they’re looking for skip the search results page altogether. I liked the fact that empty search queries with a specified category in the right-hand drop-down menu took me directly to the category page; however, if eBay’s search was a bit quicker, they could take me to the category page without me having to click-to-search in order to get there.\neBay struggled with my Long Query - I began looking for the New York Rangers case I found on other sites which perfectly matched my query and didn’t find anything; however, with a few query tweaks I did find an appropriate FC Barcelona case and a Coach case that I would’ve expected to show up give my query. eBay also struggles with query variance. If you’re looking to buy a PS4, for example, the two accepted normal queries ‘PS4’ and ‘Playstation 4’ will give you structured results that encourage you to go to eBay’s dedicated PS4 page; however, ‘play station 4’ and even ‘playstation four’ return ad hoc results that won’t likely convert Cyber Monday shoppers looking for a discounted PS4.\neBay’s left-hand facets could use a bit of work as well. Facet counts on the categories, search for facet values & replacing the pop-in modal when I click ‘More Refinements’ would improve the user experience here a bit.\nWhile eBay performs better on paper than some of our other sites, the quality of the experience is still rough around the edges.\nAmazon\nThe Search Grader gave Amazon a score of 70% (results here) - Amazon most notably gets docked hard for being slow to load results and requiring users to click to display results, instead of displaying instant search results as the user types. Instead, Amazon displays suggested queries in the dropdown menu bar, along with some options for category filters. Still, this is pretty common practice among top e-commerce sites today, and Amazon is making a conscious decision here… to make their desktop and mobile drop-down search look the same.\n\nAmazon’s home page makes search pretty prominent, displaying a white search bar on a dark header, meaning the search bar doesn’t have to take up a lot of real estate. Amazon’s home page is very geared towards discovery, displaying Black Friday deals by category, recommended deals, hot deals. It’s deal-city!\nAmazon displays a category drop-down menu directly, but selecting a category doesn’t show you results and hitting enter without a query doesn’t take you to the well-designed category page, but instead displays top results in that category for an empty query.  Amazon’s drop-down menu displays suggestions, but takes up a lot of real estate which could easily be used to display results directly in the drop-down. Our Solutions Engineer Olivier Lance pointed out that this may be because many ecommerce sites imitate what works on mobile for their desktop version, opting for experience parity instead of optimizing for each interface.\nAmazon’s search results page performs well across a variety of queries, including the typo-ed ‘iphone case blaue with red stirpe,’ which turns up a nifty New York Rangers iPhone case that meets all my typo-filled criteria.\n\nMy Vague query pushed a ‘Recommended’ result as well as a ‘Best-Seller’ result, which is a great way to drive the user to discover products. For refining my search, I am disappointed that the facets lack counts, and I can't search for facet values (like refining my search to find Bose headphones) inside the long list of brands, and are all-around a bit too cluttered for me to really make use.\nAs an accepted authority on shopping, Amazon makes its recommendations known, appearing regularly in the first few results. It could be easier to get access to the long-tail of Amazon’s catalog - I had to open up ‘Show all departments’ and select electronics to see other cases, even though case was in my query. By not displaying… results instantly with each keystroke, it also takes longer to check Bose headphones and then change your query to Audiotechnica headphones.\nTarget\nTarget scored an 83% on the Search Grader (here) - like most sites we’ve looked at today, Target doesn’t display results in the drop-down menu, so users have to click-to-search. Query suggestions aren’t typo-tolerant on the drop-down menu, meaning my Vague Query only gets suggested popular brand queries if I don’t make an error. The left-hand ‘categories’ section is also a bit of a rabbit hole - sub-sub-categories is a lot of diving before seeing results.\nWhile they may not be typo-tolerant, I did like that query suggestions appear after you click the search bar before even typing - this is a great way to encourage discovery in a minimalist way that doesn’t feel promotional. After typing, the lack of instant results and typo-tolerant query suggestions means that Target is hoping you’ll click-to-search and that their results page will seduce you.\nTarget’s search results page stands out for a number of reasons. They have a lot of the features we love to see in a search results page - search for value values in the brand facet, facet counts for almost every facet - they even have a few features that eCommerce stores could get inspired by. For a retailer, I think Target’s execution on the click-and-collect feature is among the most natural I’ve seen - the simple checkbox ‘get it today’ above the results is very inviting, and the UX is very slick. I also very much enjoy the hover-triggered second image that Target displays on results - it’s definitely eye-catching, and if the load speed doesn’t suffer, it’s a great trick for image-centric search results pages.\n\nWhen digging in with a long, typo-ridden query like ‘iPhane blaue case’ (I actually typed that by accident - don’t judge), I found that, while Target understood ‘iPhone,’’ it didn’t understand ‘blue,’ and I really… started to feel the limits of Targets relevance here. I tried a Vague query ‘Phone’ and was a bit surprised to find two flip phones &  a landline among the top results - this may be the most relevant results for their shoppers, but I would’ve expected newer products, as someone looking for a flip phone will likely drill down by price or category to get there.\nWalmart\nPicking up a 90% on the Search Grader (here), Walmart has all of the same characteristics & features I've come to expect in these five Cyber Monday deals sites. It's typo-tolerant, it's injecting business metrics, and results load pretty quickly. There's not much feature-wise that can be said about Walmart that hasn't already been said about our predecessors, so let's jump in to the experience.\n\nWalmart's relevance really blew me away. It crushed it on my Long Query, and when I refined by device (not in image), I got various options on striped & colorful phone cases. I would've preferred to see facet counts and the facets are a bit too cluttered for me, but Walmart really came through here.\nMy vague query turned up headphones for under $10 - a truly Walmart experience - and when I refined my query to 'Bose Headphones,' it picked up on the brand and gave me a full-banner ad letting me know that the high-end brand was indeed available at Walmart.\nThe experience itself was a bit cluttered - not just the facets, but the drop-down menu and the second menu that runs underneath the search bar - it took me far too long to find out how to to click-and-collect, for example.\nBest Practices & Opportunities\nAcross these five sites, it was clear that there are some common practices and some places for improvement.\nIn terms of placement, it is pretty common practice to make the search bar as visible as possible - you can’t go wrong with a white search bar on a dark header. Dropdown query suggestions are the mode du jour, but I think Best Buy takes the cake for leveraging the width of the desktop drop-down bar… to display instant search results. It’s one less load time, one less second of attention before shoppers go somewhere else.\nIf content is king, then relevance is queen here. The sites that stood out to me were the ones that dug through the color interpretation, through the typos, and through the vagueness to provide me not only with relevance results, but with context-enriched suggestions for categories, brands & products that I might be interested in.\nBalancing discovery & finding isn't just a relevance problem. Shoppers are increasingly willing to refine an initial search if they don't find what they were looking for - they're having a conversation with your search bar - but many of the biggest e-commerce sites could make that conversation more fluid by providing more value to their facets and by providing instant search results for queries that are refined within the search results page."
  },
  {
    "id": "7062-0",
    "title": "Pragmatic Releasing: Less Worry, More Shipping",
    "author": "Raymond Rutjes",
    "text": "Here I’ll share a few things I’ve learned managing libraries at Algolia. They are by no means a perfect set of instructions or recommendations, but tips for making your (developer) life easier. \nHere are some good signs that you may want to re-work your release workflow:\n\nYou tend to batch features before releasing them\nYou wait as long as possible before implementing something although you know well it will eventually need to be done\nYou are stressed when releasing\nYou’d rather leave bugs in your code than have to release\nYou only release when forced to, and in general it puts you in a bad mood \n\nDon’t rely on a single person to release\nWhen I was just about six months into my job at Algolia, a bug inside one of our JavaScript libraries impacted customer search implementations in production. We realized that one of the latest changes committed to the repository was having unexpected side effects, which led to always displaying empty search results. Unfortunately, the one person who knew how to release the library had just left the office and was commuting home. Thankfully, they still managed to point us at a document detailing the release process of the library. We were relieved. \nThe team’s action plan was to:\n\nRevert the latest code changes introduced during the day\nQuickly release a patched version\nCalmly investigate the issue we had pushed while clients enjoy a working search\n\nThe first step was easy to do given we had used Git as the source control system for the project. We just had to revert some of the latest commits. For the second point though, the procedure included many steps.\nEven though having a lot of steps is not a problem in its own right, what was a problem is that a lot of questions came up: \n\nWill I be able to authenticate to the npm registry?\nAm I even part of the organization owning the library on npm?\nHow do I propagate the change to the CDNs so that clients can have their bug fixed as fast as possible?\nI thought that the CDN… update was now automated, why do we need to do that part? \n\nEven if I was asking myself only one of the above questions, I would have started doubting myself and feeling uncomfortable about the entire process. Asking myself all four was...too much. \nI learned two things that day:\n\n1. Any developer in the company should be able to release any given project. Put differently: relying on a single person to deploy/release a project can be dangerous.\n\n2. To be able to have anyone deploy a project, they must be comfortable doing so. Comfortable means that no questions are left unanswered in the release process, and that the release process itself is simple enough to be actionable.\n\nTo make sure a fair amount of engineers in your company can deploy a project, the best solution I can think of is a single bash script which would guide them through the publishing steps.\nSome companies like Etsy take this quite seriously and have their new employees release something to production from day one in the company\nNext time you publish a project, here are a couple of questions you could ask yourself to evaluate the quality of your release workflow:\n\nCould this be released by someone else than myself?\nIf yes, will they have to reach out to me to fix the build?\n\nAutomation is key\nI recommend automating the entire release process as much as possible, which includes taking care of those tiny little things you think are not worth automating.\nFor example, you should not have to replace the version number in any file manually, the reason being that one day you will forget about replacing one version number, or one day it won’t be you releasing. That day, you will probably lose a lot of time and possibly negatively impact production environments. Furthermore, chances are that the time you will lose that day is more than the time it takes to automate your release process today. \nA good practice is to aim for a single command line you can execute to get your software released. Ideally the… command should be interactive and guide you in the process by asking some questions like::\n\nDo you want to release a beta or production version?\nHere are the current changes, which one would you like to push as a new version?\nHere's the previous changelog and version, given we use SemVer and the selected changes, what should be the next version?\nDo you also want to release the documentation website?\n\nHere's an example including some of those questions:\n\nMake sure your can release fast\nIn almost every project I work on, there is some kind of a continuous integration setup. \nOn every new commit pushed to the repository under source control, all tests are run in a single environment or in multiple ones. This has the advantage of making sure future releases are working correctly on the targeted platforms, but has the drawback of slowing down the time needed for the release to get out.\nBecause implementing pragmatic releasing is an iterative process, some projects I’m working on are still taking up to 45 minutes to have all tests pass. This mainly happens when the project has many integrations and end-to-end tests including relying on calls to an external API, Algolia in my case.\nHaving long running tests like these can be a real bottleneck for productivity. I would personally tend to avoid having to add features to those projects, because I know it is going to be time consuming.\nJust to give you an idea, here is what the process of adding a single feature to such a repository looks like:\n\nPush the changes\nDo something else during more or less 45 minutes\nGo back to the project, and eventually realize that a test failed on a given platform\nPush a fix to support the platform and wait again for 45\nEventually remember that you had pushed a given feature to the repository and ask for reviewers to approve the code\nRedo steps 2 & 3 if any feedback has been given by any reviewer\nMerge the changes\nRelease the project with newly integrated changes\n\nIn a best-case scenario, it… takes about an hour to release even the simplest possible feature. If you are unlucky, though, you could spend a day working on getting a feature out. Now imagine this feature is an actual bug fix impacting production environments. It would be totally unacceptable to have to wait one full day to get the patch out.\nIdeally, the time to release a new version should be equivalent to the time to implement the feature and get it reviewed. A few ways to help accomplish this: \n\nReducing as much as possible any kind of a long running process directly impacting the speed of releasing new features or bug fixes.\nIf there are end-to-end tests relying on external APIs in place, caching API responses, and making tests run on mocked calls.\nIf you often have to reject PRs because the format of the commit is incorrect, have your CI platform validate the format for you. If your CI platform is slow, it is worth investigating that issue as well.\n\nWhatever the issue might be, taking some time to speed up to release cycle is key to fast iterations.\nBe confident in your code\nIn the previous paragraph, I shared how much I think speed of releasing is important for a project. One thing that is equally important is the quality of the builds you are shipping.\nThe moment you get something released, you have to be confident it achieves what it was designed for. In other words, you should test your code to ensure business expectations are met. There should be no way to release a project that has failing tests. \nHowever, I think that it is also perfectly fine to ship a “work in progress” feature as long as it doesn’t impact other features. If you have a chance to break down a big feature into many smaller ones, you’ll be able to iterate faster because the review will be easier.\nA couple of ideas to challenge your existing release workflow\nHave someone else do your next release\nNext time you are about to release your library, ask a colleague who knows nothing about your project to deploy… it. Give them the URL of your repository as the only instruction. If they succeed in releasing, it probably means the quality of your release cycle is not bad.\nIterate faster, deploy more\nNext time you have an incoming task that is easy to address, get it done right away and force yourself to release the change. If you feel like you would have preferred to open an issue and deal with it later instead, it probably means you can still optimize your release workflow.\nThe benefits of releasing often\nImprove your mental well-being\nWhen you reach the stage where you can release on demand, you address issues differently. Instead of polluting the repository with issues that would distress and distract you, you get things done instead. Plus, given other teammates are able to release the project without your help, you can live without the fear of having to remotely guide a stressed out coworker on your day off. \nImprove your productivity\nWhen you have a robust and simple release script, you can release when you see fit. The mental gap between release intention and the actual release should be negligible. \nBe more reactive\nIn the “Don’t rely on a single person to release” section, I shared a real story about a project that impacted real production environments. Between the initial report and the actual fixed release we lost about an hour of time.  By implementing a release process that follows the principles shared here, the time to release the bug fix is now equal to the time necessary to implement the bug fix. In most cases, this would be a couple of minutes if you are using a source control system like Git: just the time to revert the changes and release again.\nTooling to create better release workflows\nHere’s how I like to design the release workflow as of today:\nUse conventional commits\nConventional commits dictates a format which every single commit of your repository should follow. By doing so, you’ll be able to:\n\nAutomate the generation of your CHANGELOG… file at every new release. Conventional Changelog is a good tool for this.\nMake it easy to see what has been done since last release by dumping out unreleased work.\nMake sure your CHANGELOG never misses a single entry and that the formatting stays consistent.\n\nI would recommend you use conventional commits to avoid the burden of manually having to update the CHANGELOG file.\nI would also suggest you have your CI platform test all commits to ensure a badly formatted commit cannot be committed to your master branch. You could use something like commitlint to ensure the format is correct.\nCreate a release-pr script\nCreate a script (in bash for example) that will do the following:\n\n\nCheck that you are currently on master\nCheck that the working tree is clean\nInstall the dependencies \nRun the tests\nAsk for the new version of the project after having dumped the unreleased changes\nUpdate all the version numbers in all the files where it appears\nPush a new branch `chore(release): ${VERSION}`\nWait for a teammate’s approval\nMerge the branch\n\nHere is an example of such a bash releasing script:\nView the code on Gist.\nCreate a publishing script (manual)\nCreate a script (in bash for example) that will do the following:\n\nCheck that you are currently on master\nCheck that the working tree is clean\nInstall the dependencies \nRun the tests\nPush the new version to a public listing if required (e.g., npm for the JavaScript example below)\nTag the current commit with Git with the current version, and push the tag to the remote repository\n\nHere is an example of such a bash publishing script:\nView the code on Gist.\nCreate a publishing script (Continuous Delivery)\nPrevious step assumes you manually check out the changes after you have merged the release branch, and then run the publishing script. You could also let your CI platform handle this publishing for you every time something gets merged into your master branch. Personally, I like to keep this publishing task manual, because if… something goes wrong, you are able to fix it easily.\nFinal word\nReleasing software should be something you enjoy doing: each time you release, you either fix a bug or introduce awesome new features. Spending some time to optimize your release workflow helps delivering better quality faster.\nI would love to hear what you do to make your release flow less stressful and more fun: @rayrutjes."
  },
  {
    "id": "7020-0",
    "title": "Comparing Algolia and Elasticsearch for Consumer-Grade Search Part 3: Developer Experience and UX",
    "author": "Josh Dzielak",
    "text": "According to its 2017 State of the Octoverse report, Github now hosts more than 25 million active public repositories. Many of these projects are tools made by developers for developers, and being able to search through the masses and find exactly the right tool for the right problem has become a valuable skill of its own.\nCIOs, engineering directors and project managers don't only rely on developers for tool choice because of their technical expertise. They also understand that developer happiness equates to developer productivity—which with engineering salaries and talent shortages at a historic high—translates directly into higher quality products and lower total-cost-of-ownership for internal applications.\nDeveloper experience, it turns out, is good for business.\nTools with great developer experience are marked by intuitive design, up-to-date documentation, clear error messages, rock-solid stability and access to friendly support. Underpinning all of that, the tool must solve the problem the developer needs it to.\nDevelopers love fast test suites and code that’s easy to follow\nDeveloper Experience in search\nBoth Algolia and Elasticsearch have risen in popularity because they provide a better developer experience than the search platforms of the past.\nThough a critical feature of many applications, search remains a challenging problem for developers to solve. Moreover, building search as fast and relevant as Google and Amazon—companies that set the consumer’s expectation of what search should be—is a great deal harder than building the press-enter-and-wait search of the bygone Web 1.0 era.\nTo compete, engineering teams need to go faster than ever before, identifying and adopting tools that shorten the developer journey and enhance the developer experience at each stop along the way.\nIf developer journey is the “where”, developer experience is the “how smoothly”\n \nIn Part 1 of the Comparing Algolia and Elasticsearch for Consumer-Grade Search… series—End-to-end Latency—we saw that search performance requirements are more demanding than for other types of applications. In Part 2—Relevance Isn’t Luck—we dove deep and looked at how search engines accomplish the task of serving results that accurately match user intent, even when intent is scarce, flawed or unexpected.\nHere in Part 3, we’ll look at the high-level journey that developers take to build search, using an API like Algolia or an open source tool like Elasticsearch. We’ll also focus in one of the parts of the journey that’s important to consider early on—the crafting of the user experience.\nFive aspects of a “SUPIR” search\nNo matter what search technology a developer is using, there are five aspects that the developer must consider on the way from design to production. These are:\n\nScalability\nUser Experience\nPerformance\nIndexing\nRelevance\n\nRemember them easily as a typo-tolerant acronym - SUPIR.\nOrdering the steps this way makes a convenient acronym, but it is generally not the order that the developer proceeds in. Nor is the order linear. For example, indexing data is required before testing out any relevance or user experience, and is therefore a key step during an early prototyping phase. But indexing will also be revisited later, when additional signals become available for relevance or when scaling puts stricter demands on record size.\nWhat makes search DX different\nJust as the world that your users live in changes frequently, so must their search. Like performance tuning, relevance tuning is never “done”, it can only be “done for now”.\nThis is the first differentiator between building search and building other types of applications. Developers don’t visit each SUPIR aspect just once but on a progressive and recurring basis, roughing out and sharpening each one during design and build, and later maintaining and improving them as an ensemble in production.\nThe second differentiator between search and other… relational or NoSQL database-backed applications is the tighter level of coupling required between each layer of the stack. In traditional CRUD-based apps, the implementation details of the database’s storage engine have little bearing on the types of user experiences that can be built on top of it. This is not true in search—the feasibility of a particular user experience can come down to specific implementation details like whether the search engine uses Tries or not.\nThe iterative nature and tight level of coupling in search can be the source of numerous headaches for developers. Improving one SUPIR aspect can break several others, a frustrating event that feels more like retracing steps than moving forward. Some examples:\n\nA relevance improvement for one query makes all queries slower\nA relevance improvement for one query worsens relevance for others\nAt scale, heavy indexing loads begin to degrade search performance\nDegraded search performance requires implementing UX workarounds\n\nAs we head to the next sections and start to address the individual aspects of SUPIR, we’ll take care to point out what developers using Algolia or Elasticsearch need to do to avoid these pitfalls. First up, we’ll look at the “U” in SUPIR—user experience.\nCryptic error messages cause developers distress\n \nUser Experience\nIn search, it’s important to begin with the end in mind. What should happen when the user starts typing in that shiny new search box? The user interface (UI) and user experience (UX) will heavily affect the design of the other four SUPIR aspects, so it’s important to sketch it out early.\nDesign and best practices\nThere are many different types of search UX, ranging from a simple keyword-based autosuggest up to a full-page search-as-you-type experience with several ways to facet and filter. Today’s search experiences also often include powerful browsing and navigation capabilities.\nTo be on par with what we expect from Google or Amazon, the search… should feel like it’s a step ahead of us and practically guessing what we want. It should also provide instant feedback so we can correct our mistakes or refine our search, and it should work wherever we happen to be computing from. A satisfying search UX will:\n\nUpdate results immediately as the user types, putting them in control of the experience\nHandle spelling mistakes, omissions and concatenations\nMake an obvious connection between search terms and results\nWork on all web and mobile platforms\n\nIn many cases, these additional search UX patterns will be used to enhance the experience:\n\nHierarchical faceting and filtering\nBrowsing with or without keywords, including pagination\nSearching multiple different data types simultaneously\n\n \nA powerful product search with instant results\n \nOnce the initial requirements and design for the UI/UX have been decided on, the next step for the developer is to start building it. Ideally, they won’t have to start from scratch.\nFrontend API clients\nBoth Algolia and Elasticsearch have a REST API that makes it possible to interact with them over HTTP—therefore directly via web browsers and mobile devices. API clients contain the logic to send requests over HTTP, parse responses, and handle network and security concerns. They’re an essential building block for user interfaces and user interface libraries built on top of them. They’re also one of the first places where the developer looks for guidance before they start coding.\nAlgolia and Elasticsearch both have a JavaScript API client that works in the browser and node.js. Algolia also has official native API clients for iOS and Android.\n\nAPI clients, security and performance\nAPI clients must also implement security, including authentication and authorization logic. In search, this goes beyond just passing auth credentials. Just because a user can search an index doesn’t necessarily mean they’re allowed to retrieve all records, or see all of the fields of each… record.\nWhen using Elasticsearch, this level of fine-grained security is often implemented by an application server, rather than exposing Elasticsearch to frontend clients directly. You can think of the parallel to your primary application database like Postgresql or MongoDB—these databases are accessed directly by your servers, not your users’ clients. The advantage of this is flexibility—you can implement any authorization scheme you like—but the disadvantage comes in having to build and maintain it, thereby adding an additional step to the developer journey. A second disadvantage comes in terms of performance, as every search query will need to be processed by an application server that sits in front of your Elasticsearch instance, and it is likely that the application server will perform one or more calls to your relational database in order to authorize the HTTP request. Many precious milliseconds are consumed therein.\nWhen Elasticsearch is accessed this way, the developer does not use a frontend Elasticsearch API client at all, but connects to an existing backend which proxies the request to an Elasticsearch instance.\nAlgolia is designed to be called directly from the client, with authentication and authorization being handled by passing API keys. Algolia secured API keys can encode a lot more information than just an authorizing token, including index and search parameter restrictions, ipv4 source restrictions, rate limits and expiry information. This gives developers flexibility when it comes to security, without slowing them down.\nThe direct connection between frontend API client and Algolia host is also fast. The entire search request is fulfilled by an Algolia module running inside of NGINX—no expensive application servers or database calls required. Lastly, the connection is reliable. API clients are aware of all 3 Algolia hosts in a cluster, as well as any global DSN replicas, and can work around any downed hosts. Additionally, Algolia API… clients can fallback to a second DNS provider in order to work around DDoS attacks, like the 30-50+ Gbps attack on NS1 in May 2016.\nSo while API clients may seem simple at first glance, the details can have a big impact on both performance and reliability — and therefore the quality of the user experience. If developers don’t have to reinvent the wheel just to pass data back and forth between clients and servers, their experience (and productivity) will be vastly improved.\nReusable UI building blocks\nThough the API client is fundamental, it’s still only a small piece of the overall frontend story. User-initiated keystrokes and clicks must be handled, results must be displayed and made selectable. That could be via an autocomplete menu or a more advanced page with controls for faceting, filtering and pagination. In either case, this can add up to a lot of custom HTML, JavaScript and CSS.\nIn the Elasticsearch ecosystem, community developers have created projects like SearchKit and ElasticUI to help create these interfaces.\nIn the Algolia ecosystem, an officially-supported family of libraries called InstantSearch was created to encapsulate the various consumer-grade UI/UX search best practices and make them easy to implement. Each InstantSearch library is composed of a set of UI widgets that can be added one by one to a search page. Here are examples of common widgets:\n\nSearch box\nHits (for displaying results)\nSort by selector\nRange slider\nRefinement list (for faceting)\n\nThe framework-agnostic JavaScript InstantSearch library, simply called InstantSearch.js, contains a total of 18 ready-made widgets. It also exposes factories for creating others. Each widget is connected to any change in the state of the search and re-renders automatically. Other members of the InstantSearch family work similarly. Here’s a quick look at each one.\n\nFull-featured API clients and component libraries provide a good DX for building the UX— the “U” in our SUPIR search. They… also reduce maintenance and total cost of ownership by cutting down on custom code. Component libraries in particular mean that the engineer building the UI doesn’t need to be a JavaScript expert; in most cases a basic working knowledge of JavaScript will do.\nConclusion\nIn a consumer-grade search implementation, the user experience is critical. Consumers are picky and, if unsatisfied by a site or mobile app’s search, are likely to abandon it in search of an alternative.\nBecause we rely on developers to build the amazing search user experience that we desire, we have to account for their experience as well. A tired, frustrated development team can’t compete with a team that is happy and productive. A developer mired in the details of applying security patches or debugging deeply-nested JavaScript promises may not have the time—or the energy—to make their product’s ambitious design spec a reality.\nWith an open source and more open-ended tool like Elasticsearch, the degree to which a developer gets mired or inspired is largely a function of their search-building experience and expertise. With Algolia, many of the most complex moving parts of the search-building journey—including infrastructure, performance and scalability—are handled automatically underneath the API, freeing up the developer to focus on building a great user experience and ultimately a great product.\nRead other parts of the Comparing Algolia and Elasticsearch for Consumer-Grade Search series:\nPart 1 - End-to-end Latency\nPart 2 - Relevance Isn't Luck\nPart 3 - Developer Experience and UX"
  },
  {
    "id": "6982-0",
    "title": "Yarn: From Zero to 700,000 User Searches per Month",
    "author": "Haroen Viaene",
    "text": "Since December 2016, as part of our yearly community gift effort, Algolia has powered the JavaScript packages search of the Yarn website. This blog post explains how we collaborated with the Yarn team, what the challenges were building such a search interface, and how much this search is used today.\nYarn is a JavaScript package manager similar to npm. In the beginning, there was no way to search for JavaScript packages on the Yarn website. Since we were heavy users of Yarn, in December 2016 we built a proof-of-concept search UI, and it was released on their website one month later (Try it here!). As of today, every month there are about 700,000 searches (that's 2.3M API calls) being done on the Yarn JavaScript packages index on Algolia.\nnumber of user searches per month on the Yarn website\nFrom a Slack discussion to PR and Merge. All in one month.\nSearch on the Yarn website started with the documentation. We wanted people to easily find information on how to use Yarn. As with 300 other programming community websites, we went for Algolia's DocSearch and this was merged in yarnpkg/website#105. Then another Yarn contributor (@thejameskyle) asked in yarnpkg/website#194 if there should be package searching abilities, much like npm had.\nThis is where Algolia came into play. We are a search engine, Yarn wants search and we are heavy users of Yarn, so we figured: let's do this!\nThis is how it started on December 5th in our #2016-community-gift Slack channel:\n\n\"Hey what could we build for the JavaScript community that would help them in their daily workflow?\"\n\"It's not that easy to find relevant JavaScript packages when you need one\"\n\"I like Yarn a lot...\"\n\"Let's build Yarn search!\"\n\nWe wanted the community to feel empowered by a great new way to search for JavaScript packages. This was also an opportunity for us to work on something cool while benefiting the company. Three weeks later, on December 22th 2016, via yarnpkg/website#322, we proposed our first package search… solution. Ten days later it got merged, and instant-search for JavaScript packages was available on the Yarn website.\nIn early 2017, we met with the Yarn team for a one day in-person brainstorming in London. The goal was to think about evolutions of the search experience along with defining a package details page. Algolia proposed design views of what search could be and from that we drafted a master plan.\nFeatures behind a great package search\nIt shows results instantly ⚡️\n^ This is not sped up. It is THAT fast (try it!). Yes, it still wows even us every time.\nInstead of showing a dropdown of results, we chose to replace the page completely with the search results. This requires more data to be available immediately, but gives more context on the decisions you make while searching for a fitting package. Having the search page be the main entry point will make sure that you don't need to know exactly what you want before \"committing\" to a search.\nIt displays a lot of metadata\nAfter using npm search many times, we knew what was missing and what was superfluous from the search results and package detail pages. We brainstormed a bit and iteratively added a lot of useful metadata.\nHere's a comparison between the two search results pages (npm on the left, Yarn on the right):\n\nnpm search results on the left, Yarn search results on the right (click to enlarge)\nIn the search results of Yarn we decided to directly display, for example, the number of downloads for every packages, the license, direct links to GitHub, and the owning organization.\nThis metadata helps the user to not have to open many package detail pages before getting the information they want.\nIt has completely rethought package detail pages\nFor the package detail page, we took a similar approach. We started with the same base metadata as npm shows, but also took the opportunity to add a lot more. We decided to show changelogs (when available), GitHub stargazers, commit activity, deprecation messages,… dependencies and file browsing.\nHere's what it looks like:\n\nnpm detail page on the left, Yarn detail page on the right (click to enlarge)\nWe believe (and we had a lot of feedback about it) that all those additions are providing an enhanced experience that helps users when finding and comparing JavaScript packages.\n\nTIL yarn has a responsive package details pagehttps://t.co/w2QkQoDP9P pic.twitter.com/wBnQ9biD85\n&mdash; John-David Dalton (@jdalton) March 30, 2017\n\nThis is an iterative process, and suggestions and feedback are always welcome.\nTechnical implementation and challenges\nThe first step to providing a search for JavaScript packages is to replicate and monitor changes from the npm registry into an Algolia index.\nThe code for this replication is all open source and available at algolia/npm-search. The most important API being used here is the npm replication API.\nThe npm registry is exposed as a CouchDB database, which has a replication protocol that can be used to either set up your own npm registry, or in our case a service (the Algolia index) that has the same data as the npm registry.\nReplicating the npm registry\nReplication in CouchDB is a very simple but powerful system that assigns an \"update sequence\" (a number) to any changes made on a database. Then, to replicate a database and stay in sync, you only need to go from the update sequence 0 (zero) to the last update sequence, while also saving the last update sequence you replicated on your end. For example, right now, the last update sequence known on the npm registry is 5647452 (more than five million changes).\nEarly on we saw that going from 0 to 5647452 was very slow (multiple hours) and we wanted it to be faster. So, we made a replication system consisting of three phases:\nThe bootstrap. Instead of going over all update sequences, we save the current last sequence, then we list all JavaScript packages and replicate them by bulk to Algolia\n\nThe catch-up. Starting from our last known update sequence,… we catch up to the new last update sequence of npm (maybe 5000 changes since bootstrap start, which is fast)\nThe watch. Once we are \"up to date\" then we just watch the npm repository for any new changes and we replicate them\n\nFor all of those phases, we use the PouchDB module which we recommend because it has an awesome API to interact with CouchDB databases.\nGetting the necessary metadata\nAll the phases go through ​the same steps to get the required metadata for displaying. Some of the metadata is also retrieved on the frontend directly, like the GitHub ones (stargazers, commit activity).\nHere are all our sources:\n\nnpm registry, example for express: http://registry.npmjs.org/express\nnpm download counts: the npm downloads endpoint\nPackages' dependents: the dependents endpoint of npm (there's no official documentation on that)\nChangelogs: a clever first resolved, first served list of calls to various ChAnGeloG files, like History.md's express changelog\nGitHub Stargazers⭐️, commit activity: we get them on the frontend directly from GitHub using the browser of the person doing a search. This way we benefit from a larger rate limit on GitHub shared amongst all users. This is also what npm search does for opened issues on their detail pages.\nBrowse view: we get this from the unpkg API, which gives us the files, folders and sizes for all published packages\n\nQuery Rules to the rescue\nThere are a lot of Algolia features baked in our JavaScript packages search index; you can see the whole index settings in our repo.\nOne of them that really helped us is Query Rules. When you are searching for a package, there are two questions to answer: the package that you exactly typed, and the package that you probably typed. We found that other searches often don’t have what you typed exactly early in the results, even though it exists.\nWhat we have as a solution is a query rule that applies when the user types the name of a package exactly or without special characters (to… allow people affordance in how they type dashes).\nExample query rule to boost exact matches\nThis allows a query like `reactnative` to have as first result `react-native` which is very popular, and as second result `reactnative`, which is deprecated and not supposed to be used, but still exactly what the user typed and may be looking for.\nFor a package search, we can't make any assumptions like \"Maybe the user was looking for this package instead of what they typed\". Instead we want to always present them both the exact package match if any and then the popular ones.\nThe future of Yarn search\nA big part of our success was made possible because we opened the JavaScript package search to multiple websites and applications (which is another milestone for us!), namely:\n\nYarn (65% of searches)\njsDelivr, the free Open Source CDN (10% of searches) serving one billion downloads per month of our libraries\nAtom autocomplete module import\n\nWe will soon open the JavaScript package search API to more websites and make it an official community project. The plan is to create a single page documentation for anyone to reuse this JavaScript search API in their applications. From editor plugins to community websites like CodeSandbox, we know the demand for an easy-to-use and instant package search is high.\nBuilding on that we want to add new features like:\n\nBundle size information like siddharthkp/bundlesize\nAdvanced filtering with tags\nAnything YOU would like to see, let us know\n\nWe did not stop at the search feature. I am proud to be a frequent contributor to the Yarn website, helping on adding translations, reviewing or merging PRs and updating the Yarn cli documentation.\nThanks\nThis project wouldn't have been feasible without the help of everyone from the Yarn and Algolia teams. Since our first contact with the Yarn team, communication has always been great and allowed everyone to feel confident about shipping new features to the Yarn website.\nWe also want to thank very much the… npm team for being responsive and advising us while we were building our replication mechanism. \nWe hope you enjoyed this article, see you soon for this year’s community gift 🚀\nThanks to Vincent and Ivana for their help while writing this article."
  },
  {
    "id": "6956-0",
    "title": "Building an App That Gives Music Advice Through a Conversation",
    "author": "Paul-Louis Nech",
    "text": "What would a search interface without a search box look like? After a few weeks of exploration, I created an app that gives music advice through a conversation, and called it “Musicologist”. This is the story of how I built it, hoping to give you an idea of what kinds of new search interfaces are coming to the fore and how you could start building them yourself.\nIntroduction: a dying search box\nIf you were a search box, you would worry about the recent press. From The Verge reporting that it will disappear in the next decade, to eBay saying it will at best “become redundant,” it’s a rough time for our good old search box. But is the end of the search box the end of search interfaces?\nIf you go beyond the headline, it’s a somewhat different story. The Verge’s article is referring to research by Susan Dumais, who said that “[search box] will be replaced by search functionality that is more ubiquitous, embedded and contextually sensitive”.\nSo the search box might disappear, but only to be replaced by a new generation of search interfaces: accessible everywhere, integrated in your activities, and aware of the context in which you are.\nConversational search interfaces are a first step in this direction. They are ubiquitous, as you can deploy them on any platform where your users reside. You can design them to be integrated in your user’s journey, providing value while interrupting them as little as possible. Finally, they let you leverage your user’s context by remembering previous conversations to adapt to them.\nBut what would a search interface without a search box look like? I spent a few weeks to explore what has been done in this field, and to propose an example of what kind of UX a new type of interface could provide: the “Musicologist”. This application gives you music advice through a conversation, and its code is now released open-source on Github. Here is a demo to show you the Musicologist in action:\n\nIn this demonstration, you see a… search interface built on three components: a mobile frontend, a conversational agent, and a backend powered by Algolia. Let’s see what each component does and how you can build it.\nThe agent: understanding your users\nWe built the conversational agent with Dialogflow (formerly api.ai), a conversational API that helps you build conversational experiences in natural language.\nAt its core, the agent is a middleware capable of turning an input sentence into an intent that can contain several parameters. It can then forward those to a web service, and turn its answer into a reply sentence. Intents describe what the user can say: each one is a different intention that the user can express in many ways.\nFor the Musicologist there are two main intents: search for songs and choose a result. Each one is defined by the sentences that a user could use to express it; for example, the search for songs intent:\n                                Two expressions that would show an intent to search for songs\nAs you can see in the screenshot, some words are highlighted. These are parameters of the intent, corresponding to different entities. Each entity describes a category of objects you could recognize. You can use the many system entities provided (phone numbers, city names, months of the year, etc), or you can define your own if none of the former fits your use case.\nTypo-tolerance and other limitations\nThere are, however, some limitations in the way entities are recognized. For example, there is almost no typo-tolerance in the entity recognition system. If you use the system in its default configuration, you can only recognize exact entities. Let’s define a project entity called “DocSearch”, and use it in an Intent. The agent recognizes it:\n\nBut if you do a typo, the agent is confused and does not recognize the entity anymore:\n\nThe agent didn’t recognize the DocSearch project\nThere is an option called Allow automated expansion, but unless you give a lot of… examples to your agent, it has good chances of inventing non-existent entities:\n\nWith automated expansion, the agent believes “Docsarch” is a valid project\nThis demonstrates the limitation of an entity recognition system that you cannot customize further. Like with handling typos, there are several techniques you would want to use when detecting entities, like removing superfluous terms or normalizing them.\nThankfully, you don’t have to implement these on your own. You can augment your agent by leveraging an API to give it more powers:\n\nHandling typos: you might think that with conversational interfaces, users won’t do typing mistakes as they are speaking to their devices. But being ubiquitous, your conversational interface can be deployed on various platforms, through which your users can talk (Amazon Alexa, Google Assistant, …) as well as write (Slack, Messenger, …). As your users will expect the same quality of results in both cases, customizable typo-tolerance is key to a great user experience across channels.\nIgnoring variants: be it via voice or text, your users might not speak exactly how you expect it. From plurals to declinations or missing diacritics, it is important to understand your user across all variants.\nRemoving optional words: especially via voice, your users might tell you more than what you expect, and phrase a query that’s too precise. But when you don’t find anything relevant to it, you will provide a better experience to your users by giving them results for a part of the query: being able to configure optional words lets you fine tune this experience.\nDefining advanced synonyms: you can define two-way synonyms in your agent, but this only helps if the two terms are strictly equivalent. This is not always the case: for example, you might want to show songs of the genre Rockabilly when queried about Rock, since the former is a subgenre of the latter. In this case, you don’t want a query with “Rockabilly Song” to return… any Rock song: you can solve this with one-way synonyms.\n\nThis is only a glimpse of what you can do: there are several relevance features you can use to improve how your conversational interface understands its users.\nThe backend: coming up with meaningful answers\nThe agent now gets what the user means (let’s say searching for songs) and the eventual entities in the intent (for example Led Zeppelin). But how can it provide a relevant answer to its users?\nFor the Musicologist, the backend is the agent’s intelligence engine: it will use Algolia to connect the intent with the relevant content taken from an index of songs. This gives some memory to the agent, giving it access to a great amount of data — as well as relevance, as the agent can now recognize an artist even if it is mistyped:\nThe agent now recognizes the correct entity, and can provide relevant results.\nThe backend’s code, built with Node.js, is actually quite simple: it takes the request from Dialogflow, extracts the search parameters and queries Algolia with these, then transforms the search results into the format expected by the agent.\nOnce your backend is deployed, you can start talking to your search interface! You can deploy it on various platforms in a few clicks:\n\nDeploying your agent to Slack, Messenger, or Alexa is only a few clicks away\nFor example, here’s what your agent could look like in Slack:\n\nOf course, you can customize each integration further. For example, you can use Messenger’s Quick Replies pattern to propose predefined answers to your agent’s responses. Still, this is quite powerful: your agent can be deployed on pretty much any platform without more work.\nYou now have an conversational search interface that can be accessed anywhere, and can already satisfy your users through voice-first interfaces or text-based conversational channels like Slack. This approach is optimal for low-friction interactions where you want to help your users find what they are looking for… without interrupting their current flow.\nAt other times however, your users will use your search interface to discover the content you can provide. It’s thus important to adapt your search interface accordingly, by helping them navigate and understand your results. In this case, a purely voice interface is not optimal: being able to ask your questions in natural language is great, but getting many search results as a spoken sentence make them hard to understand. If this is the only way to delve into your data, your users will likely miss the rich search results UI they have gotten used to with consumer-grade search on Google, Amazon, etc. It also makes it harder to interact further with the results: imagine filtering results on several criteria by talking! It’s a lot more difficult to think about such a sentence than to tap on a few filters.\nIt seems that for exploring your content, conversational interfaces and graphical search interfaces both have some advantages over the other. But why couldn’t your users have both?\nThe frontend to navigate your results\nTo provide our music aficionados with an interface more suited for discovery, we built an Android frontend to the Musicologist. This is the actual search interface where users will interact with the agent.\nThere are several advantages to having a mobile frontend for our Musicologist. First of all, it helps us leverage existing speech-to-text and text-to-speech systems. The application will use any speech recognition service installed on the device to turn the user’s voice into a written sentence, forward it to the agent, and then voice the response using the installed text to speech engine.\nBut having a mobile frontend brings more than voice-related components. It lets you adapt the user experience for content discovery by providing new ways to grasp the search results and interact with them.\nThe first area where this is helpful is understanding your search results. A voice interface puts a much higher… cognitive load on the user than a graphical interface, because they will need to pay attention to the voice interface as long as it speaks to avoid losing information. When you display the results, the user will read the information at the time they sees fit, rather than being forced to listen.\nMoreover, you can apply all the usual techniques that help the user get the most out of your results. You can display minor attributes that could be useful, highlight the results, snippet the relevant text, etc.\nThis can sound like a lot of work on top of the agent and backend, but it is very straightforward to build with the InstantSearch library - Algolia’s open source search patterns library providing widgets and helpers for building an instant-search experience on Android. It is built on top of Algolia’s Android API Client to provide you a high-level solution that abstracts the hard work of building such an interface.\n\nSearch results, visually highlighted and snippeted when relevant\nAs you can see, with a graphical interface it will be easier for your users to get the information they want. But that’s not all: you will also make it easier to interact with this information. A lot of the patterns your users learned to expect on great search interfaces still make sense in a voice-enabled application.\nTake infinite scrolling as an example: sure, you can ask the agent to show more songs, but now that you have a list of results, it feels quite natural to simply scroll to see more results.\nLikewise, you can tell the agent to play a given song, but sometimes a tap on the search result will be more adequate to play that song in your favorite music app. With hands full, it will be natural to voice a query, then ask the agent to select a result, while in another context you’ll find it quicker to tap on a search result.\nThe power of hybrid search interfaces\nSuch voice and graphical interfaces bring the best of both worlds: accessibility and ease of use via voice input/output,… but also rich information display and interaction possibilities.\nOn the one hand, you have the possibility to use the search interface hands-free, and you get the results you are looking for without having to touch your phone. You can even get your search results without looking at it!\nOn the other hand, you can interact with the results without using your voice again, for example if you already started listening to some music and just want to hear the next song. All the familiar interactions with search results can be leveraged in such a hybrid search interface.\nThe Musicologist is a proof of concept of such a hybrid interface, but many search interfaces could benefit from this approach. There are many use cases for which a hybrid interface would bring value to your users: cooking, driving, troubleshooting your car’s engine… whenever they already use their hands but would enjoy a screen. Those are just a few examples of situations where users would benefit both from hands-free control and a rich graphical user interface.\nWith the Musicologist, I wanted to give you an idea of what kinds of new search interfaces you could build. We hope this article will help you get started with building great voice search interfaces. If you build something with Algolia, please do let us know!"
  },
  {
    "id": "6926-0",
    "title": "Good API Documentation Is Not About Choosing the Right Tool",
    "author": "Maxime Locqueville",
    "text": "As a member of Algolia’s documentation team, I am often asked: “What tool are you using to build your documentation?” Of course, I answer the question, but I am often tempted to say that it’s probably the least valuable piece of information I can provide.\nIn this post, I am going to give you some of that information: what things you should care about when building your docs, and how those things will make the choice of tool the least important thing.\nDocumentation is usually composed of two big parts: the content and the software rendering it. You might have guessed where I am going with this: a quality README.md stored on GitHub can be far more efficient than over-engineered documentation that is well displayed but has issues with content.\nThe perfect tool is not out there\nThere are plenty of ways to build API documentation: static website generators (Jekyll, Hugo, Middleman), web frameworks (Ruby on Rails, Laravel, Django), dedicated doc tools (Docusaurus, Read the Docs, Sphinx), SaaS products (HelpDocs, Corilla), and that’s just the tip of the iceberg — there are so many more.\nDepending on the one you choose you’ll have more or less flexibility, and more or less work to build and maintain. All tools will let you decide to a certain extent what you can do, and constrain you on the other end. I don’t believe there is a tool that can fit 100% of the needs in the long term. Documentation is something that needs to evolve, and you may have to change your tools several times as you outgrow certain constraints and have new needs.\nTwo years ago, we moved away from an internal tool that was aggregating content from GitHub ReadMes, a database and an external software managing our FAQ. This change took us full two months, and this is not counting the months of preparation prior to making the change.\nBy far the most time consuming task was to undo the formatting that our original tools required us to make. We had no consistency — some were Markdown, some… were Markdown with custom extra features, some were plain HTML — and so while moving away from our previous tools, we had to edit thousands of files manually in order to unify everything.\nPut the focus on making the tool fit your content (not the other way around) \nInstead of focusing on which tool to use, a better option is to focus on whether you are doing everything possible to be as little software-dependent as possible. If you can respond to: “Can I switch easily to a new tool?” with a “Yes!”, then you are on the right track.\nBuild all components to be software-independent\nWhile developing custom components, it's good to keep in mind that they should be as little dependent on the software as possible.\nNow, to answer that question from the beginning of the article…It’s been two years since we’ve been using Middleman for our main documentation. It’s doing the job, but it has some downsides and we’ve had to customize it quite a bit to our needs.\nHere are some of the things that we added/modified:\n\nCustom sitemap \nCustom data files system\nCustom snippet generation\nInjections of custom metrics from Google Analytics for ordering purposes (for example the FAQ entries listed in https://www.algolia.com/doc/faq/ are the most viewed ones)\nAbility to have a snippet file that can be auto-injected into any content file\nAn Algolia indexer\n\nThese customizations are done in a way that we can reuse them in any project. The modifications represent ~800 lines of custom code, which is rather small for documentation like ours, but it enables us to be able to move data files to any other software in a matter of days rather than months; this is us adapting the tool to our content.\nKeep the content properly structured\nWhat is even more important is how you organize your content so that you can re-organize it programmatically when needed, or transform it so it fits another tool.\nThe more structured the information is, the easier it is to:\n- reuse it across different… parts of the documentation\n- change the organization system itself when needed\nTwo tips on that front:\n\n1. Keep the content centralized\n\nAs mentioned earlier, our documentation comes from a system that was split in many different parts. Today, we have documentation in a single repo that you can run independently from the main website. This removes dependencies and allows us to focus on content and doc-specific modifications. It also give us the ability to iterate more quickly both on content and code parts.\n\n2. Choose the right file format\n\nAlso very important is where you write your content. When using a static website generator, it is “the norm” to put all content inside the Markdown files. This can work for small docs, but when your documentation starts growing above hundreds of pages, with different types of content (guides, tutorials, reference, FAQ), and you are seeking consistency, using structured data files is a better approach.\nThat’s why at Algolia we documented all methods and parameters in yml files and not Markdown files. While we ended up with Markdown inside yml, which can seem a bit counter-intuitive, it is quite useful. It also allows you to reuse the content in different ways across the website.\nThe two pages above are generated from the same yml file. When editing, this makes it very easy to keep consistency between different parts of the website.\n\nSo by focusing in this way on content first - its needs, structure, maintainability - and then finding and customizing the tool, you can come up with a documentation that is easy and quick to evolve.\nOnce this is in place, a good next step is to have your team and customers contributing content. Which leads me to ...\nBonus tip: get more contributions from the team and your customers\n….and make those contributions as frictionless as possible.\nThere are a few actions we took to achieve this. When logged in as an admin, next to every section or code snippet we have an edit button that links to the… correct file in GitHub, enabling admins to modify the content he or she is currently viewing.\nLet’s take an example of a new developer joining the team. One of the first things they are going to do is learn about the product by working with it. The easiest route for that is using the documentation. While they are using it, they will notice typos, unclear bits, undocumented features…if they have to think about where to provide feedback, or how to edit the file, there is a high chance that they will do nothing and switch to another task.\nAnd if that’s true for your team, it’s even more true for customers. It is very unlikely that a user who noticed a typo will look around the website for the support email to tell you that they found something wrong.\nThis is one of the reasons we have the following big form at the bottom of every documentation page and also accessible from every section of the content:\n\nThe customer cannot miss it and that’s the point: if they see it once they know it’s there and the friction to contribute is low.\nThis also has the benefit of giving a great developer experience to our customers. When someone reports an issue on the doc, it goes straight to our regular support channel, where we have a good response time. We also fix and deploy a majority of the issues within the same day. A company that takes immediate actions on feedback gives an impression of care (one of our core values), and that’s exactly what we are aiming for.\nWhen someone in the team creates or updates a PR, the change will be rebuilt and deployed to a new domain. To achieve that, we use the continuous deployment feature of Netlify, which brings several benefits. The main one is ability to preview, not only for the person doing the PR, but also for the person reviewing it, because they don’t have to deal with running the doc locally for small changes.\nThis is just one example of how to reach out to your readers. It creates a virtuous cycle where everyone (team +… customers) contributes more and more, and enjoys doing it.\nIn short...\nThere are so many things to consider before worrying about which tool to use. Naturally, you do have to start somewhere and choose a tool, so I advise you to choose the one you and your team are most comfortable with. Just keep in mind to focus on the content, and adapt the tool to the content, not vice versa.\nWe’d love to hear your feedback and other experiences with this topic: @maxiloc, @algolia."
  },
  {
    "id": "6899-0",
    "title": "Announcing Multi Cluster Management:  SaaS Search Built to Scale",
    "author": "Xavier Grand",
    "text": "Today we’re announcing the release of Multi Cluster Management, a feature dedicated to improving our fellow SaaS customers’ lives when it comes to search.\nWhile our search engine works for every industry, and our features — from fast indexing to typo-tolerance — work out of the box for every vertical, there are some needs that are particular to certain verticals.\nBeing a SaaS company ourselves, we share some of the pain points of our SaaS customers, and developed the Multi Cluster Management feature to help address some of these challenges:\n\nRapidly scaling to large data sizes — several TBs and growing for our current biggest SaaS users\nManaging infrastructure costs and gross margin contributions to ensure business success of SaaS applications.\nManaging and tracking multiple TBs of data across multiple Algolia clusters without additional work required for load balancing and splitting at the user level\nMatching and migrating individual users to the right cluster based on capacity needs\nEnhanced features and service offerings for users depending on their plan or tier level\n\nHow the problem of scale is addressed today\nRegardless of your search provider, when the total size of your indices reaches a certain size — typically a few hundred GBs — you’ll need to scale your infrastructure beyond the first cluster or server with which you initially started. For the sake of argument, let’s assume that you reach this limit when you reach 10k users.\nSo while the first 10k users were handled smoothly, scaling beyond that will add a great deal of pain and complexity into your search infrastructure. At this point, you’ll need to redesign your search solution to spread your users into more servers and manage all the additional complexity that comes along with it.\nOn Algolia (without Multi Cluster Management)\nAt Algolia, a single search instance consists of a triple-redundant cluster of servers that are operating in sync to provide the fastest and most reliable… search experience possible for your users. Without Multi Cluster Management, if your 10k customers no longer fit on a single cluster, you will need to create a new Algolia application on a second cluster, and manually split your users to balance the load: 5k users on the first cluster, 5k on the other one.\nThis “solution” doubles the complexity of your initial search application as you will now need to manage two Algolia applications — with two APP_IDs, two sets of API keys — in addition to tracking which user is on which application.\nThis is where the pain merely begins. When you reach 20k users, both clusters will be full. You’ll need to set up a third cluster, and re-partition your users manually, and again at 30k users, and at 40k users...a painful process indeed!\nOn other engines (like ElasticSearch)\nOn other engines, you would rely on the cross-server sharding mechanism. Your users would be automatically balanced across the different servers, but this balancing comes at a cost.\nFirst, each search query requires a full search across each server (or shard), which increases search latency and lowers users’ perception of your search engine’s performance.\nFurthermore, since the users are automatically sharded across servers, you don’t have the ability to control which user is assigned to which servers, so you can’t make sure that a given user will be allocated the right amount of resources.\nAdd in the fact that as you scale (either up or down) and the number of shards changes, you will need to reindex all of your documents, potentially taking search offline until the indexing is completed or doubling your infrastructure and associated costs in order to avoid this downtime. So once again, a painful process!\nSolving for scale with Multi Cluster Management\nMulti Cluster Management is designed to solve the pain of scaling for SaaS customers on Algolia by making it as easy to manage two, three, four or 100 clusters as it is to manage one cluster. For… starters, this means that all clusters will share one application; with one APP_ID, and set of API keys.\nLet’s return to our example above, but this time with 100k users and cluster sizes of 10k users per cluster. To manage this setup with Multi Cluster Management, we would always use the same application ID to access our set of 10 clusters. Assigning users is as simple as making an API call to the application and telling it which user belongs on which cluster. No longer is it necessary to track each user’s location or specific application ID.\nOnce users are assigned, the cluster management becomes abstracted: if you add, update, or delete records for a specific user, there is no need to remember which cluster or application that user is on as the request will be automatically routed to the appropriate cluster. For users, every query entered is always routed to the right cluster with no increase to latency, preserving the instantaneous search experience that they have come to expect.\nAnd finally, let’s say you need to migrate a user to a new cluster for any number of reasons—rapid growth, upgrading to a new tier of service or moving data to a different geographic region. Doing this is now a straightforward, two-step process, with no user facing downtime introduced by this migration:\n\n1. Order a new cluster\n2. Make an API call to assign this user to the new cluster\n\nThat’s it! The user will be automatically migrated with no downtime, no manual operation, and most importantly, no complex migration process.\nThe inverse is also true: adding a cluster or removing a cluster is as straightforward as sending the necessary API calls to migrate users.\nHow this helps SaaS companies\nWith Multi Cluster Management, we have started addressing many key considerations that our SaaS customers experience as their businesses scale:\n\n1. Search performance\n\nYou know how obsessed we are with performance. For that reason, we wanted to avoid having to search across multiple… servers when performing a search. With this design, each user is hosted on a single cluster, so a single server can answer queries for a given user. Not having to merge results from different servers allows us to keep the same performance Algolia is known for.\n\n2. Adding and removing clusters\n\nAdding or removing a cluster doesn’t require a complex migration strategy. A few API calls and your new setup is live, with no downtime in the process.\n\n3. Flexibility\n\nYou have a complete control of mapping users to specific clusters, so you can assign users as your needs dictate; for example, you may want to host a specific user on a dedicated cluster, on a shared cluster in a specific region, or even on a dedicated cluster in a specific region.\n\n4. Resource control\n\nBy controlling to which clusters users are allocated, you are able to manage your infrastructure to achieve the most effective and efficient outcome based on your needs.\nTechnical choices we made\nIn this section, we’ll dive deeper into the feature by investigating some of the technical choices we made when creating Multi Cluster Management (MCM). In particular, we want to look at four different topics:\n\n1. Why search on only one server at a time\n2. Why we made user-to-cluster mapping configurable\n3. Why not use clusters with more servers?\n4. What use cases are not covered by Multi Cluster Management?\n\nThrough these four topics, we hope to explain our reasoning in developing this feature, give our users an in-depth explanation of the feature’s capabilities, and finally explain how to optimize its use.\nWhy search on only one server at a time\nThis is probably the most defining design choice of MCM: every search happens on a single server.\nWhen facing a situation where the size of the data is bigger than one server, most search engines decide to shard the data across a cluster with multiple servers using a hash-based mapping: this means that, when searching, you need to query each server, retrieve the matching… results, merge the results, and finally provide the answer to the user. As you can probably guess, this introduces a performance bottleneck at the network level between the servers themselves. Requiring multiple servers to communicate lists of results to be able to merge them is slow.\nOne of our main objectives at Algolia is to make search fast. To achieve this with Multi Cluster Management, we avoid communication between different servers of a cluster by storing 100% of the required data on a single server. Fortunately, this is easily achievable for SaaS use cases where the data is easily partitioned on user-by-user basis.\nOne good property of data partitioned by user is the fact you can create small, autonomous slices of data, where each slice of data fits within the storage limitations of a single server. This ensures that each user will fit within a single, Algolia cluster. And because an Algolia cluster is defined as three fully-redundant servers, we need to query only one of the servers within the cluster to get the full list of results - eliminating the network bottleneck discussed above.\nWhy we made user-to-cluster mapping configurable\nOur initial intuition was to automate the assignment of users to clusters, so that the first time you send an object related to a new user, this new user would be automatically assigned to a cluster. However, we found a few use cases which required additional flexibility:\nMulti-region indexing\nLet’s say that ⅔ of your users are in the US, and ⅓ in Europe. Ideally, you would like to have your servers located as close as possible to your users. By making user to cluster mapping fully configurable, it is now within your control to assign users to whichever cluster makes sense to ensure optimal performance for their given regions.\nDifferent tiers of service\nIf you are a SaaS service, chances are you provide different levels of plans. In that case, you may want to provide a different quality of search to different users. For… example, you may want to provide advanced search features such as Query Rules to your premium plans.\nOr you may even want to make sure that your most important prospects don’t face an issue during the trial or testing phase that could jeopardize their perception of your product, by placing them on clusters with greater search or indexing resources.\nFor these reasons, we decided to keep the mapping of users to clusters configurable: the first time you send us a userID, you need to decide in which cluster this userID will be hosted.\nAnd to make this smooth, we provide two handy tools:\n\n1. A migration tool: re-assigning users is as easy as an API call\n2. An overview of each cluster’s status: we expose an API giving you an overview of how full the different clusters are, and how much capacity your largest users are consuming on each cluster.\n\nIn the future we may provide different strategies to further simplify user assignment. An example of this would be to introduce multiple mapping modes:\n\nManual: you need to provide us the userIDcluster mapping\nRandom: the users are assigned randomly to a cluster in the pool\nEmpty-First: the users are automatically assigned to the cluster with the greatest unused capacity\n\nWhy not use clusters with more servers?\nAlgolia’s infrastructure is built from the ground up to ensure reliability. For that reason, every single search cluster consists of three fully redundant servers, so that if any two servers go down, the end-user’s search availability and performance will remain unaffected.\nWhen designing MCM, one of our options was to add more servers to these clusters. For instance, we discussed having clusters of four, five, or 15 servers, where each of these servers would have hosted only a subset of the overall data.\nInstead, we decided to trust our initial design choice of three servers per cluster, and then link these clusters together for the following reasons:\nReliability\nHaving each cluster composed of three servers allows… us to build a highly available infrastructure where the three servers are hosted within different data centers. This allows us to continue offering a 99.99% SLA by default (and 99.999% SLA as a premium option) for our customers, even when multiple clusters are in use.\nEasier to manage\nFor cluster size, complexity grows with scale. The bigger a cluster gets, the more issues it creates. For example, a cluster of 15 servers has 5 times more chances of having any one server go down. Therefore, using five clusters of three servers each and keeping these clusters independent from one another removes an unnecessary risk created by large cluster sizes.\nWhat use cases are not covered by Multi Cluster Management?\nAs you may have understood by now, one of the core design choices for Multi Clusters Management is the idea that a search will happen on only one cluster at a time.\nThis choice means that there are use cases for which MCM is not applicable. As long as the size of the dataset into which you’re searching at a given time fits into one cluster, MCM will work perfectly. But if the size of that single dataset outgrows a single cluster’s capacity, and that dataset is not partitionable, then MCM will not work.\nFor example, let’s say that you want to create an index of all the known objects in the universe (such as stars and planets). This index may end up representing a few TB of index size. If your plan is to enable search across all of the planets and stars at once, then you’ll need to search into your entire dataset. However, this dataset doesn’t fit onto a single cluster, so in order to run this search query you would need to be able to search simultaneously across clusters. This use case, however, is incompatible with this design, as queries are always routed to a single server to be answered.\nAn example of where MCM would be a fit for searching all the known objects in the universe, however, would be if searches are always done on a solar system by solar… system basis. Assuming that no single solar system consists of a data set too large for a single cluster, then MCM will route each search request to the cluster on which that data is hosted and return results.\nThis design choice is what makes Multi Cluster Management particularly adept for SaaS use cases. For example, for CRM companies, the user data is easily partitioned by end user who is paying for the service. From our experience in SaaS, the number of use-cases where a single customer doesn’t fit in an entire server are very rare (we haven’t encountered the case yet).\nBuilding on the tradition of Algolia’s infrastructure solutions\nAt Algolia, we always pay special attention to problems of scale. A few years ago, we launched DSN to give customers the ability to provide great search to users anywhere in the world. We then started to spread our clusters to different data centers to be able to rely on multiple provider for servers, electricity, networks... Multi Cluster Management is a unique  — and we hope — effective solution to a set of problems that scale presents to SaaS companies.\nWe look forward to hearing your feedback: @algolia, or comment below!"
  },
  {
    "id": "6854-0",
    "title": "Introducing “We Rate Tweets” — a Glitch App for Searching Your Twitter Timeline",
    "author": "Jessica West",
    "text": "Glitch, the latest project by creators of Trello, FogBugz and co-creators of Stack Overflow, is a new platform for building web applications that features online code editing and instant deployment. In this post, you’ll learn how to use Glitch to host and remix an Algolia-powered search for your tweets.\n\nWant to try it out right now? Try searching through Algolia's most popular tweets or read the documentation to learn how to create and customize your own copy.\nWhat is Glitch?\nGlitch was created out of a desire to make code more accessible to people across the board and give companies that make developer tools a fast route for users to start building on top of their product.\n“A key goal of Glitch is to make development more accessible. We’re building Glitch as much for new starters, and those with just a little or rusty programming knowledge, as we are for full-time, professional developers.”- Gareth Wilson\nThe thing we dig about Glitch is the power it puts into the user’s hand to get going fast on a variety of projects ranging from a sample API call to a robust web application or blog. By allowing users to raise their hand when they need help, Glitch is making it easier for new coders to get up and running. It is helping knit communities together with building blocks for coding.\nThere is a plethora of getting-started applications that allow you to do something called Remix, which will create a copy of the codebase on your own profile to work with. This allows you to take your idea even further after some of the foundation has been laid for you.\n\nJenn Schiffer, community engineer at Glitch, affectionately talks about the platform as\n\"the friendly community where you'll build the app of your dreams\"\nAlgolia is one of the early adopters of Glitch alongside Twilio, Facebook, Trello, and the Slack API. Visit Algolia's home on Glitch - glitch.com/algolia - to see more apps that use the API and learn more about search.\nIntroducing We Rate Tweets\nInspired… by one of DevRel team’s favorite Twitter accounts, Dog Rates, we created We Rate Tweets to scratch our own itch of wanting a faster tweet search with a way to classify and rate our tweets by their popularity. Ratings are on a scale of 1-10, or more than 10 in special circumstances. 😉\n\nOnce you have connected your Twitter account, you can start searching your own timeline. We will have already indexed your tweets and assigned a rating score with each one. That way, rather than seeing your search results in a chronological order, you see your most retweeted and liked tweets first.\nIf you recognize that you talk a lot about a subject - selfies with your dog, live-tweeting at a conference, hitting up some sweet yoga class or drinking wine while doing any of those — you may want to search on keywords you use in tweets, like dogs, JavaScript, wine, or yoga. From there you can see your tweets that match those words, and even share those exact search results via the page’s updated URL.\nJust using the app doesn’t require any coding skills on your part. Just login with your Twitter account and you'll be able to search back through your last few hundred tweets.\nRemix to get your own live copy\nWant to index even more tweets and index tweets of other users? You can do this very easily with the “Remix” feature on Glitch. This will create a copy of We Rate Tweets onto your Glitch profile. From there, you can:\n\nCustomize the look and feel, including the emojis used for rating\nAdd more calls to the Twitter API to get a larger history; the world is your oyster! \n\nYou’ll need a Twitter account and an Algolia account first. Instructions for creating those and everything else are in the README on Glitch.\nWe love hearing from you ?\nReach out to us on Twitter and tell us what you think, or join our online community on Discourse and share what you've done with your remix. Happy searching and Glitch-ing!"
  },
  {
    "id": "6826-0",
    "title": "We Built an Atom Plugin to Find and Install Any NPM Module",
    "author": "Gianluca Bargelli",
    "text": "The desire to automate everything is pretty common, especially with developers: why should I waste my time doing the same operation countless times?\nThat desire is at the base of the process that led to the making of our Atom plugin: an autocomplete that allows importing packages from NPM quickly, with a few keystrokes. On top of this, the plugin will prompt the user to save the selected package as a project dependency if a `package.json` is present.\nWe published the extension on the Atom Package Registry and open-sourced the code on GitHub, and here is the story about how we built it.\nThe problem we sought to solve\nMany of us use JavaScript and Node daily for a variety of different tasks: scraping data, slackbots, contacting APIs, indexing content... you name it. Luckily, there are plenty of open source libraries on NPM which we can leverage to perform any task in the simplest and fastest way.\nWe really wanted to solve two main productivity issues in the process of building tools:\n\nLooking up packages on NPM or Yarn, either via CLI or on the web\nInstalling the package in our `node_modules` folder (normally via CLI)\n\nLet’s make a contrived example: let’s say we’re writing a small terminal script which needs to add padding to a string, and we don’t want to re-implement this functionality from scratch.\nWe would normally start by researching the library on NPM or Yarn’s Website, querying for “pad” and evaluating each library by looking at the number of downloads and GitHub stars, and checking if it’s maintained or not.\n\nAfter selecting our library, we need to explicitly require it in our project dependencies using the command line:\n\n \n \n \nFinally, we can get back to our text editor to actually require the package:\n\n \n \n \n \n \nThe number of required steps in order to install a simple library is definitely high and requires to hop from your text editor of choice to a browser and to the terminal - all that for just one library.\nContext… Matters\nWhile installing a one-shot library is not a problem, doing this operation frequently can lead to a frustrating experience. We didn’t want to waste time researching libraries or running the same instructions again and again. We rather wanted to spend our time focusing on solving our functional problems and creating product features.\nOne of the first pain points we wanted to solve with our extension was to reduce the need to switch to a browser in order to search for a library. On top of that, we wanted to contextually provide useful information for exploring different options.\n\nIn order to provide a selection of the most relevant packages, we’re using an Algolia index with custom ranking which takes into account the number of downloads from NPM in the last 30 days, in conjunction with a Boolean attribute `popular` computed at indexing time, which sorts the most popular results to appear first.\n\nIn short, having contextual help (in this case, importing JS modules) is tremendously helpful compared to using 3-4 separate tools.\nLet me do that for you\nAfter researching and finding the best suited package for our task, we still need to install it locally on our laptops and track in our project that we’re using that dependency.\nNormally, this is automatically handled by NPM or Yarn by using their CLI; that said, the Atom extension will detect if the package is missing and will prompt the user to save it:\nThe different options come down to different types of dependencies handled by NPM and Yarn, `dependencies` and `devDependencies` being the most used ones for developing a project.\nProof of concept in the time it takes to brew a double shot\nThe idea for Algolia Atom autocomplete was born in front of a coffee machine on a rainy Parisian morning. My colleague Raymond (the author of Vue InstantSearch, amongst other things) and I discussed how we could leverage Algolia to build something that may prove useful to other developers.\nWe remembered that Haroen, another… fellow Algolian, built a really nice search UI for Yarn:\n\n🎉 https://t.co/BdkV4mw8Vg 🎉 check out the new search available everywhere pic.twitter.com/IvorEaCzN1\n— Haroen (@haroenv) February 25, 2017\n\nAs this implementation is using indexed packages from NPM, most of the job was already done: we just needed to display results where it made sense the most. Chatting by the steaming cup of coffee for 10 minutes, we figured out that we could use a package autocomplete inside a text editor to automatically require packages, the lazy way.\nAfter moving the project to our desks, we choose to go with Atom because its simple JavaScript API layer allowed us to go very fast: from the concept to the first working prototype it took less than a couple hours (also thanks to Ray’s superhuman developer skills). The idea of also saving the dependency came the day after, in a pair-programming session on our lovely couch.\nSince it was a fun hack project we didn’t do any promotion of it at the time. Until today, the only proof that we actually released it was this rather embarrassing tweet.\nLessons learned\nBuilding something quick but useful that could also be useful to other developers feels great. I had a fun time building this project, and I also had an insight or two in the process:\n\nValidate your ideas with the people around you as soon as possible. If the people around your are non-technical, try to see if you can explain the problem clearly without them fainting from exhaustion.\nRe-use existing tools and existing knowledge as much as possible. This will help remove the noise and focus on the actual problem to solve.\nAs soon as you have defined the problem and have a rough idea how to solve it, don’t wait any longer, just do it!\n\nSo long and thanks for all the fish\nIf there’s enough interest around our Atom Plugin, we will consider porting it to other text editors (DEVisual Studio Code, Sublime Text and all the rest), and work a little bit more on cross-platform… support (Windows, I’m looking at you).\nIf you have any feedback about this project or want to share something you built using Algolia feel free to leave a comment here, join us on Discourse or ping @algolia, @proudlygeek & @rayrutjes on Twitter."
  },
  {
    "id": "6802-0",
    "title": "Serving One Billion JavaScript Library Downloads",
    "author": "Vincent Voyer",
    "text": "This month, with the help of jsDelivr, we reached a milestone 1 billion downloads (that’s 26TB!) across all of our libraries. Since we now know how to deliver that many JavaScript libraries in a fast, robust and stable way, we thought we'd share our experience with the JavaScript community.\nAlgolia is developing multiple open source projects (like InstantSearch.js) to simplify the integration process of our search engine. When a developer wants to include one of our JavaScript libraries, she can use either:\n\nThe npm package published on the npm registry\nA JavaScript file hosted on a CDN, to be used in a `` tag\n\nProviding those two ways allows us to target different use cases:\n\nAdvanced JavaScript users familiar with module bundlers like webpack\nBeginner or intermediate JavaScript users used to copy/paste `` tags in their html\n\nOur goal with this strategy is to reduce the friction of using our service at every level of the developer experience.\nPublishing on npm is rather easy: you just run `npm publish` in your directory and that's it.\nCDN delivery of JavaScript libraries\nAs for CDN delivery of our JavaScript libraries, an obvious option would be to find a commercial CDN provider and write a deployment script to automatically upload new versions to the CDN.\nThis also means someone would need to maintain this new system, configure the CDN, update the scripts, monitor uptime (if it fails, our users’ websites are down) and everything that comes with maintaining a live production system. Early on we decided that our main business was search and not delivering JavaScript libraries to the world, so we searched for solutions already providing this.\nThat's when we found out about jsDelivr.\nDelivering your libraries with jsDelivr\njsDelivr is a free CDN built exactly for this. It offers production quality of service and natively integrates with npm.\nOur deployment process looks like this:\n\nAs you can see, the only manual action is to run `npm publish`. After that, the npm… registry and jsDelivr handles everything.\nSince the beginning, Algolia and jsDelivr have been in close relation. jsDelivr offers Algolia a great way to easily deliver JavaScript libraries to the world. And jsDelivr uses Algolia to provide JavaScript search on their website.\nDive into jsDelivr\njsDelivr was launched in 2012 and is currently developed and maintained by Prospect One, a company opened by the original founders of the project. Since the start, it was fully open source and focused on developers, including the development of an API for all public CDNs with the help of the community.\njsDelivr has a unique-among-public-CDNs infrastructure: it uses multiple CDN providers and load-balances between them based on performance and uptime, instead of using a single CDN provider.\nAt the moment, jsDelivr uses Cloudflare, Fastly and Stackpath for all global file delivery except for mainland China, which is exclusively served by Quantil, a company that has a large network of servers across all major Chinese cities. This is made possible by the ICP license held by jsDelivr.\nIn total, jsDelivr serves about 19 billion requests every month. That’s almost 500TB of bandwidth of small ~3kb js files — all that from 176 locations all around the world.\nTo see more detail on how jsDelivr works under the hood, check the infographic below that visually explains all the failover systems integrated into it.\n\nPro tip: websites do not need auto updates\njsDelivr has some great features to automatically provide updated version of packages to consumers. For example, you can use scripts like:\n\nhttps://cdn.jsdelivr.net/npm/instantsearch.js@latest would be always updated with any latest version\nhttps://cdn.jsdelivr.net/npm/instantsearch.js@2 would always be updated with the last minor version of instantsearch.js v2 (following semver)\n\nThis sounded very cool for early Algolia: we could update our client websites at any moment with auto updating, and we could advertise that they would be… always up to date. Alas, WRONG!\nIn fact, this turned out to have many disadvantages:\n\nExtra consciousness about any line of code added to our libraries\nNo way (when using /latest/) to make any substantial permanent breaking changes. For example, to release a completely revamped, modernized JavaScript client with a clear upgrade path was not feasible if people used /latest/: their website would auto update even before they could make changes to support new versions\n\nUltimately, when your website works and has no issues on Monday, there's no need to push an update on Tuesday from our side. When there are new features and fixes you might want, we will communicate with you via our community forum or change log.\nNowadays, for production systems, we recommend using fixed versions (like https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.0), and being aware of new updates when you need them.\nStill, auto updating packages has their advantages. When creating jsFiddle or CodeSandbox examples for our users to show us bugs to be fixed, we use auto updating so that we never have to manually updates those templates.\nThe future of JavaScript delivery\nJavaScript modules are a great addition to standardize the modularity aspect of JavaScript apps. If you want to learn more about them, read the modules chapter of Exploring ES6 book by Axel Rauschmayer.\nThis feature is now coming to Node.js and browsers, which means there will be adjustments to make to both our build and delivery strategy, so we can support new usages like ``.\nWe trust jsDelivr to help us make the right decisions and potentially build new features for JavaScript modules.\nBig thanks to Dmitriy Akulov for his help with this post. "
  },
  {
    "id": "6774-0",
    "title": "From Windsurfing to Venue Search: Great Search Implementations",
    "author": "jason harris",
    "text": "Want to show off what you’ve built with Algolia? The Show and Tell category on our Community Forum is a great place to gain visibility for your project and get feedback at the same time. Our 1,100 community members actively swap stories of success and overcoming obstacles on their projects.\nWe ? our community and enjoy seeing the many ways developers are building with Algolia, including the great search experiences built within large enterprises, small business, and those midnight-oil burning side projects that include massive amounts of creativity and originality.\nToday, we’ll highlight recent posts to the Show and Tell category so you can see some great ways to implement search.\nDigging into PHP's #internal mailing list\nCommunity member Mattieu Napoli built externals.io as a way to give PHP's #internal mailing list a new level of accessibility. The site uses an Algolia InstantSearch to afford the user threaded views, search and up/down voting to enhance the UX. Externals.io also features highlighting and inline results in a responsive manner.\nSee Externals.io on ProductHunt and Algolia's Community Forum.\n\nFinding the right windsurfing equipment\nBuilt by Spanish developer Kikobeats, windtoday.co is a marketplace to find deals on windsurfing equipment. Windtoday.co enables visitors to find windsurfing goods based on category, brand, condition, and more, using great features like type-ahead search and facets. Also, for those on mobile, Kiko recently introduced a mobile version of windtoday.co using react-instantsearch.\n\n \nPowering philanthropic giving with Grantmakers.io\nGrantmakers.io is a site intended to provide live search, summary data and rich profiles for 71,042 US-based foundations. Developer Chad Kruse built Grantmakers.io with Algolia, Jekyll and GitHub Pages in this great example of a search implementation. Even better, Chad has open sourced everything on GitHub for others to learn, hack and build upon.\n\nUniting blog content and e-commerce sales… through search\nOften times it’s hard for companies to find a way to tie content marketing and ecommerce sales together. With AskFrannie (an essential oils website), developer Rolondo Garcia built a custom WordPress plug-in that uses product search at the bottom of articles, and invites the visitor to click to an online catalog of products. This method is great because it matches reader intent with relevant products in a casual way. That is, if a reader has read an entire article on, say, essential oils, they’re more likely to have a desire to see products presented in a relevant plug-in that a banner ad on the site.\n\nFind event space at hotels, restaurants and activity spaces\nAsk any event manager and they’ll tell you that finding the right event space at the right location can be very time consuming. Bizly.com aims to solve the problem by enabling a curated venue search in major cities across the United States. Powered by optimized SQL queries, Laravel Scout and Algolia, Biz.ly’s search is lightning-fast and very rich.\nLet the community see what you’ve built\nFeeling inspired by these Show and Tell examples? Head on over and share your post. An ideal post describes how the search works, the Algolia libraries you're using, and other parts of the stack including languages, frameworks and APIs. The community also loves screenshots and a link to your project. We look forward to seeing what you've built!"
  },
  {
    "id": "6747-0",
    "title": "Enhance & Promote Results: Introducing Query Rules",
    "author": "Nicolas Baissas",
    "text": "Today, we are releasing Query Rules, a new feature which enables you to modify, override, & enhance the behavior of the engine’s configured ranking for a subset of the queries. We wanted to share with you how we approached this key addition to our API, why we decided to build it, and explain the design steps leading to the release today.\nBefore we dive in, let’s look at a few examples of what you can do with Query Rules:\n\nTransform descriptive text into a filter: for example, by automatically transforming a query word into its equivalent filter (“cheap” would become a filter price< 400; “red” a filter on the color…)\nMerchandising: manually select a result for a specific query (e.g., decide that the latest iPhone 8 should be in the first position for the query iPhone)\nEntities detection: detect entities (colors, brands…) in the queries and transform them into filters or boosts\n\nBuilding upon our ranking formula\nWith Algolia, the same relevance algorithm is applied across the entire index. We expose features like Custom Ranking so that customers can customize the ranking strategy for their needs, and achieve a great relevance for the vast majority of queries. However, in the past few years, our customers started to bring to us examples of outlier queries.\nWe began compiling a list of these outlier situations. Here are a few examples:\n\nWhen Apple releases a new iPhone, it should be in the first position of the list for the query “iPhone” (even though at this stage it has no views, no sales… no “logical” reason to be ranked high)\nYou need to get rid of a big stock of a specific model of a vacuum cleaner, so you’d like it to be promoted in the results (at least until enough units are sold)\nThe query “phone” naturally (according to the configured ranking strategy) retrieves feature phones instead of smartphones, but you’d like to highlight smartphones first\nThe query “red dress” matches with non-red results because the name of the… brand begins by “red”\nThe query “cheap phone” doesn’t return any results (the records don’t contain the word cheap), and would behave better if “cheap” was transformed into a numeric filter on the price\nQueries originating from users on mobile devices should highlight results that are mobile-friendly\n\nThere are hundreds of examples like this where having an exception to the general rule would make sense, either to improve the relevance, or to override the ranking for business reasons.\nThinking about our options\nThere are two main ways to address the types of exceptions we were seeing. The natural way to handle this would be to analyze the use cases one by one and add a configuration to the engine to handle each of them individually. We could, for example, develop a form of synonyms that would transform a word into a filter. Eventually, these settings would form a merchandising tool, allowing users to tweak and override the ranking logic.\nWe certainly had the experience on the team to execute on this approach. Several team members, including our founders, have used, or even built merchandising platforms prior to founding/working for Algolia. However, it is exactly because of this experience that we had doubts that this was the right approach:\n\nCreating a new setting inside the engine for each exception quickly leads to a bloated solution: there are just too many varieties of merchandising strategies\nBuilding a quality merchandising tool that would work for all of our users - each of whom have specific needs & exceptions - is virtually impossible\nMultiplying the exceptions inevitably affects performance\n\nMore importantly, we wanted to do more than merchandising and address the needs of other industries. Media sites don’t use merchandising, but they still want to promote, for example, partner content. SaaS systems may want to improve the ranking by adding rules automatically based on the output of a machine learning tool.\nTo do this, we would need to… be able to impact search results on a subset of queries in two distinct places:\n\nBefore the search query is processed, in order to override the query parameters\nAfter the results are found, in order to modify the results list\n\nThe solution\nWhat we came up with is a rules system for queries — or Query Rules — that sits inside the engine via two modules:\n\nA query preprocessing module that will modify the search parameters before the search is processed by the engine\nA results postprocessing module that will modify the results before they are sent back to our users\n\nEach rule has the following pattern: IF the query contains X, THEN modify Y. A condition and a consequence:\n\nIf the query contains “iPhone”, add this result in the first position\nIf the query is “cheap phone”, replace the word “cheap” by a numeric filter price… to a million rules to an index, with little to no impact on search performance\n\nIn fact, we like to think of Query Rules as more than a feature: we like to think of it as an extension of our engine. We have designed it to be open-ended enough to allow our users to solve their own unique problems and push the boundaries of what is possible with search.\nIt’s been exciting to start sharing the feature with a few beta testers — we’ve been amazed at how easily they grasped its potential, and the combinations it allows.\nThe two conditions and seven consequences we are initially releasing are only the beginning — we look forward to getting your feedback and learning which ones you’d like us to add next!\n\nMeanwhile, we put together an online workshop, where two of our team members will show you in practice how to create smarter search with Query Rules.\n=&gt; You can register here."
  },
  {
    "id": "6728-0",
    "title": "6 Tips to Make the Most of a Hack Day",
    "author": "Marie-Laure Thuret",
    "text": "When I joined Algolia, the InstantSearch team was composed of two developers. Ten months later, we doubled the team and will have two more engineers joining us soon.\nWe are now focusing on several big projects at the same time. Even though we are a single team, we are inherently split into different projects. This means that not everyone is interacting with one another on a regular basis.\nIn addition, we don’t always have enough time to step back and try new things. As we were thinking of a way to solve those issues, the hack day retreat was born. The concept is simple: 6 team members, 5 hacks, 1 place outside the Paris office, and then non-coding team building activities to round off the day.\nHere are some tips you might find useful if you want to organize a similar hack day.\n1. Prepare the hacks upfront\nTo prepare for this special day, we were asked to think about several hack ideas we wanted to realize upfront. The idea was to choose the one we wanted to work on the day before. This is a mandatory step because you don't want to spend your time looking for an idea on D-day. You’ll only have something like six hours dedicated to coding — use them wisely.\nYou should also make sure that everyone spent some time to do pre-research and pre-work around the hack, particularly if they know that they will manipulate things they’re not very familiar with.\nFor example, one of the hacks involved virtual reality throughout React VR plugged with a speech-to-text feature. Prior to hack day, we bootstrapped a blank app and made sure that those blocks were correctly connected to each other. When talking to the computer through the browser, the VR part would receive the corresponding text. Without doing that in advance, we could have spent half of the day configuring it.\nA hack implies a lot of unknowns, so you really don’t want to get stuck with configuration issues.\n2. Have a hack day outside of your office, in a nice place\nAs you're going to spend a full day coding,… it's important to find a nice place where everybody can feel great and enjoy themselves. Being outside of the office really helped switch our brains off the regular daily routine and be totally focused on the hacks.\nWe manage to find a nice loft in Paris’s 11th arrondissement, full of commodities for everyone:\n\n3. Take the opportunity to empower junior developers\nIf you happen to have a junior profile in your team (we recently onboarded one), a hack day is definitely an opportunity to learn a lot, albeit with some slight modifications. Here a few tips to make the most of it:\n* Never assign a junior person a solo project. It’s the best way to make them feel excluded and not integrate with the team.\n* Choose a simpler project that will cover some fundamentals related to the team's day-to-day job (for instance, how to build a simple application with React).\n* Put the emphasis on pair-programming and sharing knowledge (it is the perfect time to learn some workflow or debugging tricks!)\n4. Focus on making an MVP\nFocus on making a Minimal Viable Product rather than the best product possible. It's way more rewarding to present something that works during the demo than a vision of something that may never happen. Try to add things step by step without thinking too far ahead.\nFor instance, in one of the projects, a spreadsheet was used: you don't have to think about how it can scale and spend time researching the best tool to handle infinite scroll. Instead, you can allocate this time trying to go deeper in the project's features. Here as always, setting priorities is key to effective time management!\n5. Demonstrate your hacks\nDon't end the coding time abruptly. Instead, plan a demo time where everybody can showcase what they did. If the team can manage to push the demos on a repository afterwards, that's even better, as you'll be able to share the content with everyone interested.\nHere are the hacks we realized on our hack day:\n\nAMP + Algolia\n\nAccelerated Mobile Page is… a page format proposed by Google. Its goal is to speed up their display on smartphones. The goal of this hack was to find out if you could build such a page using Algolia. See what Haroen discovered on this GitHub thread.\n\ncreate-instantsearch-app\n\nReact has create-react-app to quickly bootstrap a React application, but they were no equivalents to start easily crafting a vanilla js app using instantsearch.js. \nCheck out what Alex built here: create-instantsearch-app.\n\nEditing indices from a spreadsheet\n\nThe goal of this project was to create a simple app allowing you to edit your index from a spreadsheet: https://github.com/algolia/algosheet/\n\nIndex videos transcripts and search inside them\n\nThe goal of this project was to index transcripts of YouTube videos and offer to search through them. Looking for a quote? You’ll be redirected to the right place inside the video.\n\nReact VR + React InstantSearch (featuring Speech-to-text)\n\nIn late 2016, we launched React InstantSearch compatible with React and React Native application. This hack was the occasion to see if it was working fine with React VR and to think of a new search interface using voice.\nCheck out what Marie built here: react-vr-feat-react-instantsearch\n6. Mix fun and work\nWhile coding is fun, you'll also want to do some team building activities involving everyone at the same time. We choose to do an escape game at the end of the day as it's a great way to have fun while strengthening team ties. It's also a way to discover how your teammates are thinking and to collaborate.\nWe had a great experience staying for 60 minutes in the vastness of the ocean trying to restart the submarine engine and make the torpedo ready for any possible threats... and guess what? We all managed to accomplish our mission.\n\nFinally, you can wrap up the day by sharing a drink and a dinner. It gave us the opportunity to share our feelings about our work at Algolia and the place of our team — and to simply enjoy the moment.\nThe… hack day was definitely a success for us: it was a great parenthesis from our day-to-day jobs, and not only did we have some time dedicated to exploring other things (be it AMP, virtual reality, or exploring the possibilities of Algolia), but it was also the opportunity to get to know each other in a different context.\nIt was the first time we organized a hack day for our team but it will definitely not be the last. We are thinking about what we could do next time to make it even better. Have ideas? Comments? Lessons learned from your own hack days? Let us know: @mlthuret, @algolia"
  },
  {
    "id": "6696-0",
    "title": "A Simple, Secure Tool for One-time (Self-Destructing) Messages",
    "author": "Eran Chetzroni",
    "text": "It is a very common practice and a very bad idea to send sensitive information over Slack or email. No matter how much you trust Slack or Gmail, there are types of company information (for example, SSH keys, certificates, security tokens...) that warrant an extra layer of security.\nThe challenge, then, is to create a more secure platform that is also easy to use in order to invite adoption.\nA complicated way to tackle the issue\nAlgolia’s Foundation Squad, which I am a proud member of, sends secrets and passwords on a daily basis. Existing ways of sending secure messages were quite cumbersome: you had to first ask the receiver to create a private and a public key, have them publish it somewhere like a web site, MIT’s public key service, or KeyBase, or send it directly to our squad. Only then we could start sending secure messages to each other.\nWhile operations engineers are used to (if not happy with) this level of effort, asking other teams— dev, sales, marketing, execs — to indulge in such a procedure is simply not practical: I could already see employees reverting back to email or Slack.\nExperience has shown us that the best way to mandate security measures is to make them as simple and easy to use as possible. Hence, I started racking my brain to find a new solution.\n“This tape will self-destruct in five seconds. Good luck, Jim”\nA solution is emerging\nWhile working on a different project in our continuous efforts to make Algolia security top notch, I started using Hashicorp Vault. Vault “secures, stores, and tightly controls access to tokens, passwords, certificates, API keys, and other secrets in modern computing. Vault handles leasing, key revocation, key rolling, and auditing through a unified API”. Through Vault, Hashicorp changed the way companies store their secrets.\nDigging a little deeper inside the Vault API and engine, I stumbled upon the concept of Response Wrapping. “Response Wrapping” is using Vault’s cubbyhole backend.… Cubbyhole is a short-term storage place where you can put a secret that will be consumed by a user or a service who is holding a specific token. No token can access another token's cubbyhole, whether to read, write, list, or for any other operation. When the token expires, its cubbyhole is destroyed.\nSo, a Vault token has all the features we need to create a self-destructing message service:\n\n1. TTL - a time during which the token will be alive\n2. Number of uses - the number of times you can use the token\n3. Cubbyhole - a special place where we can save secrets\n\nLet’s put all of these features into use and create a secret message service workflow:\n\n1. User inputs secret message\n2. Create a token with TTL=24Hours and NumberOfUses=2\n3. Write the secret to the “cubbyhole”\n4. Token NumberOfUses - 1 = 1\n5. Give the token back to the user\n6. User sends token to relevant recipient\n7. Recipient uses token to open the secret message in cubbyhole\n8. Token NumberOfUses - 1 = 0 ; hence the token and secret are deleted\n9. Success!! \\o/\n\nYet Another Secret Messaging Service?\nNow, before building the final tool, I did some research to make sure I am not reinventing the wheel. I decided to look for a self-destruct messaging software, and found a couple of candidates, but they all had at least one of the following issues:\n\nThey didn’t allow the option of self hosting, which made security an issue, thus defeating the purpose\nNot simple enough to use\nThey required a complex deployment on the user’s part, such as installing Redis, Node and other dependencies\nThe backend storage is typically not that secure\nThey are not open source\n\nSide note: I don't explicitly list the tools because the \"domain\" of secret messaging services is so tiny, that I believe your own research will take only a few minutes to come to the same results as I have. You can see a list of some nice projects here: \nhttps://github.com/Kickball/awesome-selfhosted#pastebins\nThe research justified building a… new tool with the following requirements:\n\nHosted on our server (aka super secure) easy to deploy\nEase of use\nUsing Hashicorp Vault (nust as an experiment)\n\nAll I had to do now is create a very simple API with 2 public methods:\n\nSetSecret - which puts the secret in Vault and returns a token\nGetSecret - uses the token and gives back the secret\n\nOn top of that I built a very simple web UI:\n\n1. You insert your secret, submit it\n\n\n2. You get a URL with the one time token\n\n\n3. You send the URL to the happy recipient via Slack or email\n\n\n\n\n\n\n4. She opens the secret message\n\n\n\n\n\n\n\n\n\n5. If you try to open the message again\n\n\nAnd that’s all there is to it!\nWhile this was at first just an experiment in using Hashicorp Vault for a secret messaging tool, it has really caught on at Algolia, where I see many coworkers using it for all kinds of secrets sharing.\nIf you like the tool, you can try it yourself on GitHub. It is open source, and we put on ProductHunt so it can be found easily (and of course, we’d love your vote 🙂\n\nLet us know what you think!"
  },
  {
    "id": "6684-0",
    "title": "Bringing the Missing Frontend Search Block to the Laravel Vue.js Community",
    "author": "Raymond Rutjes",
    "text": "Last year, Taylor Otwell, the maker of Laravel, decided to create an official Laravel package named Scout. The package allows Laravel users to add full-text search on top of their existing database without much effort.\nI was very happy to notice that Taylor identified search as being something complex that developers would need help with. I was even more excited by the fact that he chose Algolia as the default engine.\nVue.js and Laravel === the perfect mix\nIn Laravel 5.3, Taylor Otwell chose to power the frontend with the Vue.js framework. Every Laravel app since 5.3 now ships with a ready-to-use Vue.js skeleton.\nBecause Taylor made that move, Vue.js instantly got my attention. My thought process was that if Taylor chose to ship his backend framework with Vue.js in the frontend, it probably means that Vue.js shares some core values with Laravel. Vue.js is easy to learn, it thrives at empowering developers to add logic to the frontend, and it advances forward quickly.\nI remember this one time I opened a very naive PR to adapt the lifecycle of the components in core of Vue.js. Evan You, maker of Vue.js immediately picked it up:\n\nTook a long flight and got 10 new PRs when I got off the plane 😂\n— Evan You (@youyuxi) March 3, 2017\n\nLaravel and Vue.js are each made and driven by a single person. While some might argue this is a disadvantage, I see it as an opportunity to have opinionated people be more innovative and also be more reactive. This is even more true when those people have a very clear idea of where they are going and manage to stay open to the community.\nBuilding a UI components library for Vue.js\nBuilding search on the frontend has the advantage of offloading all search related operations from your Laravel app. Thanks to the Laravel Scout package, little setup is required to send your data to Algolia. I wanted to help developers also benefit from great search experience — this time, on the frontend.\nWe wanted to create a library that would embody the… same principles as Laravel: something users could easily install in existing applications and shape for their own need.\nVue.js offers a very convenient way of creating components in single files. This approach is very readable and makes it easy to reason about the search experience you are building.\nSo, we ended up bootstrapping a bunch of components that can just be dropped into your Vue.js skeleton. Components will then handle all the logic consisting of listening for user changes and asking Algolia for the new results:\nView the code on Gist.\nAll you need in order to use the above syntax in your own Vue.js application is to actually register the plugin:\nView the code on Gist.\nHow I challenged Vue InstantSearch\nAfter sharing my excitement about this new library, other engineers at Algolia wanted to try it out. They were intrigued by both Vue.js and Vue InstantSearch, so we started writing some documentation in order for them to try out my work.\nTo allow users to experiment with the library, I created a step-by-step getting started guide. When you reach the bottom of the guide, you have a working instant search experience.\nWe were really happy when testers told us they managed to get their search up and running in a couple of minutes, even insisting on the fact that they knew nothing about Vue.js previously. At the same time, I have to say that the feedback was largely due to the benefits of Vue.js framework itself, since Vue InstantSearch leverages its available features.\nBecause of the excitement around the library, and because we had already been invited to speak at the annual Laracon US event, we thought giving a preview of the library at the conference would be a good idea. We didn’t expect much other than to share the idea with the community and get some feedback in advance of the full release.\nMaxime Locqueville gave a live coding talk where he explained how to index data with Laravel Scout, and how to leverage Vue InstantSearch. By the end of the 30 minute… talk, he had illustrated how to build a fully blown instant-search experience with customized relevance. It was the first time Vue InstantSearch was mentioned in the wild, and then came this:\nhttps://twitter.com/jeffrey_way/status/892150662380888064\nLaracasts is the place where Jeffrey Way explains Laravel A-Z. I was really honored that Laracasts didn't just talk about us, but were the first real Vue InstantSearch users.\nTo be honest, I was a bit scared that Jeffrey would find blocking issues with the state of the library at that time. He raised some small questions in his first recording which we quickly addressed, and I was also very glad he gave us some more feedback via direct message.\nThe Vue InstantSearch journey is just starting\nToday we have over 100 users and about as much GitHub stars, but the project was still considered unreleased until today, because I wanted to make sure we had the right docs and enough feedback from testers.\nThis project has been so far the best pre-launch experience I’ve witnessed, largely thanks to  the communities it addresses. I’d also like to give a special mention to my colleague Haroen who has been challenging ideas and reviewing a fair amount of everything that is in Vue InstantSearch. The project would not have been in the current stage without him.\nV1 is, of course, just the beginning. Vue.js & Laravel have a strong future and we are excited to be a small part of it. Now, what can we do better? How can we help your use case? We look forward to your feedback: @algolia & @rayrutjes."
  },
  {
    "id": "6625-0",
    "title": "Performant Web Animations and Interactions: Achieving 60 FPS",
    "author": "Emily Hayman",
    "text": "Silky smooth interactions are critical for providing a natural-feeling application. The devil is in the details, and ill-performant web animations feel awkward, “janky”, and, above all, slow. Developers often invest quite a bit of time to reduce first page loads by even a few milliseconds, but forget to consider the impact of the interactions that follow.\nLike many at Algolia, I’d consider myself user experience obsessed. Performance is a crucial part of our formula for a stellar user experience: just as a speedy search leads to happy users, so do performant web animations.\nMeasuring Success\nYou can use the frame rate of an animation to measure how responsive an application feels. 60 frames per second is the generally accepted target for achieving a “natural” feel. What this means for developers is there are roughly only 16.7ms (1000 milliseconds divided by 60) to achieve all the work that has to happen in each frame. Consequently, the overarching goal is to limit the amount of necessary work. \nAs with all things in life, this principle comes with a caveat: more frames means more processing, which means frames might be dropped. If all the updates necessary won’t fit into the 16.7ms time allotted, it might be better to deliver lower frame rates (30fps) more consistently.\nBrowser 101: How a Pixel Is Born\nBefore diving too far into the nitty-gritty, it’s important to take a step back and ask: how is the browser actually generating the pixels a user sees from the code you write?\nOn initial load, the browser downloads and parses the HTML, and then converts elements into a “content tree” of DOM nodes. Additionally, style information gets parsed and calculated to generate the “render tree”. In the interest of efficiency, the rendering engine may split up the work that needs to be done, and certain parts of the render tree may be built before all the HTML is done being parsed.\nExample of a call tree of an initial page load\nLayout\nOnce the render tree… is completed, the position and size of elements is recursively calculated from the top left corner to create the layout. This computation can generally be accomplished in one pass, however, it may require additional passes depending on the flow of elements. Element placement is highly interdependent. In order to optimize the necessary work, the browser tracks changes and marks these elements and their children as “dirty”. However, due to the heavy correlation between elements, any layout changes will be quite expensive and should be avoided.\nPaint\nAfter the layout is created, content is displayed on the screen by the painting process. During this step, visual styles are used to paint the page according to the correct visual formatting model. Similarly to the layout process, “dirty” elements are tracked and consolidated into one larger rectangle. Re-paints will only occur once per frame to redraw this “dirty” region. As such, re-paints require a large amount of work and should be avoided. \nComposite\nIn the final step, all the painted elements are composited. By default, all elements are painted into a single memory layer; however, by separating elements onto compositor layers, updates will only affect the elements on their respective layer. The CPU draws the layers, while the GPU generates the layers. Hardware-accelerated compositing is incredibly efficient at basic drawing operations. The separation of layers allows for non-destructive changes. As you may have guessed, changes to GPU-composited layers are the least expensive.\nGetting Creative\nAs composite-level changes are the most performant, the only properties that should ever be changed are those that only trigger compositing. These properties are opacity and transform. Seriously, that’s it. However, this isn’t as limiting as it might first seem — you will just have to get a bit creative.\nTransformation\nTransformation allows for endless possibilities of visual changes to an element: you can… position it (translateX, translateY, or translate3d), scale it, rotate it, skew it, or even apply a 3-dimensional matrix transformation. In some cases, you might need to shift your thinking and consider how a change that would cause a re-layout or re-paint could be achieved with a transformation.\nAs a quick contrived example, consider the case of a “box” element that is shifted to the left 10 pixels when an “active” class is applied. \nInstead of changing the “left” property (like below):\nView the code on Gist.\nConsider translating the element across the x-axis:\nView the code on Gist.\nOpacity\nBy changing opacity level, you can easily show and hide elements (similar to changing “display” or “visibility” properties, but much more performant). Consider the case of a mobile menu toggle animation: in its open state, the menu will have an opacity of “1”. However, when it’s closed, its opacity will be “0”. It’s also best practice to define pointer-events as “none” to ensure a user doesn’t accidentally interact with the “hidden” menu. The “closed” class should be toggled as a user clicks the either “open” or “close” buttons. Here’s the respective code:\nView the code on Gist.\nAdditionally, opacity changes allow you to control the visibility level of an element. Again, this requires a bit of thinking outside the box, but can be quite powerful. For example, rather than animating an element’s box shadow directly, you can instead alter the opacity of a pseudo-element containing the box shadow. Here’s an illustrative code snippet:\nView the code on Gist.\nThis can instead be written as:\nView the code on Gist.\nForcing Promotion\nThere’s even better news— you have control even beyond which properties you select. It’s possible to manually promote elements to their own compositor layer. By forcing promotion, you can ensure an element is always painted and ready. This is an easy way to inform the browser which elements will… need a little more help — that is, anything that is paint expensive. This includes any element that will be changed (i.e., animated in some manner). Certain styles (such as: position: fixed and overflow: scroll) are quite expensive as well. You’ve probably seen bugs come up in the past with elements that “shimmy”, “flicker”, or simply don’t behave as expected. It’s common to see fixed headers on mobile blink as a user tries to scroll down the page. Promoting these elements is an easy fix for these types of issues.\nThe Hacky Method\nPreviously, developers were forced to “trick” the browser into promoting elements by applying styles that would trigger a new compositor layer but wouldn’t necessarily have a visual effect. Generally, this was achieved with either backface-visibility: hidden or transform: translate3d(0,0,0).\nThe New, Shiny Method\nFortunately, browsers now provide an explicit property to inform browsers what types of optimizations will be needed ahead of time called will-change. You can provide varying values such as a list of properties (“transform, opacity”), “contents”, or “scroll-position”. Perhaps most useful is “auto”, which will apply the default, standard optimizations. A quick example:\nView the code on Gist.\nAs in all things, however, moderation is important. There is such a thing as too many composited layers. The browser already does its best to optimize, and will-change optimizations are resource heavy. Using will-change implies an element is always moments away from changing. This is especially important on mobile — many composited layers can have a noticeable negative performance impact.\nAnimation Methods\nThere are two ways to animate elements: using either CSS (declarative) or JavaScript (imperative). Which one you choose depends on how you need to best accomplish your goal.\nDeclarative Animations\nAs CSS animations are declarative (you tell the browser what to do), the browser knows the full extent of… the operation, and thus the end point. As a result, it can make optimizations. Additionally, CSS animations run off the main thread, and prevent blocking more potentially important work. Generally speaking, CSS animations will be the more performant option. Keyframes in combination with animations can provide many powerful options for interesting visual effects. Here’s a small code snippet for rotating an element infinitely:\nView the code on Gist.\nThat said, CSS animations lack the expressive power of JS. One possible solution is to use JS to listen for user input and toggle a class based off the action. This class can then contain the necessary styles for the animation. Here’s a quick example that shows how to toggle a “class-name” on a box when it has been clicked.\nView the code on Gist.\nIt’s also worth mentioning that if you’re operating on the bleeding edge, the new Web Animations API allows you to harness the performance of CSS, within JS. Using the API, you can easily handle synchronization and timing of animations while still taking advantage of the perks of the CSS approach.\nImperative Animations\nImperative animations tell the browser how to perform the animation. In cases where CSS animations would grow too unwieldy or when more control is needed, JS should be used. It should be noted that, unlike CSS animations, JS animations will be run on the main thread (and as a result are more likely to drop frames), and are generally the less performant option. That said, there are a few options to consider when JS animations are needed.\nrequestAnimationFrame\nOne option to allow the browser to optimize for performance is requestAnimationFrame. You can consider it the setTimeout of the modern age; essentially, it’s a native API for running an animation. It will theoretically be called at 60fps, however, in practice it requests animation drawings at the next available opportunity — there is no set interval. The browser optimizes by grouping changes into… a single repaint, which saves on CPU cycles. \nIt can be called recursively:\nView the code on Gist.\nAdditionally, you should consider leveraging requestAnimationFrame for performance-intensive events like resize or scroll (as opposed to binding directly to the event).\nScroll Performance\nSpeaking of scroll, achieving smooth scrolling performance presents its own challenge. Fortunately, a few recent additions to the specs provide further options for fine-tuning.  Passive event listeners enable you to improve scroll performance by indicating to the browser that preventDefault will never be needed (which eliminates the need for scrolling to block on touch and wheel event listeners). Usage is as easy as specifying {passive: true} on a listener.  });\nView the code on Gist.\nStarting in Chrome 56, this option will actually be default for touchmove and touchstart.\nAdditionally, the new Intersection Observer API allows you to easily determine when an element has entered or exited the viewport, or intersected with another element. Rather than clogging up the main thread with event handlers waiting on an explicit intersection, the Intersection Observer allows you to execute any work needed only when monitored elements cross paths. This is particularly useful for creating infinite scroll or lazy-loading experiences.\nRead, then Write\nThe correct term for the side effects of back and forth reading and writing to the DOM is “forced synchronous layouts”, and when done in quick succession it’s known by the more illustrative term “layout thrashing”. As mentioned previously, the browser tracks “dirty” elements and queues up changes until necessary. By reading certain properties, you force the browser to perform premature calculations. This back and forth of read / write will cause reflows. Fortunately, this anti-pattern has an easy fix: read, and then write.\nTo illustrate this, here’s a contrived example where the layout is read/written to rather harshly:\nView the code… on Gist.\nInstead of reading and writing at every iteration, it’s better to read outside the “forEach” method:\nView the code on Gist.\nThe Future of Optimization\nBrowsers are continuing to add more and more options for fine-tuning performance. One new property (currently supported in Chrome and Opera!) called contain allows you to indicate that an element’s subtree is independent from the rest of the page. Essentially, this is an easy way to tell the browser that it’s safe to optimize an element. You can specify as values strict (for all rules), content, size, layout, style, or paint to limit the scope of any changes. This will ensure that DOM updates in the subtree do not trigger reflows on the parent document. In particular, this can be useful for third party widgets that you lack control over. A quick example:\nView the code on Gist.\nTesting Performance\nWhile knowing how to optimize is all well and good, it’s important to test your app’s performance as well. The best tool (in my humble opinion) is by far Chrome developer tools. Hidden away under “More Tools”, the “Rendering” pane offers several options including tracking “dirty” elements, calculating frames per second, and highlighting layer borders and scroll performance issues.\nOptions available to you under the “Rendering” pane.\nAdditionally, the “Timeline” tool (under the “Performance” tab) allows you to run animations and then drill down into problem areas. The main gist here is: “red” is bad, “green” is good. You can actually click into red areas and determine which functions are most problematic.\nAnother interesting option (hidden under the “Capture Settings” section) is “CPU throttling”. Using this setting, you can apply a slowdown multiplier to measure the impact on performance. This is particularly useful for testing how an animation will run on mobile. While the FPS might be quite good on desktop, it’s always important to consider other less powerful… devices.\nScreenshot of a healthy timeline.\nTest and Iterate\nThe easiest way to optimize animation performance is to reduce the amount of work needed in each frame. The most efficient way to reduce this workload is to only need updates to elements on composite layers, as these changes will be non-destructive. That said, sometimes you’ll have to get a little hacky and think outside the box. Performance fine-tuning is often a game of testing and iterating — but in the end, your users will thank you."
  },
  {
    "id": "6605-0",
    "title": "Making Search Talk: Connecting Algolia and Alexa",
    "author": "Dustin Coates",
    "text": "Mostly, I just wanted to stop typing so much. \nA couple of years back, I bought an Amazon Echo. It was on a whim, really. I had been buying all of my food on Amazon Fresh, Amazon announced a special Prime-only smart speaker, and I decided that Jeff Bezos needed more of my money. The speaker was nice, but I was dying to extend it, because, again, I was tired of typing, and instead I wanted to use my voice to interact with the world.\nDon't get me wrong: I can type with the best of them. I was the best typer in my third grade class. But I really only enjoy typing when it's either a competition or in the service of something else, like chatting with my girlfriend or railing against the designated hitter in baseball. \nWhen I speak with other developers it's the same thing. Some might love refactoring code, others building new products, and still others writing clean documentation. But I haven't heard one who would say: \"I typed 5,000 characters today. What a good day!\" Mostly, through shortcuts, snippets, and the like, we want to type less.\n\nOver the years, I noticed a couple of things. The first was that the service provided by my new employer, Algolia, was a really good fit for what I often did with the skills. It was easier to work with than Amazon’s DynamoDB (the general go-to data store for skills hosted on AWS Lambda), it was fast, and it could be leveraged inside AWS Lambda (where the majority of the Alexa skills reside) as well as it could be on the web.\n\nThe second thing I noticed was that I was typing the same things over and over. For Alexa, there would always be code that looked something like this:\n\nView the code on Gist.\nAnd with Algolia, I was always initializing the client, providing my credentials, etc. In defense of my colleagues who work on this, the amount of setup necessary is remarkably small when you consider what Algolia does. And, they've made it even easier with InstantSearch. \nTaking inspiration from InstantSearch.js, I wondered to myself if… there might be something similar that could connect Algolia with the Alexa Skills Kit.\nYou see, it’s become clearer than ever that voice as a user interface has arrived. Voice-enabled assistants such as Alexa, Cortana, Google Assistant, Siri, and more are making an impact and changing the way we interact with our computers. \nAmazon is certainly sitting in the pole position. Through the Echo line and partner devices, Amazon is expected to control 70% of the voice-enabled speaker share in 2017. This is a market that will see 130% growth in 2017, with voice-enabled assistants growing 23.1% on their own.\nAll this means that millions of new customers are purchasing Alexa-powered devices and expecting Alexa skills that just work. They expect the skills to be fast and tolerant of ambiguity. Often they're looking for information, as we can see in popular skills such as those from Kayak or Boston Children's Hospital.\nThe developers creating Alexa skills want to be able to create them quickly and trust that they'll work. This sounds like something to which Algolia is well suited.\nLet’s take a look. Algolia’s got typo tolerance (how often has Alexa heard “New York” when you meant “new work?”), synonyms (which frees you from having to use the skill builder for entity resolution), and search relevancy at the forefront of what we do. We’re fast, which is important for “quick thinking” conversational UI. We also don’t require information to come to us in a specific manner. And, Algolia can be set up in the time it takes you to watch the latest Game of Thrones.\n\nIntegrating Algolia with the Alexa Skills Kit was already simple, but how can we reduce the boilerplate--how can we reduce the typing? We decided to make it even easier by creating the Algolia Alexa Skills Kit Adapter. A small JavaScript library, the adapter is intended for use on Amazon Lambda and provides both tools for integrating Alexa and Algolia, as well as a framework for structuring your Alexa… skill.\nThe adapter is all convention over configuration--allowing you to stop thinking about the setup and start focusing on the user interface. While there might be more than 15,000 skills available for the Echo, it’s still early days and an intuitive VUI will help raise your skill above the rest. That’s what we wanted to optimize for.\nWe’ve already leveraged it to great effect in building skills that use Algolia. To see an example, check out a skill we built that enables searching for name meanings and origins.\nVoice-first is a nascent but growing technology. There are many opportunities for companies to stake their claims early. Algolia’s here to help. Get started with our Alexa Skills Kit adapter to build your own search into your skills.\n\n "
  },
  {
    "id": "6569-0",
    "title": "Improving Web Performance to Mirror Engine Speed",
    "author": "Jonas Badalic",
    "text": "Performance is a core feature of Algolia. Our search engine delivers millisecond results across millions of records. It was originally designed for mobile devices where the resources are very limited, but was after transformed into an online API that now runs as an NGINX module. Our front-end libraries are written in a way that allows us to express that same speed on the client and ensure a high quality experience across devices. Algolia’s engine and integrations are a great benchmark and an inspiration for performance, so we wanted to mirror that excellence in performance of our website. \nOur website is the face of our product. It’s the medium that’s used to present ourselves and the product. It is the first impression that many people get of Algolia. And to embody that core feature of speed we wanted to make sure we go an extra mile on our website as well. \nReducing the payload\nNo bit is faster than one that is not sent; send fewer bits.\n— Ilya Grigorik, web performance engineer at Google; co-chair of W3C Webperf WG.\nAs Ilya says, the fastest way to have a fast website is to send as little data as possible. Our website was relying on plenty of external dependencies, which increased the total size that needed to be transferred and delayed the load time. A total of 180 requests and 2.4MB had to be transferred by our website visitors.\nTotal payload of 2.4MB and ~180 requests sent to load the website\nDrop JavaScript dependencies\nRemoving dependencies such as jQuery, Underscore and a few others reduced our total JS size to about 20% of it’s initial size. This didn’t have a big impact on the way our front-end code was written, but it forced us to write it using the native browser API such as the querySelector API vs. using jQuery. \nTo ease the process, I integrated transpiling of our code so that we could write ES6 code and transpile it using Babel. This made writing the code more productive and faster.\nRuntime performance\nTo avoid the jank behaviour once… the website is loaded and ready for user interaction, it’s very useful to profile JavaScript code to find bottlenecks. \nChrome Developer Tools has a very useful rendering tab inside its console drawer, which show areas on your website that are regularly being updated and might be the cause of jank.\nAnalyze page performance by using Chrome dev tools\nThis mostly meant rewriting our scroll event listeners using the new IntersectionObserver API, making sure our DOM operations are not too frequent and our animations only cause composite browser operations (see csstriggers for a list of operations needed to update a property).\n\nThis reduces the heavy lifting needed by the browser to paint and repaint the screen, which in return provides a silky smooth experience for your users.\nReduce CSS size\nBecause the front-end team didn’t have a real convention of writing CSS, the size and specificity of it was growing fast as we added new pages. To tackle that, we wrote a small set of our own helper classes and adopted a way of writing the HTML using those classes. Doing it reduced the CSS file size to ~60% of it’s initial size, and paved a good way of adding new pages to the site while not increasing the CSS size further.\n\nThe pesky time-consuming part was done; now it was time to make sure the users actually see the page as fast as possible. \nPrioritizing first paint\nTo prioritize first paint I needed to determine which assets are critical for our website to render. That meant asynchronously loading all of the render blocking assets except a few very small images like our logo and our main application.css file which weighs about ~50KB.\nThe goal of this is to show the website on the screen faster by loading the rest of the assets in the background. \nBelow is what the loading looked like before the critical assets were optimized.\n\nThe optimized experience:\n\nThis optimization results in a faster perceived performance experience whereas the total loading time stays about the… same.\nAlong with having as few critical assets as possible, it is also a good optimization to have those assets hosted on your domain. Otherwise each of the requests made to different domains will have to go through the DNS lookup, connection and SSL negotiation phase, which will accumulate on the round trip time needed to perform the request.\nFor instance, if you are using Google fonts from their CDN and your server supports HTTP/2 protocol, it’s probably better to host the fonts yourself on the same domain as the initial request. This will bring significant improvements for the visitors coming from mobile networks, where the signal quality is poor and request round trip times are higher. \nIn our case, self hosting a font instead of loading it from google fonts CDN improved load time by about 1s on 3G connection.\nBefore (loading font from fonts.googleapi) vs. after (self hosting the same font)\nIf you look closely, you can also see that the fonts.googleapis request actually requests a CSS file which contains the @font-face rules that then create the actual request to load the font files. This means that, by including @font-face rules in our application.css file, we also save an additional request — a double win. If you are looking to do a deep dive into font loading strategies, Zach Leat from FilamentGroup wrote a very helpful overview of the improvements you can do today.\nAdding WebP support\nWebP is a new type of image format which enables better lossless and lossy compressions. The support for it is growing, so I decided to test it.\n\n40KB WebP vs. 57KB JPG\nI ran a few compression tests and saw that it was able to compress the file size to about an average of 75% of the original, which saved us hundreds of Kilobytes.\nWhen looking to integrate WebP support into our build process, I found a simple way to do so using Cloudflare and their Polish option. I saw that they allow automatic WebP image compression through their Polish feature, which took complexity of… integrating WebP out of scope; enabling it was as simple as clicking a button.\nAfter the Polish option and WebP compression are enabled, Cloudflare does the heavy lifting. It checks if the image request contains accept header with values image/webp or */*, as seen below. If the header matches, it converts the original image into WebP format and adds a content-disposition header with the value of inline; filename=”path/to/image.webp” instructing the browser that the file will be displayed inline on the page and giving it the file path to the resource.\n&nbsp;\n\nAccept header with webp support — img/webp and */*\n\nResponse header with content-disposition\nIn our case, Cloudflare’s solution worked well which meant I didn’t have to update server configuration and integrate WebP conversion at build time. However, if that is not the case for you and you want more flexibility, Ilya Grigorik wrote a sample config for detecting WebP support, and there are multiple libraries that you can use to convert images to WebP format.\n\nUsing HTTP/2 server push\nOne of the great things with HTTP/2 is that it has features like multiplexing connections and server push, which are substantial performance improvements to HTTP/1.1.\nMultiplexing connections allow browsers to send multiple requests through a single connection, which significantly reduces the number of required connections between the client and the server. \nServer push is a feature that allows the server to start sending assets that the client has not yet requested, but knows that the client will need, and so it eliminates the extra time the client would otherwise take to parse the response and request the assets. \nYou can implement server push either by adding custom HTTP headers, or by adding the link rel=”preload” and as=”&lt;type&gt;” to the asset source in your HTML, in which case you will need to polyfill the behaviour.\nTo additionally improve the time to first paint, I decided to avoid polyfilling link… rel=”preload” and set Link headers for our remaining render-blocking assets. This resulted in faster load time of assets and improved time to first paint by about ~400ms (depending on connection quality).\nTo validate the assets were server-pushed, check the developer tools network tab, where you can see that the request was not initiated by the browser after parsing the document, but was rather pushed by the request for index.html.\n\nInitiator — Push/(index) indicates asset was server pushed\nIf you are looking for a good hosting solution with advanced features like HTTP/2 server push, have a look at Netlify — they just added server push support and their hosting is very solid.\nThe hidden bottleneck\nAs I was optimizing our website, I looked for the obvious quick wins, but there is one thing I didn’t really look at — the HTML document size.\nThe compressed size of our index.html was 60KB.\nThe reason for that were inline SVG assets. Inlining SVG is often advised in the web community because of its flexibility. There are plenty of articles that advocate for it, but they are often flawed in that they recommend it as a universal solution, whereas it should depend on the use case. There are often better ways to load inline SVG assets than inlining them straight into the document. \nInlining SVG files bear two major consequences:\n\ndocument size increases\nassets are not cached\n\nIf you are accessing a website where the index.html file size alone is ~60KB, it will take time to fetch the document itself and after it’s finished, you still need the rest of the critical request to render the page. \nBy combining SVGs into a store, asynchronously loading and injecting them into the document, I was able to reduce the size of our HTML file from 60KB to ~15KB + as an added benefit, we were now caching those — a double win again.\nMeasuring results and changes\n\nThroughout the development I used two main tools to measure the performance impact of our… work — Chrome Lighthouse and webpagetest. The first tool — Lighthouse— can either be accessed through Chrome developer tools under the audit tab, as a CLI tool or as a Chrome extension. It provides valuable information and front-end metrics, whereas webpagetest can be used to go deeper into the network audit itself. \nThe results\nWe have seen a big improvement in loading performance: our website now loads much faster even on poor connections, ensuring that our visitors get a fast experience both when browsing our content and using our engine.\nThe total size of the website is now ~700KB compared to the original 2.4MB, with about ~300KB of external dependencies that we decided to keep for now. The amount of total requests is now in the 70s range compared to ~180. \nIn addition, our team was able to improve runtime performance and add more logic and animations to the website without having a negative impact on page performance.\nTo sum up\nThese improvements have helped stay on track of providing a unified and fast experience to all of our users and visitors (our documentation and our community page have also been updated with performance in mind).  \nI have had the chance to do a presentation of the topic to my Algolia co-workers, raising performance awareness within the company. A few weeks after, I did the same talk at a PWA Paris Meetup that we hosted in our Paris office. For those interested, the video is available on YouTube. \nLast but not the least, I’d love to hear your comments and suggestions on the topic: @JonasBadalic. Thanks for reading 🙂"
  },
  {
    "id": "6555-0",
    "title": "Computing Statistics on Terabytes of Data on Multiple Machines",
    "author": "Rémy-Christophe Schermesser",
    "text": "At one point, every company needs to compute some basic statistics on its data. I’m not speaking about advanced statistics but simple ones, like means, top values, etc. The algorithms to compute those metrics are fairly straightforward, but can take a lot of memory. So what happens when the amount of data is so big that it can’t fit in a reasonable amount of RAM? \nToday at Algolia we generate 2 TB of logs per day, so computing metrics on this doesn’t fit into any of our machines. Furthermore, the data team would like to have those metrics in pseudo real time, so we need to process the logs in real time.\nHow to compute metrics on terabytes of data\nFor some of our data, it was OK not to have an exact value but an approximation. For example, instead of having an exact average of 3.9, we get 4.0, and that’s ok. Thankfully, some algorithms and data structures are able to lower their precision to have a lower memory footprint. Those data structures are what we call probabilistic data structures (or algorithms), and they all share some properties:\n\n\nHave a small memory footprint — we are talking KB of memory instead of TB.\nUsable in a single pass: each piece of data is processed once by the algorithm.\nNo statistical hypothesis on the data being processed.\nHave a precision within an acceptable error margin. Most of the time the margin is a parameter of the algorithm.\n\nWe’ll see later how they work.\nHow to compute metrics on a stream of events\nAs we have logs arriving continuously, the team thought that we could leverage this and compute metrics on the fly. There are algorithms designed to process data on the fly, and they are called streaming algorithms. Sure, the name is fancy, but we code them every day. Let’s take an example. \nLet’s say we have an array of integers, and we would like to compute the sum of each element. In pseudo code we would probably do something like:\nView the code on Gist.\nNow imagine that our array is infinite (what we call a… stream). Our code won’t change, and at any point in time, the variable sum would contain the sum of all elements we saw. This is a streaming algorithm: an algorithm that can process an infinite number of elements and that is able to give a coherent state after each iteration.\nBonus point: most probabilistic data structures are streaming algorithms.\nHow to compute metrics across multiple machines\nWe are able to compute simple metrics with a small amount of RAM and from a stream of data. But what if your stream is so big that one machine can’t handle it? As we found a way to reduce the memory footprint, couldn’t we find another trick for CPU? \nOne simple option would be to split the workload across multiple CPUs. For this, we will use a mathematical property. Yes, math. Bear with me a few seconds, you won’t be disappointed.\nLet’s say you have a set of elements. On this set you have an operation, let’s call it +, that takes 2 elements of this set and gives back an element of the same set. We say that this set and this operation is a monoid if:\n\nFor any a, b, c that are elements of this set, this holds: (a + b) + c = a + (b + c)\nThere exists an element e of this set where: e + a = a + e = a\n\nLet’s take some examples:\n\nIntegers and addition, where e is zero\nStrings and concatenation, where e is the empty string\nLists and concatenation, where e is the empty list\nBooleans and &amp;&amp;, where e is True\n\nWhy did I bother you with this?\nLet’s take a simple example, with integers and addition. If you want to sum 1, 2, 3 &amp; 4, you can ask Google or you can spread this sum on multiple CPUs, let’s say 2. Because this is a monoid you know you can do sub additions, for example: 1+2+3+4 = (1+2) + (3+4). So, you can ask the first CPU to compute 1+2, the second one 3+4, and only then sum those sub sums. And voilà, we have our final sum, which is 10.\nTherefore, if some set and operation validates 2 properties, we have a sound way to spread the computation on… multiple CPUs or machines.\nBonus point: most probabilistic data structures and streaming algorithms are monoids.\nConclusion\nAlgorithms like the ones above are not new — far from it. The first probabilistic data structure was invented in 1985 by Philippe Flajolet (hence the pf* commands in redis). Monoids are even older. \nHappily for us, a lot of these algorithms are already implemented in big data software (think Spark, Apache Beam, Algebird, etc.). \nThe main thing we should remember is that some simple mathematical properties give us many nice coding features. Perhaps now you feel sad if you slept through your math classes :)."
  },
  {
    "id": "6511-0",
    "title": "Introducing InstantSearch iOS: Create Great Search UX with Swift and Objective-C",
    "author": "Guy Daher",
    "text": "We’re excited today to be releasing InstantSearch iOS, a library of views and helpers for building a great search experience on iOS. It is built on top of Algolia's Swift API Client to give iOS developers a way to quickly build a search experience that will delight their users.\nBefore we dive into how InstantSearch iOS works, let's talk about why we need this library in the first place.\nImplementing search on iOS today: hard and complex\nMobile screen sizes are small by nature, which makes it hard for end users to see and filter their results. It is therefore expected that developers invest a lot of their time into building a search experience. Let’s explore possible ways to implement search on iOS today, as well as their costs and benefits.\nLocal search\nThe most basic search experience that you can provide is by creating a local search on the data that you have on your device. This data can reside in one of the following places:\n\nIn memory, such as in an array or a dictionary.\nOn disk such as using CoreData, SQLite or Realm.\n\nIt is fairly straightforward to implement a basic string matching algorithm. Just follow a tutorial such as the Raywenderlich one, and you’ll get something working in an hour or so. They even add a way to filter by one refinement by scoping with a scope bar. \nBenefits:\n\nNo 3rd party dependencies, you’re in complete control\nEasy to implement and maintain\nFree\n\nCosts: \n\nLow quality. This is a basic search implementation; no advanced features such as typo tolerance, custom rankings and synonyms.\n\nImplement search on your backend\nAnother popular way of implementing search on mobile is to provide a search functionality from a backend. If you do it this way, the mobile app would request search results by simply making a network request to your backend and adding the query and the filters in the request.\nThis is the most complex way to implement your search since you have to take care of every part of your search architecture: the search… business logic on the backend, the communication between the backend + iOS, and finally the presentation and update of your search UI experience. \nBenefits:\n\nNo 3rd party dependencies, you’re in complete control\nReuses the search business logic across more platforms such as Android and web\n\nCosts: \n\nComplex implementation for getting relevant, typo-tolerant search results with custom rankings\nNeed to host it and maintain it on your server\n\nSearch-as-a-service \nThe last way to implement a search experience is to rely on a 3rd party search-as-a-service to provide you search results and refinement capabilities. That way, the mobile app will just have to call an API over the network to get all the search results. This means speed superior to other solutions’, plus the search backend is taken care for you in this case.  \n\nBenefits:\n\nPremium search features that work out of the box: speed, relevance, typo-tolerance. No need for search expertise on your part.\nReuse the search business logic across more platforms such as Android and web.\nSearch backend taken care for you, no need to worry about hosting and availability. \n\nCosts: \n\nReliance on 3rd party company\nCosts some money to use SaaS\n\nThe common pitfall  \nAll of the three methods above will help you build the search business logic of your app, but they have something missing: you still need to build the search UI on the iOS app. If you’ve tried to build a complex UI search experience, you quickly found out that it can be tough to get it right: from dealing with all the states of the search lifecycle to updating all components as results come or filters change, it can be quite a challenge to implement a search UI correctly.\nIntroducing InstantSearch iOS \n\nInstantSearch iOS offers a way to easily build search UI experiences on iOS by using Algolia as the backend search-as-a-service. It lets the developer focus on the look and feel of their app, without having to worry about Algolia API calls, or query crafting… and state management. \nSearch is its own domain of expertise\nSearch is very complex if you want it to be done in a correct way. A good search has to provide speed, relevance and typo tolerance, which is a lot to ask, especially to mobile developers whose primarily skill is building apps, not writing algorithms for fetching the most relevant search results. \nTranslate the concepts of search into UI\nThinking in terms of terms such as disjunctive faceting offers a lot of cognitive complexity to the developer. InstantSearch makes it easy since developers will only have to think in terms of “and”, “or”, “"
  },
  {
    "id": "6494-0",
    "title": "Inside the Engine Part 8: Handling Advanced Search Use Cases",
    "author": "Julien Lemoine",
    "text": "Building great search UX is one of the hardest problems in engineering, even for companies that have a large team working exclusively on search. \nThere are three essential hurdles to clear:\n\n1) engine performance\n2) interface intuitiveness\n3) result relevance\nWe’ve addressed search performance in several of the previous articles of this series; this article will focus on the link between user interface and relevance. It will cover a wide range of use cases — from common to very specific. If users complain that they cannot find what they are searching for quickly, it is probably caused by a mix of non-intuitive interface and poor relevance; this is why it is important always to consider them together.\nProviding good relevance for the most common cases\nWhen people are searching for something very general like a company name on LinkedIn or a person on Twitter, you should make sure you are using a notion of popularity on the results: a result with more followers has more chance to be what the users are searching for. Solving this problem has been our primary focus at Algolia from day one. We have addressed this by developing a configurable but easy to understand ranking algorithm where developers have full control of their search configuration. You can configure the popularity via Custom Ranking, make sure you can address a complex problem like typosquatting, promote some featured items, etc.\nThere is an endless way to configure the ranking, depending on the use case. We could even help Twitter solve their search issues :), as you can see in the following example:\n\nSearching for the official Barack Obama account on Twitter with a typo does not return the official account because of typosquatting (May 2017)\nProviding good relevance for advanced search use cases\nSolving advanced search use cases is an entirely different problem where you need to let the user refine search results via filters. We will cover in this section the three main approaches that are used to… facilitate an advanced search.\nUse of faceting\nFaceting is probably the most standard way to address advanced use cases. The principle is to let the user filter the results based on categories to reduce the size of the results set and remove all false positives or ambiguity. This feature is used in a wide variety of use cases, and there are a lot of different ways to present faceting results, as you can see in our faceting documentation.\nFaceting example: filter on genres and rating\nThat said, faceting is not the perfect solution in all situations as two significant concerns can hurt your relevance:\n\n1) It requires homogenous data: you need to have the same categories inside all your records to have a good UX. If this is not the case, you might have duplicates in your faceting results that will lead to users not finding their record while filtering.\n\n2) The category that the user wants is hidden: when the number of categories to display is large, you won't be able to show all of them, and you might miss the one that the user wants. It will be a challenge to satisfy this user, who will probably just consider your search not good enough.\n\nSearch in faceting results\nTo fix the second problem—faceting not visibly proposing the category that the user is looking for—you can offer users a search for facet values. This means that the full range of categories does not even have to be displayed. Rather, you can offer the most relevant or meaningful categories, and let the user search for the rest. You can find this type of experience on LinkedIn: you can filter on locations when you are looking for a person or a company. LinkedIn proposes you to search for a particular location that is not listed in the selected facet values, which allows being very granular in your search. \nThe only problem that you might have on Linkedin is that this search is not contextual. You are searching inside all locations but without applying the current textual query and filters. Let’s say… that you are looking for a Director of Marketing job in Buenos Aires but that there aren’t any available on LinkedIn. After you’ve searched for the job title, LinkedIn allows you to refine your search and select “Greater Buenos Aires” in the location facet value search, although this will not return any results. This is frustrating as you’d naturally expect not to be offered filters that will lead you nowhere. \n\n\nImplementing a search in faceting results while applying the contextual query is very complex, and this is probably the reason why LinkedIn preferred to implement a degraded version. A better search experience is our primary motivation at Algolia, which is why, a few months ago, we released a feature called search for facet values that allows you to develop this type of experience in minutes while applying the contextual query. Here is an example of an Algolia search in the brand value in an e-commerce context.\nSearch inside facet values: example of the brand facet\nAs you can see, the search is completely contextual: all filters/refinements are related and inclusive of one another. You can find more documentation about how to use it in our developer documentation. \nAdvanced query syntax\nSearch for facet values is very useful when you are looking to expose an advanced search to your user without burdening them with a learning curve, which is mandatory for a consumer product. If you are working on a business product that people use every day, you might want to expose them an advanced syntax inside the search box like Slack does.\n\nExample of advanced syntax in a search box (Slack)\nIn practice, proposing such an experience requires the same feature as searching for facet values. It is just a different way to expose the same feature. The goal of this display is to let your advanced users directly perform their advanced query via the keyword and minimize the number of steps they will need to search for their content. You can implement such interface… easily with our search for facet values feature. We will release a guide soon to help you through the implementation.\nThe implementation\nWe described in the previous sections how the search for facet values feature is useful for implementing an advanced search interface. We will now focus on the implementation to discover how this feature works internally.\nI am sure you will think this type of search simply requires a change in the way we perform the query, like rewriting the query to specify which attribute we will target. But in practice, this is a significant change as we do not return records but rather highlighted facets with counts. \nLet's take a simple example of a professional social network like LinkedIn to illustrate the implementation. We will take simple records containing only four attributes. Those attributes will contain a string and will all be configured as attributes for faceting:\n\nThe name of the person\nTheir title\nThe company where they work\nThe location of the company\n\nHere is an example of such a record:\nView the code on Gist.\nAt first sight, this seems like a simple problem to solve for any search engine. Let’s look at what you need:If a user performs the query ”Twilio\", they will retrieve hundreds of Twilio employees. Let's say the user wants to sell Twilio a service in San Francisco and wants to see all the Director titles to find the closest one to their service (this is, of course, a purely fictional use case :)). \n\n1) The profiles need to work for Twilio, so we can restrict the search on “Twilio” to the “Company” field.\n2) They need to be a Director, so you can restrict the search on “Director” to the “Title” field.\n3) If multiple people have the same title, you want to see the title only once, so you need to deduplicate the results. To achieve this, you can use a facet on the attribute “Title”, and display the facet values returned.\n\n\nIn theory, this search should provide the results that you want, but in… practice, it’s more complex than that:\n1. People often have multiple job titles in their profiles. And if someone has both “Director of Sales” and “Account Executive” listed as a job title, this query strategy will mention both job titles in the list (because the query relies on faceting). Do you really want to see “Account executive” in the list of results for the query “Twilio Director”?\n2. Since we’re displaying facet values, and not search results, we cannot highlight the words that match the query. It’s always better to have highlighting, particularly when people type something with typos (Diretcor), or if you search at each keystroke (Direc).\n3. It creates a complex UI/UX, because users will need to specify that Twilio should only be searched in Companies, instead of allowing them to search Twilio in all fields (which would also potentially improve the relevance, on top of reducing the complexity).\n\nThere is a better solution: search directly in facet values while providing the context of the query. Let’s see how we can do this by using the “search for facet values” feature on the facet “Title”:\n\nView the code on Gist.\nThis query will be applied in two steps:\nFirst, it will retrieve only the results containing the word Twilio in one of the searchable attributes. From this list, it will extract all of the values for the facet Title (i.e., all of the jobs titles listed in every profile that contain the word Twilio).\nThen, in this filtered list of job titles, it’ll search for the ones that contain “Director”. This allows us to only retrieve relevant results, by using all of the regular search features (typo-tolerance, highlighting, prefix-search, ranking…). The result is the list of all the Twilio job titles containing “Director”, deduplicated and ordered by count — exactly what we were looking for.\nIn other words, this feature requires a two-step process that is only doable efficiently when implemented at the heart… of the engine. \nEvolutions\nThe release of this feature enabled all our existing users to build a great advanced search interface quickly. We, of course, do not plan to stop here and are already thinking about the next evolutions of the feature. For the moment, the ranking of results is pretty basic and is only using the frequency of facets, which means you can have a result with typo first. We plan to improve the feature by providing a different way to rank the results.\nIf you want to know more about the internal aspect of the Algolia engine, we recommend reading the other posts in this series:\n\nPart 1—Indexing vs. Search\nPart 2—The Indexing Challenge of Instant Search\nPart 3—Query Processing\nPart 4—Textual Relevance\nPart 5—Highlighting, a Cornerstone of Search UX\nPart 6—Handling Synonyms the Right Way\nPart 7—Better Relevance via Dedup at Query Time"
  },
  {
    "id": "6448-0",
    "title": "Connectors, Docs & the Future — a Deeper Look into InstantSearch.js v2",
    "author": "Alexandre Stanislawski",
    "text": "Back in November 2015, we released InstantSearch.js v1 in order to give you an efficient way to build search UIs. We have received good feedback so far: 1600+ GitHub stars, 600+ upvotes on Product Hunt, and 1700+ live implementations and counting.\nOver the last 18 months, we have learned a lot with your feedback, our experience creating other libraries, and with our internal usage of those same libraries. \nWhat we’ve found is that:\n\nWhen it comes to widget customization there are never enough options.\nThe learning curve has one steep jump in it: when you have exhausted the API options, you have to create custom widgets which requires search knowledge.\nThere are some limitations in the current implementation that we want to overcome so that we can confidently continue to improve InstantSearch.js.\n\nWe’ve done our best to integrate what we’ve learned into InstantSearch.js v2, which we are releasing today. Here are some of the changes we’ve implemented.\nCustomization API for widgets\nBefore getting down to nitty gritty details of this new API, let’s have a look at a practical use case.\nBlog layout as of June 2017\n \nOn our blog we use a menu widget to let the user navigate into the categories. The behavior of this widget works perfectly here because of its unique properties:\n\nOnly one element can be selected at a time\nIt displays all the values with the counts\nIt also displays a special all item which deselects the current category\n\nThis behavior is exactly the same as another well known UI element: the drop-down menu.\n\nSome of you rightfully asked: \"Can we render your menu widget as a drop-down element?\"\nThe problem is that the HTML of both UI menu elements is completely different and can’t be expressed as template options. The two widgets’ HTML implementations are very different:\n HTML rendering of the menu widget and a drop-down\nReflecting on that, we would have to either:\n\nAdd a \"renderAsSelect\" option to the InstantSearch.js menu widget\nProvide a… menuSelect widget (we even considered a PR for that)\n\nBut adding more options does not make an API any good. This problem is actually deeper and can be phrased as: \n\"How can I precisely control the rendering of InstantSearch.js widgets?\"\nAs we discussed during our release of react-instantsearch earlier this year, widgets are made up of two parts:\n\nSearch business logic, which deals with the Algolia engine\nRendering logic, which renders the UI and handles the user interactions\n\nWith the connectors API, we abstracted the search business logic into its own API, making it easier than ever for you to precisely control the rendering of any widget. The default IntantSearch.js widgets are implementations of their respective connectors. \nExample usage:\nView the code on Gist.\nThe connector handles the business logic and exposes a simplified API to the rendering function (items, refine, widget parameters). In this case, it receives a function to change the currently selected value and the list of options to display. This makes the core logic of a widget easily reusable.\nIn v2, all the built-in widgets with a UI rendering now have their own connector. Because this is a significant change in the way the library is used, we needed to rethink the documentation: \n\nRevamping the documentation\nIn v1, the documentation layout was a single page with multiple columns. Visually it looked great but when the API grew, some issues started popping up:\n\nThe content felt constrained and was hard to scroll through\nIt was hard to find space for new topics\n\nAfter InstantSearch.js v1, we iterated a lot on our other documentation websites: places, JS Helper, React InstantSearch. With this knowledge and experience, we found that the best way of organizing documentation for libraries like ours is having multiple pages. For InstantSearch.js v2, we are introducing getting started and guides, and we are reorganizing the API reference into independent sections.\n\nThe getting started tutorial is the part… that helps our new users most. Making it a prominent part of the navigation easily orients them. This is where we want to provide a good overview and real learning value.\nGuides are also very important because we found that you can only do so much with an API. Guides provide a framework to users whose use cases may be too niche or advanced to use an obvious solution.  \nFinally, we kept and split up the API reference. Like we’ve done since InstantSearch.js v1, this part is completely built on top of the code documentation (jsdoc). We even go to the point of using it to create pseudocode examples to explain the API.\nTo learn more about what we think about documentation, read the blog post that we wrote about the user’s journey learning a library.\nBuilding for the future\nFor the last part of this release, we also wanted to make sure that we are going to be able to continue to improve the library during this major version’s lifecycle. For this, we had to introduce a few breaking changes. All of them are documented in the migration guide.\nOther notable changes:\n\nInstantSearch.js now ships a simple and effective theme for all widgets ?\nThe slider is now based on AirBnB's rheostat slider ?\nThe build is now 33% smaller and uses preact internally ?\nThe searchFunction (an advanced feature) has been improved. It now has all the methods ?\n\nFinal word\nWith this version, we want to:\n\nGive you more power to customize widgets\nSimplify the project to new users\nMake the core of InstantSearch.js future proof\n\nInstantSearch.js v2 is available and you can try it now:\n\nRead the documentation\nMigrate a v1 application\nCheck the NPM package on the Yarn website \n\nAs always with Algolia, your feedback is key to provide you the best tools. Encountering a bug? Open a GitHub issue, Want to showcase your awesome InstantSearch.js website? Post it to our forum and get expert feedback from our community.\n\n "
  },
  {
    "id": "6431-0",
    "title": "Harry Logger and the Metrics’ Stone",
    "author": "Rémy-Christophe Schermesser",
    "text": "Introduction\nAs Algolia grows, we need to reconsider existing legacy systems we have in place and make them more reliable. One of those systems was our metrics pipeline. Each time a user calls the Algolia API, whether the operation involves search or indexing, it generates multiple lines of logs.\nWe generate around 1 billion lines of logs per day, which represent 2TB of raw data\nThose logs contain information about the query that, when harvested, can yield insightful results. From those we compute metrics. Those are all the numbers you can find in the Algolia dashboard, like average search time, which user agents are using an index, etc. It’s also used for the billing as it’s computing the number of objects and the number of operations our customers perform. \nAs we are big fans of Harry Potter, we nicknamed this project “Harry Logger”.\nThe Chamber of Logs\nThe first thing we had to do was to transfer, in a resilient way, using as few resources as possible, the logs for the API machines to our metrics platform. The old system was doing a  fine job, but worked by having a centralized system pulling the logs from each machine. We wanted to go to a push strategy using a producer/consumer pattern. \nThis shift enabled us to do 2 things:\n\nReplicate the consumers on multiple machines\nPut a retry strategy closer to the logs, in the producer\n\nWe needed something that works reliably and in a clean way, hence we asked Dobby to do the job. For performance reasons, Dobby was developed in Golang:\n\nThe prisoner of SaaS\nOur second job was to compute metrics on top of those logs. Our old system was a monolithic application that ran on one machine, meaning it was a single point of failure (SPOF). We wanted the new version to be more reliable, maintainable and distributed. \nAs SaaS is in our DNA, we went to various companies that specialized in the processing of metrics based on events (a line of log in our case). All the solutions we encountered were top notch, but as a… company, the quantity of the data we generate on a daily basis presented an issue. As of today, we generate around 1 billion lines of logs per day, which represent 2TB of raw data. And no vendor was able to handle it. At this point, we were back to square one.\nThe Streams of Fire\nAfter much consideration, we concluded that we had to build our own system. As the logs are a stream of lines, we decided to design our new system to compute metrics on top of a stream of data (and it’s trendy to do stream processing). \nWe tried the typical architecture:\n\nAs we didn’t want to maintain and host this architecture (we have enough API servers to maintain), we decided to consider a cloud provider. We managed to find every tool on the shelf, which meant we’d have less software to operate and maintain. As always, the issue was price. This streaming platform was 100 times more expensive than our old system. We had a lot of back-and-forth with our cloud provider to try to reduce it, but unfortunately, it was by design in their system. And again, we went back to square one.\nThe Half-Blood Batches\nDuring our tests, we found that the Stream processing software we were using was also able to work in batch mode, not trendy but maybe it was a way to fix our pricing issue? By only switching to batch mode, the price was reduced by a factor of 10! We only needed an orchestrator to launch the batches:\n\nAfter some development of reliable orchestrator we had a fully working system, but it was still 50% more expensive than we envisioned.\nThe Order of DIY\nOne of our engineers, a bit fed up by the amount of time we took to optimize the system, decided to do a proof of concept without using any framework or PaaS software. After a few days of coding, he managed to develop a prototype that suited our needs, was reliable and had a running cost 10 times lower than the batches!\n\nThe tale of the Wealthy Bard\nMigrating our log processing toolchain yielded many outcomes that were valuable to our… team. In addition to improving the reliability and the evolutivity of our current toolchain, we also increased our internal knowledge regarding our PaaS provider. The process also helped us identify deployment pain points that we will address later this year. \nFinally, we iterated by using and composing different solutions to solve the same problem. The best solution, for us, was the one closest to the no-compromise with respect to your budget limits. In our case, it was possible to have both by finally keeping the multi-master key/value storage of our PaaS provider."
  },
  {
    "id": "6415-0",
    "title": "Testing for Failure in a 99.999% Reliability World",
    "author": "Rémy-Christophe Schermesser",
    "text": "Even in a perfect world where everyone is doing test-driven development (TDD), even when everything is well planned and as a result, those plans succeed - Even in this world, things will fail. Bugs happen. Inevitably, there is always a little thingy that was forgotten. That’s this little thingy that this post is about. \nIn our years of building Algolia, we've had our share of learning experiences when it comes to building reliable infrastructure, especially when it comes to hardware and networks. As we strive to have a five 9s SLA, we need to have as many failovers as possible. Part of this failover is done in our API clients, where we implement automatic retries in case of TCP or DNS failures.\nTCP is complex and can fail. The TCP stack of most programming languages is sound, but not all HTTP clients know how to handle failure correctly. That is why we need to thoroughly test how our API clients are behaving in case of network failures.\nIf something fails or is not fast enough, we want to make sure that another method is used. So, the most important factors, in our view, are timeouts. We need to be sure they are handled correctly by the HTTP clients we are using. \nWith our knowledge of TCP, we knew that the timeouts we wanted to enforce were: \n\nConnection timeout: the time to make the initial connection, i.e. the time to initiate the TCP handshake\nRead timeout: the time to wait to read data, i.e. the delay between 2 bytes sent by the server\n\nFor DNS, it’s a bit more complex. Most of the time, it’s not a connected protocol and uses UDP. We saw that it is handled very differently in each programming language, so we needed to make sure our API Clients were behaving in the same way whatever their programming language. Hence, we wanted to enforce only one timeout: the time to resolve a hostname.\nSimulating network errors\nWith in mind what we wanted to test, how could we simulate network errors easily and in a way that is language agnostic?\nFirst, connection… timeout. This one is quite easy, as you only need a host that resolves to an IP that doesn’t answer. Some ranges of IPv4 are reserved, so you only need one host that resolves into  the private network range, we chose randomly 10.255.255.1.\nSecond, read timeout. For this one, we need a host that resolves to an IP that accepts connections, but never answer when we ask for data.\nThird, the DNS timeout, which is a bit more tricky than the first two tests. To test for this condition, we need a host where its DNS resolution times out. So we created a new domain where the DNS resolution is handled by a server that timeouts. Ring a bell? It’s the same as the connection timeouts. The resolver of our domain is the same IP that the one for the connection timeout: 10.255.255.1.\nWith all of this we could test timeouts in every language possible.\nSimulating user input\nWe are operating a public facing API, so anyone can send us a request. And a small part of those are invalid:\n\nInvalid JSON\nBad UTF-8 characters\nSQL injection, remote code, or attacks of the same kind\n\nFor this, there was already a lot of resources on Internet, so we used them.\nFor JSON, we use YAJL, so we were also pretty confident in the handling of JSON. For various reasons, we tried developing our own JSON parser, so we wanted to make sure it was handling bad and good JSON correctly. We stumbled upon this article and this test suite. We used it to test our JSON parser &amp; YAJL. Funny thing, we discovered that YAJL accepts line feed (\\f) as a valid whitespace character, where the JSON standard doesn’t.\nUTF-8 is a complex encoding format, and it’s quite easy to generate a sequence of bytes that result in a bad UTF-8 character. For this, we aggregated multiple source of bad sequence so we could use them.\nLast, but not least, we evaluated naughty strings. It’s strings that could be a security issue/flaw: https://github.com/minimaxir/big-list-of-naughty-strings. \nSo with little effort, we manage to add… quite a few tests to ensure that we handle corner cases correctly.\nDiscovering Failure\nFor the previous failures, it was something we knew beforehand. So it was quite easy to know what to test and how to simulate errors. But what happens when the unexpected happens?\nLet’s take an example. We have an internal application that reads logs that are in the following format: “key1=value1;key2=value2”. This format is quite straightforward. It’s a key/value separated by semicolons. So, there isn’t a ton of code needed to parse it. But this application is business critical and should handle incorrect logs in a proper way, aka not to crash. \nTo ensure it doesn't crash, we can add some basic unit tests as well on some corner cases we thought, but there was probably a lot more that we didn’t think about.\nOne way to do this is to use property testing. It’s a way to test code where you let the computer generates the testing data. It comes from functional languages, where it works pretty well as all functions are pure and could be described by its inputs and outputs. \nProperty testing works when you describe properties on your code, you describe how to generate the data, and then you let the property test framework generates the data and it checks if those data validates the properties. \nLet’s take a full example with our log parsing application. One property could be “I should not throw an exception if I receive an invalid log”. \nSo what is a log?\n\nIt’s a string\nIt’s a sequence of strings separated by semicolon\nIt’s a sequence of “key=value” separated by semicolon\nWe could then generate the key/values we expect, and so on\n\nThen we run the testing framework, and it will test our application with the data that is constrained by what is a log. With this we managed to found some corner cases in our parsing of logs. One field was expecting a IP address but we didn’t check it was in the correct format, for example.\nIn Summary\nTesting for failures is not… a lot of work if you know what are the areas to look for. As long as you know it, you can find some good documentation of corner cases. For all the rest, with little effort, you can code property tests that will test your software in a new, and unexpected way."
  },
  {
    "id": "6402-0",
    "title": "Redefining Incredible; Redefining Search",
    "author": "Nicolas Dessaigne",
    "text": "Today we're proud to announce that Accel has led a $53 Million Series B investment in Algolia - you can read more about it here.\nWe closed our Series B exactly two years after our Series A. Back then, we knew there would be many challenges to face but we believed we could create something incredible. Since then we've continued to learn, to evolve, and as a result we've become a stronger company. We've grown from 20 to 115 team members, our customer base has increased from 650 to 3,000, and we have 8,000 active implementations, across which we’re powering 25 billion searches each month. We have customers in 100+ countries and we have offices in four cities (San Francisco, Paris, New York City, Atlanta).\nWith the team, I often say that a funding round is just a means to an end. In the end we will not be judged by how much we raised but by how much we delivered. Today is a time for us to take a new look at our future, and define what \"incredible\" will mean to us next.\nIf software is eating the world, SaaS APIs are eating the development world. \nSaaS API companies like Algolia are designed from the ground up to innovate. We hold our roots not only in cloud hosting, but in APIs which sought to optimize the speed with which products communicate with users; that meant APIs to post to social networks, to deliver texts, emails & phone calls. \nNow we’re in a new era: thanks to pioneers like Amazon, Twilio and Stripe, developers are now able to create the very building blocks of an application itself - authentication, payments, and, yes, even search -- all through APIs. \nFor companies like Algolia, APIs aren’t a secondary tool for extending the core product’s reach, such as Uber’s API, for example, which allows Uber to enable other applications to connect into their ride-hailing functionality. APIs are our lifeblood. They have to be reliable - their SLA payback for anything above %.01 downtime should align with that commitment - and importantly they have to… continuously innovate. \nThe benefits of SaaS APIs are already visible to the developer community: Stripe’s release of Radar in the past year showed how, without making any changes to one's code base, developers could benefit from Stripe's payments innovations. \nDay after day, APIs must get better for their users. What’s great today is merely good tomorrow and eventually will not be enough. We don’t just want to push the limits of what’s possible with search, we’re designed from the ground up to do so. \nAll our resources are geared toward improving our API and making the life of developers easier, empowering them to deliver the most engaging experience for their users. Our core engineers handle support & documentation, as well as internally training the whole company on new features before they are released. Our marketing team focuses on educating the greater developer community on how search works and how Algolia works for them.\nBack to the future: what does “incredible” look like tomorrow?\nSearch must become an effortless conversation between user and product. Today’s search trends are rooted in conversation, and ultimately Algolia’s mission is to connect intent with content in the most frictionless way possible. Search and conversation are ultimately rooted in the same three principles - speed, relevance and user experience. \nWe have invested in building the essential building blocks for creating advanced search experiences, often very nuanced, like search for facet values, a highly valuable feature of search for ecommerce sites and social networks, which end-users ultimately will never notice (unless it’s missing when they need it). As we forge ahead, these building blocks will form the foundation for seamless conversation between user and software. \nWe will continue to make investments in leveraging the advantages of being a hosted API to deliver innovations in search, not only making it possible to have consumer-grade search previously… reserved only for the titans of tech - Microsoft, Facebook, Amazon, Apple, and Google - but also by bringing search innovations to the public that will redefine search itself.  \nWe will also continue to improve the developer experience, striving to simultaneously lower the barriers to entry for any developer or product team to achieve consumer-grade search, and to raise the bar for what’s possible with search by deploying new innovations across every API client.\nYour success is our success, and vice versa.\nThe best way to build a community is to be one, and Algolia owes a lot to a lot of different people. To our customers, we owe our ability to grow as a company. To the developer community, without whom Algolia would never have been able to advance and improve so quickly. To consumers, whose expectations for excellence have laid down the foundation for Algolia to succeed. \nWe're on a mission to connect intent with content, so let's get to work!"
  },
  {
    "id": "6367-0",
    "title": "Building for developers—a tour of new features and resources in 2017",
    "author": "Josh Dzielak",
    "text": "New API features, libraries, dashboard upgrades and community tools have been rolling out this year at a steady pace. In case you missed anything, I'm here to give you a panoramic look at what Algolia has made available to developers since the year began. Milliseconds matter, so let's go!\nNew API features\nSearch for facet values\nIn March, we announced search for facet values. Faceting is an essential element of search but sometimes the number of facet values can be hundreds or thousands. A list of checkboxes would be way too long to handle that, but a search box is just the right size.\n\nSearch for facet values requires no additional index configuration by the developer, just add the InstantSearch widget to your UI. To learn more check out this video with Solutions Engineer Alexandre Collin, recorded at our React-themed Search Party in January.\nAlgolia Vault\nDevelopers who work in sensitive fields like government and healthcare have additional compliance requirements. Algolia Vault is a way to address these needs in a convenient but highly secure way. Send us an email to schedule a demo.\nLibraries\nInstantSearch React\nSince the launch of InstantSearch React last December, we have continued to act fast on the feedback coming from the early adopters. The latest version, 4.0.0, lets you query multiple indices simultaneously and also connect an external autocomplete. We also created a new video that shows how to build a fully working search with React in less than 15 minutes.\nWe are very proud of our InstantSearch React engineer Marie-Laure Thuret, who was selected to give a talk about testing React UI components with React Storybook at ReactConf 2017.\n Algolia engineer Marie-Laure Thuret speaking at React Conf 2017\nInstantSearch Android\nInstantSearch Android is the newest member of the Instant Search family. Read the release announcement to learn about the challenges of building search on Android and how this library helps you address them.… If you’re an Android developer we’d love for you to try it out and share your feedback.\nInstantSearch v2\nThe original, framework-agnostic JavaScript InstantSearch has just kicked off a beta for v2. This represents a significant upgrade, adding among other things a connector-style API similar to React InstantSearch, which makes working with components much more flexible.\nAlgolia Offline\nIn February we announced the release of Algolia Offline, a solution for providing a mobile search experience even where there is little or no network connectivity. This is available today for both Android and iOS. We’re very happy to count great applications like AllTrails in the community of early Algolia offline users - read here about how AllTrails uses Algolia offline.\nUnifying Algolia iOS\nWe now support both Swift and Objective-C API clients with the same codebase, written entirely in Swift. Read more about the journey to supporting two languages done by our engineer Clément Le Provost, who deserves a big hand for all of his contributions on mobile.\nDocumentation\nIf code is the heart of the developer experience, then documentation is the soul. We understand how critical accurate, complete documentation is to our developers. In the last year, we have done one major reorganization on algolia.com/docs and we are taking all of your feedback into account as we prepare for another big update soon.\nPlease continue to share your feedback by clicking on the icon in the top right-hand corner of each documentation panel.\n\nDashboard\nThe Algolia dashboard is the go-to destination for configuring relevance, locating API keys, inviting team members and much more.\nMore intuitive ranking formula interface\nConfiguring and fine-tuning relevancy is essential to shipping a successful search. We’re progressively trying to make it easier to understand and debug without taking away flexibility. In the last month, we released an enhanced Ranking Formula component which… gives alerts about relevancy issues and suggests optimizations.\n\nFaster record explorer\nWe released a faster, cleaner record explorer built with our own library, InstantSearch React. The record explorer lets you explore the data you've uploaded inside of the Algolia dashboard, and includes the ability to facet and narrow down the search by all filter types.\n\nCommunity\nCommunity forum\nOfficially released in December, the Algolia community forum has grown to over 600 users and now averages over 40 new posts per day. The forum is the official place for our developers to ask questions, get help and exchange useful information about search.\nThe forum is also the home of the Show & Tell category, where we invite you to share projects that you've built with Algolia. We always love to see what you build. Upon request, we’re happy to provide feedback or help you promote your project.\nCode of Conduct\nThe Algolia Community Code of Conduct is live and applies to all of the Algolia community. We are committed to providing a welcoming, harassment-free space at all of our events and online spaces. We want to extend our sincere gratitude to Keen IO, LadyNerds and LGBTQ in Technology - our code of conduct draws heavily on their efforts.\nNewly-designed open source site\nOur site for helping you discover open source Algolia libraries, community.algolia.com, has been updated to match our new design and it now has a search! This is just the first in a series of improvements to come. We’ve heard your feedback that you’d like a clearer path to submitting projects to be included - expect a better way to do that soon.\nSearch Party\nSearch Party is a meetup series for exploring the possibilities of what search can be used for and used with. So far, we've had a Search Party in four locations around the world, and next up is June 22 in Paris.\nFor each Search Party we also invite speakers to talk about broader technical topics that our developers are also interested… in. In January, Preact creator Jason Miller joined us in San Francisco to speak about performance and the story of Preact, a 3KB drop-in React replacement.\n\nJason Miller, creator of Preact, presents at Search Party\nYarn package search\nJust before the holidays, we continued our tradition of making a yearly contribution to the community by releasing an Algolia-powered package search for Yarn. Then, in March, our intern (and now employee) Haroen Viaene made it easer to find by putting a search bar at the top of every page. But that wasn’t all - at the end of April we added new package detail pages that show popularity, contributors and file trees from unpkg.com. Here's an example of the detail page for React.\n\nDocSearch\nDocSearch is now used by 250+ open source documentation sites and API portals. We’re thrilled to have Stripe in the DocSearch family and to help developers get even more out of Stripe's amazing docs. We're also very excited to power search for webpack, a team doing great work that we are excited to also support with an Open Collective sponsorship.\nSwag - a new take\nLast month at Twilio Signal we tried something new with swag. Instead of giving out bulky physical items, we donated $10 to Women Who Code for every badge we scanned. All in all, that represents over $3,000, an investment we’re thrilled to make in an organization that connects over 100,000 women working in technical careers globally. Anyone who donated received a special occasion sticker to show off on their shirt or conference badge.\n\nDevelopers serving developers\nAlgolia only exists because of our customers, many of whom are are developers. The way we organize the company is with this in mind.\nWe now have a dedicated Developer Experience squad as part of our engineering organization. This squad is responsible for documentation, guides, tutorials and building tools and integrations that support our developers and customers.\nWe also have a Developer Advocacy team… that now has three full-time developer advocates in San Francisco, Portland and Paris. We expect to add several more advocates to the team this year - yes, we’re hiring! \nBeyond these teams, the majority of our engineers actively contribute to the community. This has been a tradition since the beginning of Algolia, which we make sure continues today with our culture-defined content strategy and our Speaker Program, which provides a 2-day public speaking workshop and CFP assistance to anyone in the company who wants to get up on stage.\nThank you!\nLast but never least, I want to say thank you to all of the developers who are choosing Algolia to build their search. We know that search is one of the most critical features of your application and we will continue to work hard to fulfill your expectations - to be open and transparent about our technology, our roadmap and our goals as a company. Let’s continue to build great things together!"
  },
  {
    "id": "6344-0",
    "title": "Supercharging WordPress so that everyone can have great search!",
    "author": "Raymond Rutjes",
    "text": "Last year, we released our WordPress plugin, allowing WordPress users to super power their search bars with an instant-search autocomplete dropdown menu.\nOur focus since the beginning has been to offer non-technical users an easy way to enhance the search experience for their end users. We also wanted to keep as much flexibility for WordPress developers to be able to hook into the plugin to customize the behavior.\nWhat we’ve learned\nWe already knew that making a plugin simple to use is a real challenge. This is even truer when trying to interface with a search engine such as Algolia which in fine has a lot of tunable options.\nTo make it as easy to use as possible, we ship our integration with default settings for the different search indices, and handle the indexing and synchronizing of content for the users behind the scenes.\nOne thing we learned is that by addressing both developers (by making the plugin extensible) and non-technical users, is that:\n\nTechnical users always want to go a step further than the API you expose\nNon-technical users always want the same than technical users but they want to be able to achieve it via a graphical user interface\n\nHere is what we did to lower the barrier\nTo give more power to our users, we decided to put more focus on our plugin configuration interface.\nIn the latest release, ordering of post types can be done by drag &amp; drop. Header labels can be overwritten using a search input without writing a single line of code and the Algolia powered by logo can also be disabled by ticking-off a checkbox.\nA complete changelog or the latest changes can be found on the plugin’s repository.\nAddressing the technical issues\nBy listening to our users, we realized that most of the reported technical issues were related to our “smart” way of queueing indexing tasks.\nIndeed, until this week’s release, we were making an extensive usage of custom post types and using remote HTTP calls to process the queued indexing tasks.\nThis… mechanism, even if in theory looked like to be an ideal solution, appeared to be the main bottleneck of our search plugin.\nThanks to the whole WordPress community, we managed to take out the complexity of the plugin and we now offer a way simpler, consistent and pragmatic solution to index content.\nAs a result, indexing is faster, more scalable and easier to debug and reason about.\nAlso, one big request we had, was to let users tune the relevancy directly from the Algolia dashboard. Until now, this was not recommended because on every new re-indexing process you would loose your settings and|or synonyms.\nWe took this release as an opportunity to meet users expectations, and ranking and synonyms can now be customized directly from the Algolia dashboard without fear of being lost again.\nUpgrade to the latest version of the plugin\nWe refactored about 3k lines of code for the v2 release, yet we tried to keep as much backward compatibility as possible.\nGiven we simplified the way we split records and store them, you should re-index all indices once the plugin has been updated.\nIf you previously customized your frontend search templates, you might need to compare your implementation with the new ones to adapt them.\nHow to contribute\nThe community has played a major role in the evolution of the plugin, and we would like to say how grateful we are.\nFor anyone who would like to participate, we now have a community forum where you can share your project or website to give it some visibility.\nFor developers, pull requests are very welcome.\nNo matter if you are a WordPress user or a developer, all your feedback and contributions are highly appreciated.\nYou can check out the latest version of the Algolia plugin for WordPress on the official plugin directory."
  },
  {
    "id": "6290-0",
    "title": "Mobile Search UX – Part Two: Deconstructing Mobile Search",
    "author": "Raphael Terrier",
    "text": "In part 1 I made the case that search UX on mobile is a pretty complex topic, from limited screen real estate, to mobility and pesky connectivity - there are a lot of traps that need to be avoided. In this post, I’ll demonstrate exactly how we can avoid these traps by taking an analytical approach to UX. We’ll deconstruct all the moving parts that comprise Mobile Search UX, and further down the road we’ll see how we can act and be impactful on mobile.\nShow Me That Text Input!\nStep one is simple but crucial: you’ll need to decide where in your app your users will get introduced to search. There are, for the most part, three options: Full search bar, Tab bar, Icon.\n\n\nFull Search Bar\nThe full search bar approach typically appears on the main screen, giving instant access to the search feature. If your product or app revolves around giving your users quick and easy access to a lot of content, then it’s the way to go. Search becomes the center of interaction to your content that would typically populate the page below in an “as you type” fashion.\n\n\nTab Bar\nWhen search is not the main access point of your content but you still want to give users a easy access to search throughout the app then a tab bar icon is the way to go. Spotify favors suggestions of albums, playlists or songs on the home screen, and when I want a specific tune I have quick access to the search screen with the tab bar icon no matter where I am in the app.\n\n\nIcon\nThe contextual search icon is among the least intrusive options. It’s easily the most flexible approach - you can display the button only in determined parts of the app that make sense for searching. For Gmail, the main use case is browsing unread mails in the inbox. Then, by order of importance in the UI, creating a new mail would come second with the large icon on the bottom right corner. Finally the search icon in the navigation bar on the top right corner enables users to search through  all my emails.\n\n\nSearch… Screen\n\nOnce you’ve determined where search starts for the user, the next UI element to analyze is the search screen. This element is mainly a  transition between the main ‘non-search’ view and the result page that we’ll describe right after. The search screen is displayed as soon as the user taps the text input search field or the search icon. This action already shows an intent -”I’m looking for something-” and that should be rewarded with useful information. As we saw earlier, each tap is painful, so we must gratify each effort. Instead of a default empty table-view waiting for the user’s query to fetch and display content, let’s guide them!Here are a few options:\n\nRecent searches enables fast back and forth between new searches and previous ones. When a user searches for a product, it’s handy to give a quick glance at previous searches for comparison.\nTrending searches guides the user by showing what other users are most looking for in your app. This helps an uninspired customer to discover your content easily by clicking a suggestion.\nCategories guide the user with a number of filter options to start with giving a scope of themes that are searchable, refining directly to a scope of interest and speed up the search process.\n\n\n\nResult Screen & Result Display Strategy\nThe final component of Search UX on mobile is the result screen. Here, there are two prevailing strategies: query suggestions & instant results. They both serve a specific purpose and I’ll explain where & when they are the most relevant. \nBefore diving in, it’s important to discuss an overarching concept to search results: the as-you-type search experience. Today’s users expect search to be instant, not only with respect to how fast results are fetched, but how frequently they are updated. Instead of waiting for the user to hit enter to to type a certain number of characters before displaying results, Algolia encourages product builders to display search results from the… first key stroke. Users today expect instant feedback from their inputs, hence “as-you-type”.\n\nQuery Suggestions\n\nTyping a query won’t immediately show results on the screen but instead a series of more complete queries based on the input. Let’s say I’ve typed “iphone” in the search box, the suggestions displayed will be more complete queries that are known on the product side to be popular or relevant for that given query. I’ll end up with a list of suggestions such as: “iphone case”, “iphone cable” etc…\nGiven that it is an extra step between the user and results, the perfect use case for query suggestions is searching through a huge data set - Amazon is a great example. The user might feel overwhelmed by the volume of possible results and not be sure where to search, what to search, what is available. Query suggestions are an effective way to guide the user and help him quickly access what they’re looking for in a large volume of possibilities by suggesting the complete query of what they had in mind. The added benefit is a much faster time to actual relevant content as the user gets relevant suggestions after only a few characters. \n\nThose query suggestions are most often based on popular queries, an analytics solution behind the search will keep track of what query resulted in a purchase, and with a high volume of uses an index of potential queries can be built.\n\nInstant Results\nAnother option is to fetch actual results with each keystroke and display them on the result page. This approach is most effective when the data set is either more limited or the user might have a more defined idea of what they’re looking for. In the Spotify example we clearly understand, because of the product’s nature, that search will be limited to the realm of albums, songs, playlists and artists. When looking for “beatles” having suggestions of albums containing “beatles” could be adding an unnecessary step between my query and the actual… result (e.g. The Beatles). When the scope of search is well defined and well-known by your users, providing instant search results is the shortest path between them and your content. One approach, as in our example, is to show a limited set of hits per type of content (5 artists, 5 bands etc…). The most relevant results will appear in the 5 firsts allowing the user to have a quick overview of matches on different types, and you can add a ‘show more’ button to allow the user to dive in by category.\nAn alternative of displaying limited results per type would be to have a segmented input right below the search box, enabling you to show more results of a specific type at once.\nFor this instant result strategy to provide good user experience, very fast response time is required, to allow results to be refreshed “instantly” at each keystroke.\n\n\nSuggestions + Instant Results Combination\n\nThe last option to consider is a combination of both strategies. When you have a very large dataset and want to guide your user but are also able to determine if a good match has occurred then mixing both can work. On the google play store we see an actual app result above multiple query suggestions. That’s a great strategy here, as there are millions of apps but it is also possible to be confident the query “airbnb” matches very well with the app airbnb, so displaying the app here has a great chance of being the result the user is looking for. \n\nThat wraps up part 2 of our mobile search UX series.  We’ve covered the most important components in mobile search today - From text input placement, to search screen and results. The decision for how you guide your user through the mobile search experience depends on a lot of factors - the size & complexity of your data set, the knowledge that you expect your user to have about the data set they are searching through, and your desire to balance discovery and intent in the search experience. With a good overview of the… structuring elements specific to search on mobile, we’re ready to go even deeper to provide best practices based on those components.\nBig thanks to Lucas Cerdan: this series relies a lot on the research into search and mobile UX that he has done at Algolia."
  },
  {
    "id": "6127-0",
    "title": "Mobile Search UX - Part One: 8 Obstacles, 1 Pocket-Sized Nightmare",
    "author": "Raphael Terrier",
    "text": "Editor’s note: this post is first in a three-part series on creating amazing mobile search experiences. Check back in the coming weeks for part 2 where we’ll deconstruct mobile search success elements and part 3, which will explore mobile search best practices.\nWhatever your app development stage — a brand new idea, iterating on a v2 or finally tackling that long-neglected feature — mobile search can be the decisive element of your app’s success. When executed correctly, search gives your users incredible access to all of your content. Done wrong, it will quickly cripple your app, making it a digital ball of stress and frustration. Before diving into how you can design successful mobile search, let’s analyze some of the pitfalls of mobile user experience (UX) that can make implementing search a real nightmare.\n1- Tiny Screen, Fat Fingers\nMost difficulties developing for the mobile platform come from the extreme limitation of space to display information on the user's screen. A common rookie developer mistake is wanting to display all the data at hand, similar to our desktop experiences. With mobile, more so than anywhere else, rule #1 is less is more.\n2- Every Single Tap Hurts (the User and Your App)\nWhen the time comes to transfer information from your brain to a digital device the most common input method today is the keyboard. You can usually count on having your ten chubby fingers to do this... unless you are using a mobile device. Then you have at most two thumbs available. On mobile, information output is dramatically reduced.\n3- Mobile Users Are, Well, Mobile\nWe use our mobile phones in less than optimal conditions — on a shaky train, on our bike at a red light, while walking (when we should probably pay attention to the intersection ahead). Those environments are not conducive to good focus and can lead to harder information input/processing on the user’s end.\n4- Network, or the Likely Absence of It\nWe take our phones everywhere and we want… them to work as well on the top of a mountain as from our bed. Coverage won’t be perfect, even in the busiest cities. Poor connectivity is extremely frustrating. As mobile developers, we must anticipate for the user being offline and plan accordingly.\n5- Information Overload\nLess is more... That’s great in theory but not that simple for our search use case. You’ll need more than a simple text input to create a powerful search experience. At a bare minimum, you’ll also need to display results. How many results you display and how much information you provide about each result is the difference between information overload and a successful search experience. Limiting the volume of displayed results on page means an even higher constraint on relevance — you can’t allow a single poor hit if you only show 5 total hits. The amount of information per hit should be kept to a minimum to fulfill two simple goals: is this hit relevant to the query? And is this hit interesting in itself?\nThen comes the trickiest part: filtering and refinement inputs. On a desktop, the sidebar provides a liberal amount of space for including plenty of filters and facets; however, when screen real-estate is precious and UI is touch-based, you have to think about the UI sliders/counters, and prioritize one attribute over another. Our mobile devices give users the impression of having access to everything at their fingertips; but you need to provide the tools to find a needle in your mobile app’s haystack of information. \n\n6- Search is an Active Process\nApps today are optimized to provide the most frictionless experience to users. We strive to minimize the amount of taps, scroll, and inputs. When you are simply consuming content in a semi-passive way, tapping a title, scrolling, reading an article or swiping through a few pictures, then a touch screen is perfect. Search isn’t that simple unfortunately. While the end goal is identical: providing relevant content with the least… amount of user effort, for search, a number of steps are needed at every query. The user has to first think of an appropriate word that represents what he/she’s looking for, type it letter by letter, evaluate the result and finally reformulate his/her query if results aren’t satisfying enough. In a few words, this process is far from a one tap action. We must serve relevant content with the least reformulations possible from the user.\n7- High User Expectations\nWe, users, have been spoiled by a few products without noticing it. For years, Google has been serving super relevant as you type query suggestions in a matter of milliseconds. We are so used to this functionality that we don’t even notice anymore, it’s part of a natural flow, day-to-day experience and we expect great suggestions at the first keystrokes. \nOn a different use case, Amazon completely revolutionized the way we shop. To give us access to their huge and constantly growing catalog, they implemented a blazing fast autosuggest dropdown menu in addition to powerful dynamic refinements. To provide what will be perceived as a great search experience, we must meet or exceed users expectations previously set by big players.\n8- Virtual Keyboard, Actual Typos\nVirtual keyboards are awful. They feel very uncomfortable to use and are prone to generate typing mistakes. From a mobile search UX standpoint, it means that without a typo-tolerant search engine that returns results at the first keystroke, you have an extremely high probability of frustrating users that will inevitably hit the wrong letter and get zero results. To ease user inputs and search process, we must be forgiving of typing mistakes and implement a typo-tolerant search solution.\nDeveloping a great user experience for your mobile app is a challenge in itself. Add search to the equation and things get even trickier. Yet, while implementing search on mobile is a demanding task, it is not mission impossible. There are definitely some great… examples to explore today and we will get some inspiration from them in the following posts.\nIn Part 2, we will deconstruct all the moving parts of a good search on mobile; this will give us a solid ground to finish by exploring the best practices in Part 3."
  },
  {
    "id": "6207-0",
    "title": "Keeping mobile apps lightweight: how we shrank Algolia Offline by 69%",
    "author": "Clément Le Provost",
    "text": "Earlier this year we announced the release of Algolia Offline, which compacted the power of Algolia down to an offline search experience while retaining most of its features. One of the biggest constraints of packaging a search engine into a mobile library is the “binary size”: how much space the compiled library occupies. Fitting an entire search engine under 1 MB was an exciting adventure!\nOn mobile, the binary size of your application is doubly important, because it impacts users twice: first when they download the app, and again by eating space on their device’s storage.\nOn iOS, Apple prevents users from downloading an application “Over The Air” (i.e. from a mobile network, not Wi-Fi) if it weighs over 100 MB compressed to prevent runaway fees on data connections—that limit used to be 50 MB prior to iOS 7.\nEven if you are below that threshold, many users are still reluctant to install apps if they are too big. Mobile storage is far from being the most affordable medium, and competition for space is fierce. Remember the last time your phone’s disk was full: probably the first thing you did was open up your application list and scroll through those taking the most space to look for apps you could afford to get rid of.\nAs a result, mobile developers strive to make their apps as streamlined as possible. This constraint naturally translates to all mobile libraries, including Algolia Offline.\nTrick #1: There are no magic tricks\nWhile Apple imposes restrictions on your applications, they also provide a useful tool to make them lighter for the end user: App Thinning. Introduced with iOS 9, App Thinning ensures that users only download the code and resources that are relevant for their device (hardware architecture and resolution). It became almost inevitable with the advent of Retina HD displays, which come with 3x resolution, meaning 9x as many pixels!\nUnfortunately, Android has yet to come out with a similar tool. Google Play has a feature called… “Multiple APK”, whereby you can publish different variations of your app for different device configurations. Contrary to App Thinning, however, it is entirely manual, making deployment a lot more cumbersome. The official documentation itself states that “[this] is not the normal procedure”.\nEven with App Thinning, binary size is still a concern: the problem has been mitigated, not eliminated.\nCloud and mobile: two different species\nBack in 2012, Algolia started as an offline SDK, so one could be forgiven for thinking that shipping a small library was easy enough for us. However, from 2013 on we switched to a cloud-hosted engine, and its rules are very different from mobile: it’s a different animal altogether.\nWith a cloud-hosted product, you control every part of your environment. You have big, fat servers with terabytes of disk: storage space is much less of an issue. Conversely, you have hundreds of thousands of customers to serve in milliseconds, so speed starts to be your main headache. It’s a well-known engineering problem: the space–time tradeoff. With a cloud-hosted product, the cursor is set firmly on “time”.\nIn addition, since our move to a cloud product, our search engine has been augmented with lots of new features. Each feature means more code, which means bigger binaries in the end.\nAs a result, when we resurrected the Offline SDK to become Algolia Offline, it weighed around 3 MB. The challenge was now to bring it down to around 1 MB.\nTrick #2: Trust your compiler\nWe didn’t have to reinvent the wheel to shrink our application by two thirds. Compilers have been around for decades… since the times when binary size was always an issue. Compilers provide lots of nice options to squeeze your code into the fewest possible bits.\nOur CTO Julien Lemoine posted a great article back in early 2013 explaining how to reduce binary size with the Android NDK. A lot of the tricks he mentioned still apply. Let’s review them.\nThe first obvious… step is to ask the compiler to optimize for space ( option, instead of the default ). In our case, this alone saves 419 kB.\nTrick #3: Resist the urge to inline\nNext, let’s be careful with inline expansion. Inlining code can bring significant performance benefits, not only because it saves a function call, but also because it makes other optimizations possible—optimizations which would have been impossible across a function boundary. As a consequence, though, the compiled size grows bigger, because the same code is duplicated in several places in your binary.\nAlgolia relies heavily on inlining, even forcing it for specific functions (typically those used within tight loops). In Algolia Offline, however, we disable this behavior and let the compiler decide. This saves another 144 kB.\nTricks #4–6: Exorcise that Dead Code\nOptimized code is nice, but what if it’s not actually used? When a linker produces a library, it merely takes every object file (i.e. the machine code equivalent of every source file), and links them together. But what if some portions of the files are not actually used? Dead code stripping is the answer. It removes from every object functions that are not useful to the library’s business. In our case, this saves 312 kB.\nThere’s a twist to dead code stripping in the case of a dynamic library. In a static library, you only want to strip private symbols that are not useful to public functions. You cannot know which of these functions will be used, because a static library is merely a collection of reusable pieces of compiled code. In a dynamic library, however, you know which public functions will be used: only the functions exported by the library! An important step is therefore to ensure that you only export the minimum viable set of functions to serve your purpose. By tightly controlling the exports, we save another 288 kB.\nWe can take dead code stripping one step further with Link-Time Optimization (LTO). By looking at the binary as a… whole, LTO can optimize away dead code across object file boundaries, which a regular linkage cannot do. It results in a more compact binary: 57 kB saved—at the cost of a significantly longer link phase during the build.\nTrick #7: Give the boot to bloat\nWe chose to ban the C++ standard I/O stream library from Algolia Offline, as it is still a major cause of bloat, especially on Android. We have compile- and link-time checks to make sure we don’t rely on it. The gain is far from negligible: one single, seemingly innocuous  in our code adds 144 kB to the binary size!\nIt’s worth noting that we now use exceptions. We avoided them in the past because they require some limited form of Run-Time Type Information (RTTI), so compilers used to trigger full RTTI support when exceptions were enabled… and RTTI causes binary bloat. Modern compilers are better optimized, so we can enjoy exceptions without paying the price of full RTTI.\nClang lagging behind on Android\nI am a huge fan of Clang, LLVM’s C/C++ compiler. LLVM is an awesome piece of software, one of the best surprises of the 2010 decade, when it started to get traction, in particular with Apple backing it.\nWe’ve been compiling Algolia with Clang on Linux for months—it resulted in both a non-negligible performance increase (5%) and more compact binaries.\nOddly enough, we are still stuck with GCC on Android! Although Clang is Android NDK’s default toolchain since r11 (latest version is r14), it still suffers from a number of drawbacks that are deterring us from relying on it at the moment. In particular, I had trouble making LTO work on our project with Clang—and without LTO, Clang cannot compete with GCC in terms of binary size.\nIt is worth noting that we already use Clang’s Standard Template Library (STL), though.\nCode is not everything\nHowever smart they are, compilers can only act on your code. But when you dissect our library, you will see that code does not account for the entire size of the… binary. A substantial part of it is occupied by data tables.\nHere, no compiler magic can save you; only proper software engineering can. (Remember that “Data Structures 101” course?)\nOur data falls into five broad categories: Unicode, HTML entities, stop words, plurals and CJK segmentation.\nPlurals and CJK segmentation are way too big to fit in a mobile library: 5 MB for segmentation and 48 MB for plurals. We had to discard those features altogether in Algolia Offline. (As you might have guessed, I did not include them in the initially advertised size of 3 MB…) With regard to the other three data tables, we found a way to make them more compact.\nThe main idea behind the compaction of our data tables is to transform random access structures into a big blob with an auxiliary index. A random access data structure is typically an array: all elements have the same size; therefore, computing the address of a given element is trivial (a multiplication). But if elements have different intrinsic sizes, you are forced to make them as big as the biggest element, which results in a lot of wasted space.\nIn a blob, by contrast, all elements are stored contiguously with no wasted space between them. This makes the structure more compact, but random access becomes impossible: computing the address of a given element is no longer easy. To solve that, we rely on an auxiliary structure (called an “index”, even if it has little to do with indices in Algolia’s engine) that contains the offset of each element in the BLOB. Because an offset is an integer and is typically much smaller than the actual element, this additional data structure doesn’t overcome the benefit of the blob.\nBy applying this principle to our data, we saved:\n\n96 kB on stop words\n124 kB on HTML entities—the structure used in SaaS is extremely sparse for very fast lookup\n476 kB on Unicode data\n\nOverall, the binary shrunk by 669 kB. Of course, we had to trade some CPU time for this: around 5% for… Unicode and HTML entities—a lot more (around +50%) for stop words, but they are accessed only a few times per query, so their performance doesn’t really matter.\nA streamlined library\nCombining all the above optimizations on both code and data brought the library back at around 1 MB per hardware architecture (with slight variations across architectures and platforms). While it might still sound big, it is actually an acceptable overhead in most use cases, especially these days where high-resolution images sometimes account for the major part of an application’s bundle.\nHere is a summary of our efforts:\n\nInitial size = 3,003 kB\nOptimize for space → -419 kB\nDon't force inlining → -144 kB\nDead code stripping → -312 kB\nControl exported symbols → -288 kB\nLink-Time Optimization → -57 kB\nRemove STL I/O streams → -144 kB\nCompact HTML entities data → -124 kB\nCompact stop words data → -96 kB\nCompact Unicode data → -476 kB\nResulting size = 943 kB\n\nThe above numbers are for the hardware architecture armeabi-v7a on Android but, despite small variations, they would be similar on iOS or for other hardware architectures.\nWe hope that these tips &amp; tricks will be useful for reducing the size of your mobile libraries or applications. There are always trade-offs to be made between speed and space, but with sensible compiler and linker settings, and carefully crafted data structures, you can achieve dramatic space savings without sacrificing much of execution speed.\nIn the end, Algolia Offline performs a typical search query under 50 ms on a mid-range device. That’s more than the cloud-hosted engine, of course; but thanks to the lack of any network transmission, the elapsed processing time is about the same. This makes instant search a reality on mobile—even offline."
  },
  {
    "id": "6163-0",
    "title": "Comparing Algolia and Elasticsearch For Consumer-Grade Search Part 2: Relevance Isn’t Luck",
    "author": "Josh Dzielak",
    "text": "A relevant search returns the best results first. Google’s famous “I’m feeling lucky” button is a standing invitation for users to test their relevance by jumping through directly to the top hit. It’s a bold gesture when you consider that Google’s index contains trillions of documents, yet it works. The top result is often highly relevant and the user ends up right where they wanted.\n“I’m feeling lucky.” - Average internet search user\nIf we want to understand what a consumer expects from their search, luck is a good place to start. If we want to understand what it takes to build that search, luck is a very bad place to start.\nGreat relevance is within reach for any search project, but it requires a careful understanding and application of complex techniques, several of which we'll look at in this article. Whether you're using Elasticsearch or Algolia, the goal should be to give your users a search they'll love, a search so good you'd even consider putting an \"I'm feeling lucky\" button next to it!\nWould I put an ‘I’m feeling lucky’ button next to my search box?\n\nIn Part 1 of this series, End-to-End Latency, we discussed the perfectly unreasonable search expectations of the average internet user, starting with speed. Global end-to-end response times must be lower than 100 milliseconds for search-as-you-type experiences to feel natural, with less than 50ms being ideal.\nIn this part of the series, Relevance Isn’t Luck, we’ll look first at consumer search behavior and why it challenges traditional approaches to relevance. Then we’ll look at how developers are adapting to these changes using the available tools, looking first at Elasticsearch and then Algolia.\nIntuitive user experiences require smarter relevance\nConsumer-quality search experiences are so streamlined that it's hard to imagine what's going on underneath, even for engineers. Yet many things that we take for granted as users have serious consequences for engineers trying to… achieve good relevance.\nSingle input\nIn the early days of the web, most searches required you to separate your query into different boxes, one for each field you wanted to search in. Today, this is only the case for advanced searches, with the irony being that it’s actually simpler for the search engine to find and rank records when the query is split up into different fields.\n\nInstant search\nSearch-as-you-type experiences have become the standard since Google Instant was introduced in 2010. Users love the immediate feedback and responsiveness of instant experiences. Product designers love instant search because it’s an efficient way for users to discover the content on their site.\n\nInstant search introduces a brand new set of relevance challenges. The search engine can’t wait for a full word to be typed, it has to find and rank matches using only the first few letters.\nInstant search vs. instant suggest\nNot all instant searches are created the same. A variation called instant suggest is a multi-step process. Each new keystroke produces suggestions for keywords but not actual search results. Search results can only be reached by clicking on a suggestion or hitting the enter key.\nInstant search produces real search results with every keystroke, no clicks or enter keys required. The user benefits from a streamlined experience and the development team has less moving parts to tune and maintain. But as we’ll see later, achieving the necessary speed and relevance is much harder for instant search than instant suggest.\nForgiving mistakes\nOn-the-go users misspell, omit and juxtapose words, and frequently get distracted and stop typing mid-sentence. Yet, when they gaze back down at their screen, they still expect to see the contact, movie or product they were searching for. This requires search engines to be much more forgiving than they were originally designed to be.\n\nMobile keyboards make forgiveness a must-have for two reasons: it’s easier to make mistakes and… it takes longer to correct them. To make the search engine more forgiving, we’ll need to apply a variety of text and language processing techniques, designed to handle everything from typos to languages like Dutch and German that dontalwayshavespacesbetweenwords.\nPromoting Results\nThousands of pages on the internet can tell you how to treat the common cold. How do you know which one to trust? Text-based signals aren’t enough—whether the page mentions the word cold 5 times or 50 times makes it no more or less trustworthy. To solve this problem for web search, Google created the PageRank algorithm which determines a page’s importance based on how many other pages link to it (and what those pages’ rankings are).\nMost search engines, including Elasticsearch and Algolia, provide a way to promote results based on signals that aren’t related to the text itself. High-level attributes like trustworthiness, popularity, and affordability are commonly distilled down to a set of numbers, which are then used to sort records.\nA particularly challenging part of relevance design is combining text-based relevance with strategies for promotion. The text represents the user’s intent while the promotion represents yours. These have to be considered in the right order.\nFor a great search UX, what the user is looking for must come before what you want them to find.\nRelevance is a journey, not a destination\nRelevance can be broken down into requirements, design, implementation, testing and maintenance phases. Now that we know what some of the requirements are, let’s jump into design and implementation, first with Elasticsearch and then with Algolia.\nAbout Elasticsearch\nIf you’re not familiar with Elasticsearch, you can think of it as a set of tools for building a broad range of search and analytics applications. At the core of Elasticsearch is Lucene, a widely-used open source search engine first released in 1999. Part of Lucene’s wide applicability lives in its ability… to apply very different similarity models to calculate relevance, including Okapi BM25 and TF-IDF, the new and former defaults used by Elasticsearch. In this sense, Lucene is a swiss-army knife, not a machete.\n“Regardless of how easy Elasticsearch has made it, Lucene-based search is a framework, not a solution.” —Doug Turnbull, author of Relevant Search, June 2016\nThe job of a relevance engineer is to choose tools from the framework for the application in question. For consumer-grade search, tools are needed to handle:\n\nMulti-field matching - Testing each query word against each searchable field\nPrefix matching - Matching word parts for as-you-type search\nForgiveness - handling typos, concatenations, plurals, omissions\nPromotion of records based on non-textual signals like price or popularity\nA predictable, debuggable way to combine and rank all of these signals\n\nAs we proceed, we must keep an eye on how each new layer impacts the relevance and the performance of the search as a whole. Like with most things in software, getting each component to work is easy. The hard part is making them all work together.\nPoor applicability\nThere are a few features of Elasticsearch that should be ruled out early on, even if they look promising at first glance. This isn’t because they’re bad features, but because they’re not going to be powerful or fast enough for what we want to achieve.\nFuzzy queries\nFuzzy queries allow forgiveness for typos and omissions, but for search-as-you-type they are simply too slow. Strategies to speed them up, like increasing prefix_length or reducing max_expansions, will cause other problems. In the case of setting prefix_length to 3 or more, we lose the ability to correct typos in the first 3 characters of the word.\nThere are also relevance concerns. There is no straightforward way to rank exact matches above fuzzy matches - the match query gives all fuzzy matches a constant score of 1, regardless of how inexact the query was. This has the… consequence of making some records unreachable, even by exact queries, when typo-free variants are scored higher for other reasons. Fuzzy matching doesn’t work with phrase matching, and it also can’t be used with cross_fields in multi_match queries, which prevents us from specifying field-specific boosts.\nPrefix, match_phrase, and match_phrase_prefix queries\nPrefix queries allow you to match the first letters of a word but don’t handle phrases, where at least one full word has already been typed. match_phrase only works with whole words. match_phrase_prefix does work with prefixes but on the last world only, and it fails to find matches when the prefix is short (1-2 letters) and the index is large. For this reason, Elasticsearch advises to avoid it.\nTF-IDF\nIf the fields you’re searching tend to be short—names, titles, descriptions, sentences or short paragraphs—you probably want to turn off TF-IDF/BM25 altogether. Term frequency (TF) won’t be very helpful, as several occurrences of the same word in a short span isn’t usually a meaningful relevance signal. Inverse-document frequency (IDF) can unfairly penalize high-signal terms simply because they’re popular, like “Plato” will be in a search for philosophy book titles. You can read a very detailed explanation of this phenomenon from Doug Turnbull and the really sharp team at OpenSource Connections called Title Search: when relevancy is only skin deep.\nThere’s one other general advantage with not using IDF, which is that the relevance of a single document is self-contained. A new influx of documents with previously unencountered proportions of keywords can’t cause a relevance regression in the documents that were already present.\nHigher applicability\nWhile the previous Elasticsearch concepts might not prove that useful in the end for consumer-grade relevance, the next few can add more value, provided that we understand their limitations.\nminimum_should_match\nThis important configuration knob… lets a query return results even if some query words aren’t in the record. For example, if the user searches for “my quick brown fox”, the query can return a record that only contains “fox” but neither of the other 2 terms, if the minimum_should_match is 25% or lower. This scenario is especially important in instant search where not all query words are present until the end (or perhaps never at all).\nA minimum_should_match value that is too high will exclude too many potentially good results, leading to zero-result scenarios more often. If the value was 50% in the example above, the “fox” result would not have been returned. A value that is too low will cause irrelevant results to appear. At 25%, the above query would also return results where “my” was the only word that matched. Finding the right value for this parameter is important, and requires you to understand both your data and the way that your users are searching.\nCompletion Suggester\nThe Completion Suggester can be used to create an instant suggest experience, as described above. It cannot be used to create a full-record instant search experience, as suggesters are limited to searching in only one field. Still, instant suggest has advantages that make it a popular choice, namely that it’s fast. Suggesters have a completely different in-memory execution path which helps make them fast enough for as-you-type search.\nOn the relevance side, suggesters have weighting and can do fuzzy matching. On the other hand, clients have to de-duplicate data and advanced features like filtering and faceting can’t be used. Read more about the pros and cons here: Quick and Dirty Autocomplete with Elasticsearch Completion Suggest.\nn-grams\nOne of the most versatile tools in the Elasticsearch toolbox is n-grams, where words are broken down and indexed as smaller groups of letters. The “brown” would be indexed with the tokens [“b”, “br”, “bro”, “brow”, “brown”, “r”, “ro”,… “row”, “rown”, “o”, “ow”, “own”, “w”, “wn” and “n”]. \nA subset of n-grams called edge n-grams are a better choice for instant search, because all tokens start from the beginning of the word and because there will less tokens overall, improving the speed of the search. The edge n-grams for “brown” are just [“b”, “br”, “bro”, “brow”, “brown”]. With n-grams a query for “row” would match “brown”. This is called infix matching and it usually does more harm than good for relevance—another good bad example is a search for “nathan” matching “jonathan”. Edge n-grams only afford prefix matching, so “row” will only match “rower” or “rowboat”, not “arrow”.\nSimilar to the limitation that fuzzy matching has with ranking exact above inexact matches, n-grams don’t come with a way to rank full word matches higher than partial ones. N-grams don’t store information about whether the token represents a full word. Without that, we can’t guarantee that “motor” will be ranked above “motorbike” for the query “motor”.\nShingles\nAnother limitation of n-grams is not being able to use them for phrase matching, where we want to giving ranking preference to records where the terms match in the right order. A name search for “anne marie” should rank results for “marie anne” after “anne marie”. This is very important when there are many people named “marie anne”, because the user may never be able to reach the “anne marie” record they were looking for otherwise. N-grams don’t store the position or order of tokens, so it’s impossible to satisfy this ranking requirement with n-grams alone.\nShingles are often used instead of (or in concert with) n-grams because they do retain information about word proximities. Shingles are similar to n-grams in that they represent tokens, but instead of breaking up words into letters, they break up phrases into words. The shingled tokens… of “quick brown fox” are [“quick brown”, “brown fox”].\nFor search-as-you-type relevance, shingles will have to be used alongside n-grams because shingles won’t help us do prefix matching. An Elasticsearch implementation using both techniques is referred to here by DogDogFish. However, the increase in index size and amount of scanning at query time make the combination of shingles and ngrams almost certainly too slow for full instant search. Presumably for that reason, and also to handle typos, the DogDogFish team ended up with a mix of instant suggest for keywords, instant suggest for spelling correction, followed by keyword search to find results.\n“If there is a particular search found to yield bad results, it can be easy to optimise towards improving and fixing that search, but then other searches end up suffering as a result.”—DogDogFish\nDelivering relevance with Algolia\nUnlike Elasticsearch, Algolia is not a swiss-army knife for implementing a myriad of front-end and back-end search use cases. But nor is it a single blade capable of only one kind of search—the API is highly customizable and individual search implementations vary widely by data, user interface and audience.\nA good way to think about Algolia is a set of wrenches, each a different shape or size, purpose-built to tighten a specific nut or bolt of a fully instant consumer-grade search.\n\nMulti-field\nAlgolia is designed to index structured data, not large blocks of text, so searching in multiple fields of a record is the default behavior. The list of fields to be searched is specified in the searchableAttributes property of the index. It can be changed at any time in the Algolia Dashboard or API.\nThe list of searchable attributes is ordered, with the most important attributes being first. The ordering is considered at query time by the ranking formula, which is the foundation of how Algolia computes relevance. Algolia’s ranking formula contains a handful of different criteria,… each meant to handle a specific relevance need. For multi-field-aware ranking, matches in some fields need to count more than others. This is done by the formula’s attribute criteria.\nUnlike Elasticsearch, there are no term or match queries that operate only against single fields; conceptually everything is multi_match. Likewise, there are no operators like bool or dis_max to combine relevance signals from multiple fields— this is handled by the ranking formula itself.\nThe ranking formula, including the attribute criteria, doesn’t require developers to specify how much more important one criteria (or one attribute) is than another, only that it is simply more important. In contrast to Lucene-style boosting, no coefficients need to be provided.\nAlgolia uses a tie-breaking algorithm, not the computation of a score, to rank documents. The algorithm considers each criteria, one after the other, to successively sort the result set into parts. Read more here about the tie-breaking algorithm and ranking criteria.\nPrefix matching\nFor the query “sun”, exact matches like “sun” need to be ranked above partial matches like “sunflower”. Otherwise, records about the sun will be impossible to reach if there are many records about sunflowers—there is no way for the user to be more precise than “sun” when they are looking for records about the sun.\nShowing a sunflower to a consumer when they wanted the sun will cause a ton of frustration, so much that we want to guarantee it won’t happen. The exact criteria in the Algolia ranking formula makes sure that full-word matches count more than partial-word matches. The criteria not only can rank one prefix against one word, but it also combinations of multiple full words and multiple prefixes.\nForgiveness\nThe typo criteria in the ranking formula guarantees that exact matches for query words count more than matches where this a mistake. The reasoning is because sometimes a typo is not actually a typo, it just… depends on what the underlying data is.\nTake a query for “jon”. This is either an exact match for “Jon” or a 1-letter omission typo for “John” or “Joan”. The search engine needs to guarantee that people named “Jon” appear before “John”, while still showing “John” and “Joan” afterwards. The “jon” query is simultaneously a typo for some records and not a typo for others. The records for which it is not a typo need to come first.\nBoth Algolia and Elasticsearch use the Damerau–Levenshtein distance algorithm for computing typos based on word distance. Spelling mistakes, however, are just a small fraction of dealing with the reality that it is a human in front of the search box, and that humans speak different languages and phrase things in different ways.\nIn Elasticsearch, search teams add analyzers to the indexing path or the query path to handle the quirks of language, and there are quite a few to choose from. With Algolia, queries are put through a two-step tokenization and alternative-word matching process that handles many common cases for a variety of different languages. For a deep dive, check out the Query Processing edition of the Inside the Engine series, but here are a few examples:\n\nAbbreviations and apostrophes: In French, “l’aigle” (the eagle) needs to be indexed as aigle not laigle - simply removing the apostrophe will produce very bad relevance. But if indexing English, “don’t” should be just one token “dont”.\nAgglutinative languages: Some languages like Japanese, Chinese and Korean don’t have word separators. A statistical approach is required to break words into tokens. Here, typo tolerance is not based on word distance but on synonyms between ideograms, so that you can search for simplified Chinese using traditional Chinese and vice versa.\nPrefix correction: Beyond Damerau–Levenshtein, Algolia keeps a dictionary of words that can be used to correct prefixes. If the user inputs “jhon” the… prefix will be corrected to “john” automatically, since “john” will have been in the dictionary. This situation is often overlooked but is essential to a good instant search experience. Mistakes are very common in the first few characters of a search, as we all know from using our mobile devices.\n\nTyposquatting\nWhen you mistype a domain name and end up on an ugly “Domain For Sale” page, you’ve been a victim of typosquatting. Domains aren’t the only place typosquatting happens. It’s also common on social networks, where it takes advantage of the search’s preference for exact matches. @BarakObama on Twitter, who is not @BarackObama, has 16.3k followers. To prevent typosquatters from getting to the top of the search results, we need to do something that is the opposite of what we usually recommend - we need to consider another signal to be more important than the text. Because the elements of Algolia’s ranking formula can be reordered, this is easy. The recommended solution is to add an isPopular attribute to the data, set it to true for the top ~1% of accounts, and put it above the typo criteria in the ranking formula.\nPromotion\nMulti-field matching, prefix matching, forgiving mistakes and understanding the intricacies of language all fall under an umbrella that Algolia calls textual relevance. Textual relevance is all about what the user wants to find.\nBusiness relevance is about what you want the user to find. When your index contains textually identical people, products or items, Algolia gives you the custom ranking to push the most important records to the top. A custom ranking might be driven by business metrics, popularity metrics, or attributes intrinsic to the record, like price. As with textual relevance, your Algolia custom ranking can take into account many signals, while still giving you a guarantee about the order in which they matter.\nBy default, the ranking formula applies textual criteria first and only then the custom ranking.… This provides a guarantee that promotional signals will not override the textual quality of the match, which is one of the most common situations that lead to bad relevance and a bad user experience.\nConclusion\nIn Part 1 of this series, we looked at how instant search forces us to think about latency in a new way. The same is true for relevance. We can no longer search by full words only nor assume the query has made it accurately from the user's head into the search box.\nThe path to great relevance looks different whether you are using Elasticsearch or Algolia. Because relevance and user experience are inextricably linked, the type of user experience you will end up providing may also be different. Recent Elasticsearch additions like the Completion Suggester are designed to help developers build an instant suggest experience that is fast enough for as-you-type search, with a tradeoff of some functionality. For full instant search, Elasticsearch users should expect to make trade-offs between speed, relevance and how predictive or forgiving the user experience is.\nAlgolia is built from the ground-up for full instant search. Tradeoffs center around different ways to model data and finding the most cost-effective ways to integrate, but not around speed, relevance or user experience.\nRead other parts of the Comparing Algolia and Elasticsearch for Consumer-Grade Search series:\n\nPart 1 - End-to-end Latency\nPart 2 - Relevance Isn't Luck\nPart 3 - Developer Experience and UX\n\n\n\nWant to learn more?\nJoin us for our next webinar to learn how you can achieve fast and relevant results from your search implementation.\nWebinar: Achieving Speed and Relevance in Search\n Sign Up\n "
  },
  {
    "id": "6189-0",
    "title": "Small but impactful search UX tips for mobile web",
    "author": "Haroen Viaene",
    "text": "As part of our annual tradition of giving a gift to the community, these past few weeks I was part of the team working to implement search inside of Yarn. During that time, I noticed some small quick-wins that could make our search boxes better, and today I’d like to share a few of them.\nUser experience is important at Algolia - both the developers who use our API and the end-users who use the products they build - and so we look a lot at how the tools we provide to developers can make it easy to create great UX.\nOne of our most recent projects, react-instantsearch, comes with many optimizations and we’ll highlight a few of them in this post.\nSubmit UX\nWe use  because it allows us to take advantage of semantics for screen readers that tell the user it’s a search input, and it will also show a ? (magnifying glass icon) as the return button on Android. In Chrome and Safari this has more advantages, namely that it shows a search button on one side, and a clear button on the opposite side.\nTo get that functionality for everyone, react-instantsearch wraps the search box in a form, and includes two extra buttons: one with type reset to clear the input, and another with type submit that will be used if “search as you type” has been deactivated -we hide the latter with some css to avoid duplicating functionality.\nSee the Pen Default React InstantSearch layout by Algoliamanager (@Algolia) on CodePen.\n\nWe only want to submit the form when a user types something, so we will put the “required” attribute on the input. That prevents a user from submitting an empty search. However the default result isn’t too beautiful:\nA required input\nWe can avoid this message showing up, but still avoid the submitting of the form, by adding the “novalidate” attribute to the form.\nSee the Pen input type text vs input type search by Algoliamanager (@Algolia) on CodePen.\n\nButton UX\nNext, we can go further in improving the search box itself. On mobile, the submit button… becomes more prominent, since it’s visible as the return key (on both iOS and Android). We would love to have something that has to do with search to be displayed there instead of return. On Android we already solved this, just by using input type=search, but iOS still shows “return”, because iOS requires the form to have an action. Search is available on the page we’re on, so we can leave the action attribute empty, and then it means the current page. This will cause the submit button to read “search” instead, and it will be translated to the language of the keyboard you’re using.\nYou can play around with the differences on all browsers in this codepen:\nSee the Pen input type text vs input type search + forms by Algolia (@Algolia) on CodePen.\n\nDon’t forget to Blur\nDon’t fret, we aren’t done with the search button yet. If you have a button that says search, you expect the search results to appear. Because we show the results in realtime, while you’re typing, we don’t need to calculate the results at this point, so pressing search now does nothing at all for users. We fix this by\nblurring (which is the opposite of focusing) the search input. The keyboard will then be dismissed when you hit search, and you can see all of the results.\nNo need for Autocorrect\nBecause Algolia is typo-tolerant, mobile-specific features like autocorrect become a bit obsolete. We don’t want to spend time undoing the suggestions by our browser to suggest other words, especially if you’re in a case like Yarn, where a lot of the packages have unique spelling that we don’t want to be corrected.\nTo disable autocorrect, you’ll need three attributes — , , and  — the latter avoids starting a search with a capital letter all the time. With those improvements, a search on a mobile device will look like this:\nThe situation before these improvements\n \nThe situation after these improvements\nA last thing that has been added is the “role” added to the form. Since… December of 2015 , with html-aria#18, it’s valid html5 to add an additional role to a form, and in this case a search role makes a lot of sense. In screen readers that support it, the search input will be read as a search form with inputs, and that’s exactly what we want.\nWith all those things  implemented in react-instantsearch in\ninstantsearch.js#1999 and instantsearch.js#2046, we get a search form that looks like this:\nSee the Pen React InstantSearch SearchBox by Algoliamanager (@Algolia) on CodePen.\n\nYou can get a search box like this one by using react-instantsearch to make your search experience, or apply this knowledge yourself.\nWhen making the search for Yarn a few other things came to my attention —I’d like to thank James Kyle a lot, because he kept finding new things to give me feedback on. One of these is stylistic, and that is to always make sure your search input stands out from the background. You can do that by making sure it has a white (or light) background, a placeholder that’s not too light and not too dark. You should add an identifiable  icon (for example ?) to get extra clarity, and it’s also good practice to give a special border to active inputs. My colleague Kevin Granger made a really cool webapp that allows you to create all kinds of search boxes that follow these criteria at http://shipow.github.io/searchbox.\nOpenSearch\nOpenSearch in action on the Yarn site\nAnother thing I learned about while making search for Yarn is the OpenSearch spec. It was developed by a9 (which is a subsidiary of Amazon) a long time ago, and deals with a lot of browser and search functionality. You can make your whole search available in the dropdown that comes up when you use “search in site”. Making your search available in the rss format that opensearch expects isn’t what Algolia is made for right now, but we can make browsers handle the “search in site” feature. To do that we have to add a  to the  of every page. In that we add a  and ,… a  is also added to show up in the UI of browsers correctly. The /opensearch.xml file should contain something like this:\n\n\n\nYou can read more about the implementation of OpenSearch in Chrome,\nFirefox, Safari, Internet Explorer, Yandex and in general.\nThe Devil is in the Details\nI enjoyed discovering these quick fixes that get a search experience from good to great, and hope they help you create beautiful search for your users. If you have any questions about the tips above, or have UX tips of your own, we’d love to discuss this more with you on the Algolia forum."
  },
  {
    "id": "6150-0",
    "title": "A Tale Of Two Languages: Supporting Swift & Objective-C",
    "author": "Clément Le Provost",
    "text": "Our road to supporting both official iOS languages with the same code base\nSince 2014, the Apple ecosystem has had two official programming languages: Objective-C and Swift. For library providers like Algolia, supporting both languages is a must—ideally from the same code base. The traditional approach is to support Swift via Objective-C. Despite the fact that it is much less frequently done the other way round, we chose to travel off the beaten path and have our entire code base written in Swift. \nIt has been a challenging adventure—but also a rewarding journey. Let’s see why.\nObjective-C has been the main programming language for Apple platforms since the release of Mac OS X. It was actually imported along with NeXTSTEP during Steve Jobs’ spectacular comeback. Before that, the official programming languages for Mac OS were C/C++, and even Pascal at one time.\nWhile Objective-C’s syntax was inspired by Smalltalk—one of the first purely object-oriented languages—its goals and implementation were more pragmatic. Objective-C is mostly a preprocessor on top of C, combined with a thin runtime. In other words, it’s an object-oriented, orthogonal extension of C. (This is why C++ can also be extended in a similar fashion, leading to the three-legged beast that is Objective-C++… but let’s keep away from horror stories for this article.)\nWhile Objective-C has been criticized by many, mainly for its unusual syntax, it is an elegant programming language. It’s highly dynamic, making difficult software engineering problems like proxying or mocking a breeze. It has a very clean object model, with classes and metaclasses. Though not fashionable any longer, it remains a remarkable piece of software engineering.\nA leap into the future\nYou must be wondering: if Objective-C is such a great language, why did Apple feel the need to move away from it?\nEven though Objective-C has evolved over the years, adding modern constructs like closures (called “blocks” in… Apple parlance) and generics (more like type annotations, actually, but let’s not be picky), it still suffers from a few shortcomings.\nIts main drawback is its lack of compile-time checks. The flipside of being a dynamic language is that many things happen at runtime, making it harder for the compiler to detect potential bugs. Also, because every function call translates in a call to the runtime, performance remains intrinsically limited.\nAnd, of course, there is that unfamiliar syntax that can deter newcomers. I actually suspect that this is the main reason why Apple moved away. Not the flaws of the language in itself, but the fear that programmers would steer away from their platforms because they didn’t feel comfortable writing code for them.\nSwift: powerful but complex\nSo, what is Swift all about?\nSwift brings a seemingly familiar, curly-brace touch to Apple programming. Looks can be deceiving, though, because it ships with many modern notions not supported by other “curly-brace languages”, like optionals, case classes, or pattern matching.\nIt also brings not-that-modern, but still useful notions like true generics (as opposed to C++’s templates or Java’s type-erasure system), value types (supported by C++ from the origins) or default argument values (idem).\nMore importantly, Swift chooses a different trade-off between robustness, speed and dynamicity. It is strongly typed—sometimes bordering on rigidity, as with optionals, but that’s for the good cause—which translates into more compile-time checks… and less runtime surprises (a.k.a “bugs”). It is fully compiled, instead of just translating into runtime calls, which means it can run faster, especially since it leverages the awesome LLVM compiler infrastructure and all the optimizations it provides.\nAll these benefits come with a cost, though. While its syntax looks easier, Swift is actually a difficult language to master—as complex as C++ is. Also, it uses unique constructs, like  or … conditional assignments, and an odd error handling mechanism—a hybrid between exception throwing/catching and traditional return codes.\nTherefore, its benefits for new programmers is not that obvious. I have seen junior programmers pick up Objective-C really easily and quickly, while I must admit that I still struggle with Swift at times.\nJumping headfirst into Swift: Lessons learned\nAt Algolia, we are always eager to try new things. It’s part of our DNA. So, when Swift 1 was released, we jumped on the opportunity—that was our first mistake. \nThere was no obvious need for it. Objective-C was still supported: the entire legacy code base, including Mac OS and iOS themselves, was written in it, so it would not disappear in the near future. Because of that, Apple had made sure that Swift could leverage Objective-C code, which means we could have supported Swift without writing a single line of code in that language.\nHowever, some customers at that time were already going full Swift and asking us to follow them— the adventure looked too exciting for us to pass up. \nWe wrote our Swift API Client, while keeping our Objective-C API client.\nA few months later, Swift 2 was released. It broke everything, but since version 1 was mostly experimental, it was fine to rewrite the library for our next major release..\nMaintaining two code bases hurt. Algolia is a fast-paced company, with new features being released almost every week. Since many of those features need to be reflected in our API clients, those libraries evolve rather quickly as well. Therefore, having two very distinct code bases to support the same platforms began to itch… especially given a major refactoring was required to pave the way for our Algolia Offline release.\nThe logical consequence was to drop one of the code bases—easier said than done! Deciding which one to drop was tough.\nThe natural choice would have been to keep Objective-C. Its feature set is entirely covered by Swift (but not vice… versa), and it’s more mature, with a stable API and ABI. It’s what most people advise, for good reason.\nOn the flip side, Swift offers nice abstractions that better fit our needs. In particular, it can make value types (like integers or floats) optional, which is useful for handling our search parameters. It also provides advanced enumerations, allowing us to deal elegantly with special cases, like our removeStopWords parameter, which accepts both a boolean or a list of strings.\nThe lack of ABI stability is not a problem in our case, since our API Client is open source and delivered through CocoaPods.\nFinally, as stated above, some of our customers already went 100% pure Swift, and we didn’t want to betray them.\nSwift first, Objective-C as a fallback\nSo we chose Swift. It turned out to be a challenging task.\nWe didn’t do it right from the start. We had to fail, and learn from our mistakes—only after version 4 of our Swift API can we speak from a state of confidence about our decision. That said, we may still discover mistakes in the wild out there, waiting to be fixed in future releases.\nBecause the ecosystem still has a huge proportion of its code base written in Objective-C, Apple went to a lot of effort to “bridge” Objective-C and Swift as seamlessly as possible. Whenever some Swift construct has an equivalent Objective-C construct, it is automatically made available from Objective-C—it’s like magic.\nThe problems arise when the magic falls short, simply because there is no equivalent notion in Objective-C for what you are trying to express in Swift. In particular, the aforementioned optional value types or advanced enumerations cannot be bridged.\nThe easiest way to solve this is to use a subset of the Swift language, namely only features that can be mapped to Objective-C. It’s safe, but it negates the purpose of choosing Swift as an implementation language in the first place. If we limit ourselves to what is supported by Objective-C, then we… might as well write code in Objective-C directly. Also, by using types or constructs that are not typical of Swift, we condemn ourselves to a poor developer experience (DX).\nBy writing some Swift-specific code and having an Objective-C compatible fallback whenever necessary, we gave ourselves more work up-front—and probably a little more maintenance—but it yields a much better DX in the end.\nLet’s see how it goes in practice with our  class, which gathers our search parameters. Every parameter is embodied by a property with the same name. It is first implemented with the optimal type for Swift. Since most of these types are bridgeable to Objective-C, no more work is required for most of the parameters. For the few non-bridgeable properties, we add an Objective-C compatible fallback. This property is inevitably also visible in Swift, which could create confusion for the developer. However, we use a few tricks to mitigate the impact:\n\nThe Swift name is prefixed with , which makes it obvious that it is intended for Objective-C, and also ensures it appears last in code completion.\nUsing an  annotation, we map this property to the parameter’s name. Since the original property is not visible in Objective-C, no conflict is induced by this. All goes as if the property simply had a different type in Objective-C.\nThe fallback property is not documented, which means it does not appear in the generated reference documentation.\n\nWe looked at other potential solutions, like having an Objective-C specific subclass. This leads to even better name insulation; however, it poses tricky covariance issues when extending the class, like what is done by InstantSearch Core for Swift. Finally, naming tricks were easier to maintain and created less friction for users of the library.\nThe end result is cool autocompletion that works smoothly from both languages:\n\n\nSmoothing out the rough edges\nWe discussed properties, but what about methods? Thanks to named arguments, Swift is actually… quite close to what Objective-C can achieve. And thanks to well-established naming conventions, Xcode is able to automatically compute the name of a selector from a Swift function, and this computed name will be suitable 90% of the time. For the remaining 10%, though, explicitly specifying an Objective-C selector name can lead to a better DX.\nFor example:\n\nthe Swift function  would normally translate into the Objective-C selector , but we preferred .\nSimilarly,  should translate into , but we chose to make the first argument more explicit with .\n\nGenerally speaking, it is OK to be more verbose in Objective-C than in Swift.\nAlso, code is fine, but shipping a good software library involves more than just writing code. Extensive documentation is a must.\nThe traditional tool used in the Apple ecosystem, Appledoc, only supports Objective-C. Instead, we use Jazzy, which supports both Swift and Objective-C, but (at the time of writing) only generates Swift documentation from Swift code. Objective-C developers have to guess the selector’s name based on the Swift function’s name—or rely on Xcode’s autocompletion. As you can see, there is no perfect solution yet for cross-language documentation.\nWhen Apple falls on your head\nThe cool stuff with Apple is how predictably unpredictable they are. Just when we thought we had Swift figured out… Swift 3 was released, and broke everything. Again.\nBeing hit by an apple is not necessarily a bad thing (ask Isaac Newton). Swift 3 brings huge improvements over Swift 2, and was totally necessary. It really looks like “Swift finally done right”. The migration gave us an opportunity to improve our code, sometimes in a backward-incompatible way that would not have been possible without a new major version.\nStill, a forced migration does have some drawbacks. You need to support two branches for a while; more importantly, you don’t control the timeline—and that’s especially true with Apple. The scope kept moving until the… final release, too. Swift underwent a tremendous amount of changes between the first and the last beta versions (making them more like alpha versions). There were even changes between the last beta and the Gold Master (GM)!\nRegardless of this constant stream of changes, customers asked for our support from day one. They wanted their app to be updated in the App Store as soon as iOS 10 was out. All this conspired to create a “rush effect”—which is never the best way to ship quality software.\nWe managed to handle it pretty well, all things considered. Version 4.0 of our Swift API Client was out on September 14th, 2016—one day after Xcode 8.0 final was officially released.\nAll of this could have been avoided if modules were distributed in binary form, but Cocoapods encourages delivering modules in source form, and anyway Swift doesn’t have a stable ABI yet. Although ABI stability was initially planned for version 4, due in Fall 2017 (see the Swift Evolution Proposals), it now appears that it will be deferred again.\nExploring the jungle\nAs a conclusion, supporting both Swift and Objective-C from the same code base is definitely possible, from either of the two languages. Which one you pick is a question of which compromises you’re willing to make. Objective-C is the safe and easy choice. Choosing Swift is more complicated, and will expose you to more hectic schedules; however, you will be rewarded with a slightly better DX for your Swift users.\nHaving walked this path, and being able to weigh the benefits and drawbacks, I would still recommend sticking to Objective-C for the time being as far as universal libraries are concerned. The decision for a standalone application is an entirely different tradeoff, and Swift does make a lot of sense in this case.\nDespite the huge improvements Swift has undergone, the language is not yet mature enough to write future-proof software—as all good libraries should be. It is surprisingly insufficient in some crucial… areas, like inter-thread synchronization. Even today, it’s not uncommon to see the compiler crash on erroneous input—instead of cleanly exiting with an error message—or burn 100% CPU for several seconds before finally giving up on evaluating a seemingly trivial expression.\nHopefully, all that will be solved in the future, making Swift a first-choice language for all types of developments. In the meantime, I’ll keep exploring the jungle for you.\nReferences\n\nSwift’s official page\nAlgolia’s Swift API Client\nAlgolia documentation\n\nNEW! InstantSearch Core for Swift: a high-level library to develop rich search experiences"
  },
  {
    "id": "6143-0",
    "title": "Algolia Vault - Bringing Physical & Digital Data Security to Search",
    "author": "Liam Boogar",
    "text": "At Algolia, we take search seriously. And we take security even more seriously. Search is the means by which users access personal & often private data, and one of the reasons customers trust that data on Algolia’s servers is because we have strong security for all of our customers - even our forever-free Community plan used by thousands of websites today.\nGreat search & secure search don’t have to be contradictory\nSome data is more sensitive than others. Last year Algolia set out to provide an extra level of security for our customers for whom privacy concerns & secure data are core to their business (or a regulatory requirement of the industry they work in). We recently began making that feature available to our Enterprise users upon request, and we're excited to introduce Algolia Vault.\nAlgolia Vault is a dual-faceted approach to making sure that the most sensitive data can't be compromised. Medical records for a digital healthcare service or personal user data used only internally should be searchable and secure. \nAlgolia Vault brings extra physical security by activating encryption-at-rest, encrypting your data even when it’s not being used. By default, Algolia encrypts all data transfers, but adding encryption-at-rest means that your data is safe in the case of physical theft. \nLikewise, Algolia Vault brings extra digital security by allowing customers to create an IP Whitelist, preventing any access from non-authorized IPs. IP whitelisting means a tradeoff against access by your dedicated Solutions Engineer & Customer Success Manager; however, Algolia Vault users can limit access to select IPs to make sure their data isn't accessed by unwanted users.\nAlgolia Vault limits implementations to back-end, meaning that your search speed becomes dependent on the speed of your servers (and not just ours). Of course, you’re still benefiting from the Algolia engine’s lightning fast search speed and our support team is there to work with you to keep… your end-users satisfied with the overall Search UX.\nWith Algolia Vault, we’re one step closer to making search available to all product builders, no matter the platform, language, or business needs they may have. Sensitive data shouldn't be punished with bad Search UX just because the data needs to be secure - financial institutions, governments, healthcare & providers are among those the most affected in terms of end-user experience due to the requirement & expectation of data security. With Algolia Vault, along with our ongoing security certifications and requirements, we hope to bring great search UX to every product, app, website & service.   \nIf you’re interested in trying out Algolia Vault today or learning more about how we prioritize security, you can check out some of our other security-related blog posts, or reach out to us directly on Twitter or by email."
  },
  {
    "id": "6102-0",
    "title": "How we built the real demo for our fake CSS API client",
    "author": "Tim Carry",
    "text": "On March 31st, we announced our CSS API Client, that replicated a search engine with only CSS. While it was only a joke in the spirit of April Fool's, it was a lot of fun to make, and also a lot of fun to see it in the wild.\nI've always been fascinated by how much could be done using only CSS (from flags to FPS games). When I started building the demo, I never imagined that I could build such a full-fledge search experience with CSS. The final demo has some huge drawbacks that make it unusable in production (the massive filesize of the generated CSS being the main one), but it still has some nice features that I'd like to shed some light on in this blogpost.\n\nThe power of selectors\nBy design, CSS does not have some of the features we expect from programming languages. It has no conditions, no loops, no functions, no regular expressions… Instead, it has selectors that can target elements in an HTML document and apply styling to them.\nUltimately, targeting elements is one of the greatest strengths of CSS, because it can select elements through a combination of tag names, classes, ids, and attributes. With the sibling selectors (,  and ), you can even go crazier in what you can target - this is a big part of how I manipulate CSS for creative CSS projects.\nThe search bar\nWhen it comes to searching, the first thing that comes to mind is the search bar. This is the ubiquitous element where you input your text, and results gets displayed. In HTML, a search bar is nothing more than an . It can thus be targeted by CSS.\nAn  element also has a  attribute. It means I can write a selector that will only match the input when it has a specific value:\n\n\nWhen the value of the input is equal to \"Tim\", I can change the background of the input to green. This might not seem much, but this is the cornerstone on which the whole CSS API Client is built. It lets me change the styling of elements based on the value of the .\nWe'll see how this will be used, but first I need to talk a bit… about JavaScript.\nJavaScript: the necessary evil\nI know I said this CSS API Client didn't need any JavaScript, but that wasn't entirely true. I still need a tiny JavaScript statement for this to work.\nWhen you use an attribute selector in CSS, it uses the value of the attribute that is set in the HTML, at page load. And when you type something in an input, it only updates the \"virtual\" value of the input (the one you can read with JavaScript), not the one in the HTML. It means that by default, CSS does not catch changes in the value of an input.\nAs the whole demo is based on that fact, that's not very convenient.\nThat's why I need JavaScript. I want our  attribute to be updated whenever something is typed in the input. This is mandatory to get this as-you-type feeling of results instantly displayed.\n\nThis simple JavaScript statement can be read as: \"Whenever the value changes, set the value equal to the value\". It might seem obvious but is actually needed to pass the correct value from the JavaScript world to the CSS world.\nNow that I've taken care of that, let's see how to display the results.\nDisplaying results\nAt that point, I can apply some styling to the input based on its current value. It's a good start, but what I really want is to display results based on what is typed in the input.\nUsing the  sibling selector, I can target the  that is just following the  in the markup. And by using the  pseudo-selector along with the  declaration, I can add some generated content to it.\n\n\nWhenever I type Tim, Josh or Vincent, the next div will be populated with the full name.\nPrefix search\nThat works well. but a good search engine shouldn't have to make you type the full keyword before displaying results. I should see \"Tim Carry\" as a result as soon as I type \"Ti\", or even just \"T\".\nThat's when the major mental shift occurs:\n\nAs a user, when I am searching, I want the answer to the question: What results will I have if I type \"Tim\"?.\nBut here, as a builder, the question… becomes: What should I type to have \"Tim Carry\" as a result?.\n\nIn this example, the answer would be \"T\", \"Ti\", \"Tim\", but also \"C\", \"Ca\", \"Car\", \"Carr\" and \"Carry\" if I want to search in both first name and last name. Those variations are called n-grams, which means all the strings that would match a specific record.\nNot wanting to do this task by hand, I wrote a Ruby script that generates the n-grams for any word, and outputs the CSS selectors to a new file.\nDisplaying several results\nNow that I had it automated, I started feeding my script with the list of all Algolia employees, to see how it performed. That's when I realized that displaying only one result was too limiting.\nFor example, we have 5 people named \"Alex\" in the company (at some point, 20% of the company was actually named Alex), so I wanted to display all of them when searching for \"Alex\", not only the first one.\nI changed the HTML markup and instead of having only one  for the result, I created as many divs as the number of employees. I gave each div a unique incremental  (to select them more easily), and pre-filled each of them with the full name using the same  and  trick.\nThose s were all hidden by default, and only made visible when the value of the input was matching them.\n\n\nStyling\nNow that I had the full data-set to play with, I decided to give it a bit more styling, to make it look like the actual \"About Us\" page we have on the website. Because I'm only using one  per result, I had to resort to a few more CSS tricks.\nI added a picture for each of us as a background image. I used Cloudinary to make all images round, resized and grayscale. I added a fun fact on hover, using the  pseudo element and generated content.\nTo display both the name and job title, I used the special character  to add a new line in the generated content. I also added  so both lines were actually displayed separately. Then, using  I was able to style each line differently.\nEdge cases\nBy playing with an actual dataset, I… could start to see some edge-cases that I didn't expect at first.\nThe first one was handling case sensitivity. I wanted both \"Tim\" and \"tim\" to match the same results. Fortunately, CSS handles that directly without needing me to generate more n-grams.  will match \"tim\" in a case insensitive way. This works on all major browsers except Edge, unfortunately.\nAll the other challenges had to do with my co-worker, Rémy-Christophe Schermesser. Rémy-Christophe has a very interesting name when it comes to edge-cases.\nThe first is that his first name is actually composed of two names and I wanted to be able to find him either with \"Rémy\" or \"Christophe\". In addition to splitting the name on spaces (to identify first name and last name), I also had to split them on dashes.\nThe second part was that his name actually uses an accented character (é), and I wanted to be able to find him either by typing \"Remy\" or \"Rémy\". I had to normalize the names, and generate n-grams for the normalized version as well.\nFinally, his last name, \"Schermesser\", is difficult to spell correctly on the first try. I wanted people to be able to find him even if they wrote \"Shermesser\" (without a \"c\"), or \"Schermeser\" (with only one \"s\"). Typos are very common in search, especially on mobile (thanks to the fat-finger effect), so a good search engine should auto-fix them as much as it could.\nTo handle the typos for this specific word, I had to generate 40 more n-grams, one for each variation of the word with one letter removed (except for the first and last one). \"Ser\", \"Sherm\", \"Scermes\", \"Schrmesser\", etc were all valid new n-grams to find \"Schermesser\".\nThe funniest thing in this story is that, even in the office, almost no-one ever calls him Rémy-Christophe. Everyone calls him \"RCS\", and I wanted to replicate this in the engine, through the use of synonyms. Adding a synonym is nothing more than manually adding a new set of n-grams that should match a specific result. In my case, \"r\", \"rc\" and… \"rcs\" were now also valid keywords to find Rémy-Christophe.\nHighlighting\nAt that point, I could search for people either with their first name or last name, including typos, or through synonyms. I decided to also add finding them through their job title. This adds a lot of possibilities, and sometimes it was not obvious why a specific result was displayed.\nA good search engine should never let the user wonder why a result is displayed. It must be clear instantly why a result is relevant, and that's where highlighting comes into play.\nThe Algolia API handles highlighting by wrapping matching text with the  tag. But because I'm using generated content, I cannot add HTML into it, so I had to find another way.\nThe trick I used was to generate a custom font that is a merge of Raleway Regular and Raleway Bold. I kept the regular font as a basis, but added all the bold characters into the private UTF-8 space. To highlight a word, I replace its letters with their bolded version. CSS lets you use specific UTF-8 characters by using the  syntax.\n\n\nOrdering results\nAnother really important part of a search engine is that the most relevant results should be displayed first. My current setup wasn't taking relevance into account at all when displaying results.\nThis was obvious when searching for \"Co\". I had Nicolas and Julien, both Co-founders, displayed before ry Dobson. This was not what I wanted. Cory should have been the first result because the match was in his first name and that was more important.\nThe Algolia engine handles this by sorting results through a tie-breaking algorithm. We define a list of criterias, and we check each result against those criterias to sort them into buckets. I applied the same logic in my script; my first criterion was that a match in the name was more important than a match in the job title. The sorting should then create two buckets, one for a match in the name, another for a match in the job title.\nIf there were several results in a specific… bucket, it would apply the second criterion to this bucket to further split the results in more buckets.\nMy second criterion was the position of the match. A match in the first word was more important than a match in the last word. If it would still have ties, it would then sort by the last criterion, the custom ranking. Here I decided that results at that point should be displayed by order of arrival in the company.\nYou can see it in action with the query \"St\" for example:\n\n\nWe have a first bucket with the match on the name, and a second with match in the job title (Full-Stack Software Engineer)\nThe first bucket is then split into two more buckets, a match in the first name (Steven Merlino), or a match in the last name (Alexandre Stanislawski, David Strader)\nFor each bucket with more than one item, results are ordered by arrival date in the company (Alexandre joined before David, Vincent joined before Maxime, etc)\n\nNow that I know in which order items should be displayed, how do I do that in CSS? This one was actually much easier than anticipated. As soon as you start using flexbox-based display, you can apply an  to any element to change its position in the display.\n\nFaceting\nThe last feature to add was the filtering (or faceting, as we call it in the search industry). Faceting is extremely important when the number of results you get is very large, and thus hard to comprehend. It is important to let your users filter it down.\nI added a way for users to filter results by team. I used most of the same tricks (generated content, flexbox ordering, etc) that I already used on results. The one thing that is important here is that the facets are actually  elements, each linked to an invisible  button.\nIn HTML, you can link a  with an . Whenever you click on the label, the radio button will get selected, wherever it is in the page. I used that to my advantage, by linking each  to a different radio button that I put in the markup right before the main search bar.\nUsing… the  pseudo-element and the adjacent sibling selector, it let me create a \"global state\" to my page, based on which  is currently checked.\nFiltering is applied here by hiding all the results that do not match the selected facet. If I select the team \"Engineering\", then I need to hide all results that are not in the \"Engineering\" team.\n\n\nPerformance\nThe generated filesize can get huge quickly, as I need to create new rules for each new n-gram. I used one-letter ids as much as I could as it both reduces the filesize, and makes the browser parsing of the CSS much faster. I then minified the file by grouping selectors sharing the same rules together. Still, the final file is 4Mb.\nI've also removed the  keyword (taking too many characters) by artificially boosting the specificity of my rules instead ( is more specific than  for example but will still target exactly the same elements).\nConclusion\nBy splitting a complex problem into small chunks, and using the strength of the language that was at my disposal, I managed to build the illusion of a search engine in CSS, which made for a great basis of some April Fool's fun. While the demo could never be applied in production - the file size is too big, the results cannot be selected nor clicked and it lacks many features a real search engine should give you - it was a great opportunity for me to stretch what’s possible with CSS, and for us at Algolia to bring a smile to our community and a few new faces who discovered us from our prank. You can have a look at the source code on GitHub.\nSearch is all about speed, relevance and UX, and we've only scratched the surface of those three pillars here because we were limited by the CSS language; however, we're glad the prank & demo sparked some great conversations in the CSS community and we look forward to discussing more with you."
  },
  {
    "id": "6057-0",
    "title": "Goodbye JavaScript: Introducing our CSS API Client",
    "author": "Tim Carry",
    "text": "Editor’s note: This blog post is an April Fool's faux product launch. However, to see how we actually built the real demo for 'launch', see our full explanation.\nAt Algolia, we are always pushing the boundaries of search. Today is an important day for us as it marks the official release of our 11th API Client: the CSS API Client.\nCSS is an awesome language. It only takes you a few years of practice to be able to style a minimalist website in a matter of days.\nLately, we’ve seen more and more discussion about its place in regards to JavaScript. One side thinks that CSS and JavaScript have very different goals, and should be kept in separate parts of your code, to have a clearer separation of concerns. Others argue that one cannot live without the other and that CSS should be inlined directly inside JavaScript.\nAt Algolia, we've decided to take a stance to stop this never-ending debate once and for all. We came to the conclusion that both sides were wrong, and that CSS was a language so powerful that you do not actually need any JavaScript.\nThat's right: we decided to get rid of JavaScript altogether.\nJavaScript: 1993-2017\n\nLook, JavaScript is an impressive language. We even considered rewriting our whole engine with it at some point. JavaScript is asynchronous, so it is fast by definition.\nUnfortunately, the language is not mature enough for us. A new version of it is released every year, which shows how unstable it is. On the other hand, CSS3 was released in 1998, without any new version since then. We think it sends a clear signal that CSS is the mature and stable tech.\nCSS: superiority through simplicity\nCSS also has none of the features that bloats other programming languages. It has no conditions, no loops, no functions and no regexps. It is pure. You can write concise selectors like  that clearly expresses your intent at first glance.\nWe’ve exposed our entire search engine using just CSS, and you can try it live.\nSeriously, it actually exists. It’s… typo-tolerant, handles synonyms, highlighting, faceting &amp; more.\nTry it live!\nA truly offline search for your web browser\nThe best part of the client is that it works offline. There are no actual requests done to our servers, everything is directly handled by your browser. It means that once you've download the initial CSS file, you can unplug and search away. That's what we call Offline Search!\nNo call to the API also means that you have unlimited operations. You can search all day long in the same 100 records, and it won't cost you a cent. Every request being handled by your browser means that you are now using your 8 cores at their maximum potential. We've actually started shutting down 2/3 of our datacenters because we are anticipating a much lower load thanks to this CSS release.\nOne API Client to rule them all\nYou know the saying: \"Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away\". So, we removed a lot of features.\nYou won't need data instantly replicated all around the globe to reduce latency because the data now lives in your browser; you can't get any closer than that.\nAt Algolia, we also take great pride in having extensive documentation and code examples for all our API clients. We'll remove those other clients, obviously, because we know you won't need any other API client than the CSS one.\nWe are so confident in the quality of this release, that it will work flawlessly, everywhere, every time, and that there will never be any bugs, that we decided it’s no longer necessary to offer support for it. This shows how committed we are.\nFinally, because there are no calls to the API, we decided to remove Analytics and the 99.999% SLA from our services. It was a hard decision to make, but when we weighed the pros and cons, it was clear that we needed to remove all the features that didn't bring any value. Instead, we added support for flexbox.\nNext Steps: CSS Everywhere\nThis whole experiment opened… our eyes on a brand new world. We've already starting working on our new Machine Learning processing pipeline in CSS. Stay tuned!\nTry our new CSS API Client live"
  },
  {
    "id": "6072-0",
    "title": "InstantSearch Android: Optimized Search Components for Android",
    "author": "Paul-Louis Nech",
    "text": "We’re excited today to be releasing InstantSearch Android,  a library of Views and helpers to help you build the best instant search experience on Android. It is built on top of Algolia's Android API Client to give Android developers a powerful solution to quickly build the right search experience to delight their users.\n\nBefore we dive into how InstantSearch works, I’d like to talk about why we invested so heavily in the Android developer ecosystem.\nImplementing Search on Android today: not for the faint of heart\nCreating great search for your Android app is hard. Search is a complex problem in general, but, for an Android developer, adding search to their application comes with unique challenges: some are common to all mobile app developers, but the Android platform brings a few hurdles of its own.\nLimited Resources\nThe first source of difficulty comes from the constrained resources of mobile devices themselves. Because many apps are fighting for a limited amount of RAM, you won’t be able to keep all your data in memory. You cannot afford to put thousands of products in your user’s RAM if only a few are relevant to them, and you can’t store them in the device’s storage either, as it suffers from the same issue: tens if not hundreds of apps are squeezed in the small hard drive of a mobile device. This limitation hinders your ability to store data on your user’s device, which makes it more impractical to build a local search solution.\nEven if you don’t have a large dataset, you won’t be able to implement all the advanced features that a powerful search needs (typo-tolerance, synonyms handling, ...). With the CPU constraints of a mobile platform, most complex processing logics cannot be implemented locally if you build from scratch - at least not easily or quickly.\nNetwork Stability\nThe previous performance limitations are easily removed if you use a hosted search engine, as you can offload space and computational resources to your… provider.\nHowever, mobile devices are used in a specific context: your users move from a network to another, connect and disconnect to Wi-Fi networks, lose connectivity when passing under a bridge. This mobility context brings two difficulties to mobile app developers.\nWhen the user moves around or simply has bad connectivity, their device suffers long latency. The network requests can suddenly take a very long time to resolve.\nMoreover, even when the connectivity is good, the network bandwidth can still be pretty small. Although a request can be sent quickly, the response’s payload can take a long time to load.\nThese two network effects will have a negative impact on your app’s UX if you don’t consider them when developing.\nReal Estate & Screen Constraints\nThe main challenge of designing great mobile apps comes from the available screen space of the devices. At best, a tablet will offer around 10 inches of screen space for your apps, but most mobile devices will only have a few inches of screen space (on average 4.5”.)\nThis small space for your application’s interface will limit what you can display on the screen at any given time, which makes developing complex interfaces like a search UI even harder on mobile.\nIt is even harder if you consider the “Fat Finger Effect”: because it is hard to tap precisely with a touch-screen, small buttons are hard to action and nearby commands are easily mistakenly clicked. This brings another layer of complexity when designing your interface.\nAndroid developers also have to address the platform fragmentation. You want your app to display nicely on a variety of devices, each with different dimensions and resolution. It complicates further the testing of your interface and the development of new UI components.\nLack of framework for search interfaces on Android\nOne of the more peculiar parts of building on top of the Android platform is that there are no building blocks for creating beautiful search experiences on… Android.\nThe Search Overview Guide will tell you where you should perform the search, but not how:\nThe process of storing and searching your data is unique to your application. You can store and search your data in many ways, but this guide does not show you how to store your data and search it.\nThe only practical solution that this guide tips at is using a full-text search algorithm if your data is stored in a local SQLite database. But what if your data is not stored like this, or if you want more powerful search capabilities like typo-tolerance, prefix search, or query processing?\nDeveloping a great search interface for an Android application is therefore difficult. You will face harsh performance and design constraints, user expectations of advanced functionality from your search engine, and your design needs to work across the various screen sizes and network conditions that encompass today’s mobile ecosystem.\n \nAlgolia: Improving the Android developer experience\nAt Algolia, we seek to provide every developer with the best tool for their job.\nIn 2015 we brought such a tool to web developers with instantsearch.js: a widget-based library that abstracts the complexity of building a search interface and powering it with Algolia.\nInstantsearch.js fulfilled a real need, as the community reaction has quickly shown: today the project is used by 1530 projects and has almost 1,500 stars on GitHub!\nThis works great for web developers, but Android developers need their own native solution.\nIntroducing InstantSearch Android\nLet’s dive in and see what InstantSearch Android does for developers.\nWith the Algolia engine under the hood, your app can leverage powerful features like typo-tolerance or geosearch, without bearing the CPU and storage costs that would come with an embedded search engine. We’ve also got you covered for RAM consumption: InstantSearch Android has been optimized so your app can run seamlessly on any Android device, using various memory optimization… techniques like View recycling or LRU Caches when relevant. Your users will be able to enjoy your app’s great UX without sacrificing their precious RAM!\nPrepared for adverse Network\nWith the hundreds of mobile applications using Algolia, we have had our fair share of network issues and limitations to work with. We leveraged this experience while developing InstantSearch Android, which includes the most useful tools for handling bad latency or low bandwidth easily: out-of-the-box progress indicators to let your user know that you did process their query, customizable caches to avoid sending queries when it is useless, and parameters to ensure the server’s response only contains the data that you need.\nDesigned for Mobile\nWe’ve distilled down our experience building search into mobile apps to bring you optimized components for building your own search experience - if you’d like to learn more about Mobile Search Patterns, you can check out this talk we gave earlier this year at Mobile World Congress.\nWe released InstantSearch Android with a set of Widgets: UI components to help you build faster your search interface. A widget is based on an Android View, made aware of the surrounding search interface and configured to follow the best practices of mobile search.\nEach widget follows the recommendations that are common to all mobile apps so you don’t lose time, while leaving you the room for customizing its look and feel for your users’ exact needs.\nYou can enable autofocus on a SearchBox in one line, or make it so that scrolling the Hits can automatically close the keyboard: we want every pattern that is useful to app developers to be a no-brainer with InstantSearch Android, so you don’t waste time on mundane things and can dedicate your time to what makes your app awesome.\nThis toolbox is now open-sourced and available on GitHub: you can start using it today to build great search interfaces in no time!\nTo give you an idea of what you will be able to build… with InstantSearch Android, we created two example apps based on some well-known search interfaces : the Media app will show you how to build a video search interface while the Ecommerce app shows a more  complex e-commerce search interface.\n\n \n "
  },
  {
    "id": "6029-0",
    "title": "Comparing Algolia and Elasticsearch For Consumer-Grade Search Part 1: End-to-end Latency",
    "author": "Josh Dzielak",
    "text": "Before comparing Algolia to Elasticsearch, it’s good to understand a few things about the nature of search.\nSearch architecture is unique\nThe type and quality of search experience you can deliver depends heavily on your choice of search engine, hardware, datacenter region and front-end web and mobile development frameworks. It’s important to make the right choice for each part of the stack but it’s equally important to make a set of choices that work together as a whole. Because search user experience goals are so demanding, a vertically-integrated approach to architecture is more important than for other types of applications. Latency, for example, is not only a function of the search engine but of every step between the search user interface and backend infrastructure.\nSearch is mission critical\nSearch is one of the hardest features to get right, both because users benchmark search experiences against Google & Amazon, and because search is a balance of multiple disciplines, not limited to UX, relevance tuning and performance optimization. Development teams often delay building search because of a lack of confidence in getting it right and the fear that it will take longer than expected. Yet search is often the most mission critical feature—the quality of an application’s search has a big influence on the perception of application’s overall quality. In domains like e-commerce, introducing a search bug can cost millions of dollars.\nThe combination of these factors make search one of the riskiest areas of development for business and consumer applications. When comparing different ways of delivering a solution, like Algolia and Elasticsearch, we want to look at how each approach specifically addresses the full, end-to-end set of risks. In this comparison, we will look not only at the search engine but the full search architecture, starting with end-to-end latency.\n\nMission-critical search for a global user base\nThere are many different types of search… applications. To focus the comparison of Algolia and Elasticsearch, we want to hone in on a specific family of use cases which we refer to consumer-grade search. Consumer-grade search is the type of experience delivered by companies like Google, Amazon and Facebook to billions of people worldwide. It connects people with products, content and key pieces of structured data. It is fast, reliable, works on multiple platforms and the results are highly relevant.\nThe search tolerates misspellings, alternate phrasings or user mistakes. Relevance is not caveat emptor, it’s caveat venditor - the search must adapt to the user, not the other way around. Consumers have high expectations of relevance but equally demanding expectations for the user interface. They expect an effortless, multi-faceted search and browsing experience, the kind pioneered by sites like Amazon.\nConsumer-grade search doesn’t just apply to consumer-facing applications. Today’s business application users have become just as demanding, in part because many business applications are now distributed in app stores and compete directly with consumer versions.\nThe expectations of the average user can seem unattainably high, but this is why Algolia exists. Algolia is laser-focused on helping customers meet the perfectly unreasonable search expectations of the average Internet user.\nAbout Elasticsearch\nAs a search engine that also functions as a scalable NoSQL database, Elasticsearch accommodates many different types of applications while not being opinionated toward one specific case. Elasticsearch is used for search but also log processing, real-time analytics, running map-reduce and other distributed algorithms, and even as an application’s primary database. The breadth of Elasticsearch is impressive and it does things that Algolia is not well suited for - streaming logs, map/reduce querying, complex aggregations and operating on billions of documents at a time. Algolia itself has used Elasticsearch… internally for tasks like storing logs and computing rollups.\nIn this comparison, however, we are focusing on consumer-grade search. This is the most common situation we are asked to compare. Building a consumer-grade search application with Elasticsearch requires a nontrivial amount of backend and front-end software engineering. There are many more steps than just provisioning and operating an Elasticsearch cluster.\nIn this series we’ll dive into what some of those steps are; however, you can already take a look at how Algolia solves for these steps in our Inside the Engine series. In it we explore implementation details like I/O optimization, query tokenization, multi-attribute relevance, highlighting and synonym handling. These are features that must be accounted for in any search project, including those with Elasticsearch at the core.\nEnd-to-end latency budget\nThe first feature of search is speed. Whole-transaction latency, from keystroke to visible search result, is what forms the user’s first impression of a search. A search application architect needs to have this in mind from the beginning, as a huge number of factors can affect the end-to-end latency.\nTo make things more difficult, for consumer-grade search the upper bound on satisfactory end-to-end latency is very, very low. Most consumer search experiences, including Google, Facebook and those of Algolia customers, deliver new results with every keystroke. This type of experience, known as instant search, is loved by users for its interactive feel but it only works if search results can be returned in the blink of an eye. Less, even: a human eye blink takes 300-400 milliseconds. An instant search starts to feel laggy at only 100 milliseconds.\nFor as-you-type search to be as satisfying as possible, Algolia recommends the end-to-end latency be no more than 50ms. This is the speed at which search feels truly real-time, where the user feels in full control of the experience. Under these conditions, users… are likely to keep reformulating their query until they find what they’re looking for, rather than abandon or bounce.\nIf you’re using Elasticsearch or Algolia to power an as-you-type search, these are the important numbers to keep in mind as you design your architecture. It is possible to consistently reach these numbers if you know 1) where latency is likely to accumulate and 2) how to reduce or eliminate it.\nThat’s what we’ll look at in the following side-by-side table: how Algolia reduces latency in each layer of the stack, where latency can accumulate inside of Elasticsearch, and what work can be done inside or on top of an Elasticsearch implementation to reduce the risk of added latency.\n\n\n\n\n\nAlgolia\n\n\nElasticsearch\n\n\n\nGlobal User Base\nLow device-to-datacenter latency requires infrastructure in multiple regions.\nTip: add 1-2ms for every 124 miles of distance over fiber.\n \nAutomatically replicate indices to any of 15 regions throughout the world using our Distributed Search Network (DSN).\n \nIt is possible to cluster Elasticsearch across multiple data centers, but not recommended. The recommended solutions involve replicating manually via a messaging queue to clusters that are not aware of each other.\n\n\nRAM\nIf a query's data is not all in RAM, it may have to load data from the much slower disk.\n \nAlgolia indices are stored in RAM (256GB or more) and memory mapped to the nginx process. No pre-loading (warming) is required to get great performance for the first query.\n \nThe ES cluster must have enough RAM and be properly tuned to make sure large indices stay in memory. If you are also supporting an analytics workload, you risk large analytics queries evicting data for searches.\n\n\nVirtualization\nIn sharing hosting environments like AWS, performance can fluctuate because of contention with other customers.\n \nAlgolia runs on bare metal hardware with high-frequency 3.5GHz-3.9Ghz Intel Xeon E5–1650v3 or v4 CPUs. Clock speed is directly related to… search speed.\n \nElasticsearch can be deployed on bare metal and optimized hardware, but at a premium cost compared to AWS or cloud-based solutions.\n\n\nSorting\nBefore results are presented to the user, they have to be put in the right order.\n \nAlgolia presorts results at indexing time according to the relevance formula and custom ranking. There is a minimal sorting step at the end to account for dynamic criteria like the number typos and proximity of words.\n \nSorting is done at the end of each query. Depending on the number of results to be sorted, this can impact latency.\n\n\nRelevance\nSpeed is often traded off to get better relevance.\n \nTokenization required for partial word matching and typo tolerance is done mostly at indexing time.\n \nAdvanced techniques like ngrams, shingles and fuzzy matching make indices larger and also require analysis at query time.\n\n\nDNS\nDNS can be slow before it's cached by the user's device. If a DNS provider is under DDOS, requests will be slow or fail to complete.\n \nAlgolia uses two DNS providers to increase reliability. Logic to fallback from one to the other is part of all API clients.\n \nElasticsearch does not provide out-of-the-box support for redundant DNS, but you could build it yourself.\n\n\nLoad Balancing\nLoad balancing & coordination can cause network congestion and add latency.\n \nAlgolia API clients connect directly to the server with data on it. There is no network hop or single point of failure for reaching a cluster.\n \nAn ES cluster needs the right ratio of data nodes and coordinating nodes to avoid adding latency. 10G network bandwidth is recommended for large clusters.\n\n\nGarbage Collection\nApplications running in the JVM require momentary pauses to free up used memory. During these pauses, requests are queued.\n \nThe Algolia engine is written in C++, it does not use the JVM.\n \nThe JVM can be tuned to reduce the frequency and impact of GC pauses. The tuning depends on the workload and server resources available.… This is a painstaking process about which much has been written.\n\n\nSharding\nSharding allows data to be scaled across multiple indices. Overloaded shards exhibit degraded performance.\n \nAlgolia handles any required sharding behind the scenes, it is invisible to the user. Shards can be dynamically rebalanced to avoid hot spots.\n \nIf original shard assumptions are wrong, such as the choice of a shard key, an Elasticsearch cluster will have to be rebuilt or rebalanced down the road to alleviate performance hotspots.\n\n\nHeavy Indexing\nLarge indexing operations can negatively impacts search performance because they compete for the same CPU and memory.\n \nAlgolia splits search and indexing into separate processes with different CPU priorities.\n \nThe Elasticsearch cluster must be configured to use different nodes for searching and indexing.\n\n\n\nConclusion\nLatency can creep in from any number of places. Great care needs to be taken at each layer of the stack to avoid exceeding the latency budget and causing users to abandon. Algolia’s hosted search approach means that we can give our customers the benefit of our expertise in reducing latency. For users of Elasticsearch, latency needs to be understood and addressed by the implementing engineering team.\nRead other parts of the Comparing Algolia and Elasticsearch for Consumer-Grade Search series:\nPart 1 - End-to-end Latency\nPart 2 - Relevance Isn't Luck\nPart 3 - Developer Experience and UX\n\n\nWant to learn more?\nJoin us for our next workshop to learn how you can achieve fast and relevant results with your search implementation.\nWorkshop: Achieving Speed and Relevance in Search\n Sign Up\n "
  },
  {
    "id": "5986-0",
    "title": "A search inside a search: how we brought Inception to Algolia",
    "author": "Alexandre Stanislawski",
    "text": "Today, we are thrilled to announce search for facet values, a new feature to give end-users full control to explore and actually use those hidden values, enabling you to deliver on your promise to connect your users with what matters most to them.\nFacets are a key feature for a search engine, because they serve as a way to filter data by a specific value on an attribute - a good example of a facet is “brands” in an e-commerce website. With Algolia, each set of results for a search query contains the most common occurrences for each facet. This means if you have many values, some will be hidden. The simple solution to this problem is to fetch more values, but it may not be enough and the user would need to look for the value manually. Our previous solution for this problem was to recommend developers create a new index for each facet they wanted to make searchable - which was a process that created overhead.\nIs faceted search still not clicking with you? Let's dig further into the e-commerce example by assuming that you’re looking for a new pair of khaki pants from your favorite indie brand. If the brand isn’t well known, then even if you know that a store carries this brand, it may not appear in the first results or even in the list of brands among the facets if it’s not in the top ten brands of this site. You would have to filter other parameters in order for the results to be more focused on the brand you want. And that’s only if you are persistent, because we all know that Googling the website name & the name of the brand will give you some results - that’s where search for facet values comes in. It lets you filter the brands in order to find the one you want, from the very beginning of the search.\nSee it in action: redoing TED search\nTo demonstrate this new feature we built a brand new demo based on TED talks. In this search, we have set filters based on events, the kind of conference (original, TEDx, and so on), topics and languages. Each of… those filters contains more entries than we can display which makes them perfect for using search for facet values.\nTry it out!\n\nBehind the scenes\nUntil the addition of Algolia's search for faceted values, implementing this feature meant extra work preprocessing the data. For each faceted attribute, you had to build (and maintain) a dedicated index with the occurrences of the elements. This index had to be kept up to date, as records were added, deleted or updated.\nBut not anymore! Our new functionality only requires two elements:\n\nConfigure the facet to be searchable: searchable(attribute) instead of attribute in the attributesForFacetting\n\n\nUse the new method of the API to actually search in the values\n\nThis new feature added to our API is available in all our API clients using the method called searchForFacetValues(), as well as in the JS Helper. That method accepts a specific query to specify the search and the whole set of filters that you can send to index queries. This allows the search for facet values to be contextualized.  This filters the values to only those that make sense for the current query.\nFor example, in an ecommerce website for which we have two faceted attributes: type of good and color. If you search for “apples,” you don’t want purple to be proposed because there are no purple apples (not that I know of, at least).\nBaked in our widgets\nA feature is only as valid as its ease-of-use. Since we provide an end-to-end developer experience from the REST API to the UI, we wanted to implement the searchable refinement list right in our widget libraries instantseach.js and react-instantsearch.\nWhen I first I looked at the LinkedIn implementation, something didn’t seem quite right. It felt as if the feature was hidden from the users and the space could be used more efficiently. We prototyped two versions of the search refinement list and we ended up with two simple implementations that we tested internally:\n\n\n\n \n\n\n\n\nWe then conducted a study… with those two prototypes, and it turned out that for us the second version was the best. It was especially true for those who had no prior knowledge of the LinkedIn implementation.\nThis implementation is now available to you directly in instantsearch.js and in react-instantsearch. It is not a new widget but just a new option to pass to the refinement list which will transform the list into a search one.\nGoing further with search for facet values\nSearch for facet values is a great feature and with our current implementation baked into instantsearch libraries (vanilla js or react) we take advantage of it with few tweaks. But the truth is that we are only scratching the surface. This feature actually opens the door to implementation like smart search bars (like you see in Slack) that let the user refine facets within the search box.\nIf you’re wondering how to get started, we’ve updated our guides (because we"
  },
  {
    "id": "5947-0",
    "title": "Algolia: Picking up where Google Site Search left off",
    "author": "Sylvain Utard",
    "text": "Recently Google has decided to sunset Google Site Search (GSS), their paid service for powering search inside of websites. The service will cease on April 1st 2018 and stop active development on April 1st 2017.\nAlgolia helps product teams create lightning fast, relevant search, and since the announcement we’ve gotten a lot of questions and interest around how Algolia could serve as a replacement to GSS.\nIf you don't have time to read this post, watch our on-demand webinar and learn more about the benefits of moving from GSS to Algolia...\nA jack-of-all-trades search solution\nFor website owners, GSS was a jack-of-all-trades solution, notoriously easy to set up and as powerful as you’d expect a Google search product to be. From website crawling to the actual search UI, everything was handled automatically - all you needed was a small snippet of code.\nGSS will be dearly missed, if only for the fact that no technical skills were required to go live with a powerful search.\nWe’ll talk a little more later about how Algolia compares (spoiler: we require developers 99% of the time), but let’s first look at what GSS did & didn’t do.\nKeeping your search up to date\nBecause GSS was using Google’s default crawling mechanism, you couldn’t control the refresh rate of your pages. As a result, it could take several days for your search results to reflect the latest content on your website, and up to 24 hours when using their (paid) on-demand indexing API.\nIn contrast, Algolia provides you with an API you can use to push & update your content in realtime. That API is available in many different ways through several libraries and API clients. For e-commerce platforms (Magento, Shopify, etc.) & Content Management Systems (WordPress), we provide out-of-the-box plugins which include automatic indexing & updating requiring no additional software development.\nRelevance Fine-tuning\nBy design, GSS didn’t provide a lot of control over relevancy in search results. The product… started under the assumption all websites want to prioritize the same relevance as Google.com does, and therefore provided a single algorithm for every website. Unfortunately, it’s rare enough to see two eCommerce websites with the same relevancy algorithm, let alone every website on the web. GSS users could tune search relevancy in three ways: using keywords, weighted labels, and scores. But those knobs and dials were not ideal for improving your global site rankings - in fact you could negatively impact global results relevancy while trying to fix a specific query's results.\nAlgolia has a unique way to combine textual & business relevancy, making sure your users always find the right content. In Algolia you have complete control over what we call the “ranking formula” and you can influence more “traditional” things like “typo tolerance” or “word proximity,” but also custom criteria, such as your product popularity in the case of an e-commerce company or the number of comments if you were indexing article content.\nUpdating your design\nGSS has several built-in search page layouts and lets you select the one you’d like to have appear on your website. Most of the time, you want to go further than a simple integration: you want to have the same UI & UX patterns on all your website pages, including the search results page. GSS unfortunately only provides an API to fetch the results, you need to handle the full UI rendering on your end, from scratch.\nAlgolia provides you with powerful frontend libraries (like instantsearch.js, react-instantsearch or autocomplete.js) to help you build a beautiful & effective search results page, query suggestions, or dropdown menu in no time. Our goal is to reduce the time it takes you to implement incredibly powerful and beautiful looking search. We take User Experience very seriously, whether you’re a developer, product manager or website owner.\nSearch is a Conversation - Autocomplete and Instant Search\nBoth GSS &… Algolia enable autocompletion, helping your users to find the right content as they type. GSS's autocomplete feature was very simple: rendering a list of query suggestions as you type.\nWith Algolia you can build a rich dropdown menu displaying query suggestions but also include actual search results. Algolia goes even further and provides search results in real time with each keystroke, so search feels like a conversation & not an order request).\nNavigation & Refinement\nGSS provides you with a simple way to categorize your content and refine your end-users search through those categories. To define your categories, you could either add them through your page's meta-data or by defining an annotation file. Unfortunately, this provides you limited refinement capabilities and makes it almost impossible to create a flexible search experience for all your users.\nTo solve this, Algolia provides you with a full-featured faceted-search experience, letting you define all the dimensions you want to use for the refinements. Building an Amazon-like refinements sidebar takes a matter of minutes.\nIn conclusion\nAlgolia can be a great option for GSS users to consider if you're serious about search; However it all comes down to what you want to do with Search. If Search is core to your user experience, or you believe it could be with a better search experience, putting in a little extra effort and a few lines of code may be worth it. Outside of our one-click integrations with major content & ecommerce platforms, our team can help guide you through the transition and the choices to make. Sign up below to learn more.\nWant to learn more?\nWatch our on-demand webinar and learn more about the benefits of moving from GSS to Algolia."
  },
  {
    "id": "5927-0",
    "title": "Personalized Search at the Speed of Algolia",
    "author": "Nicolas Baissas",
    "text": "Search is more than a query and results - search is a conversation your users have with your product. When done right, you feel understood by the engine - and, in turn, the product - which reacts to your needs and adapts to what you’re saying as you’re saying it.\nPerhaps one of the most magical moments is when the search engine speaks to you - an individual person, with search results that are catered to your personal tastes and expectations.\n\"Personalization, yes! But not at the cost of speed.\"\nOne of the core focuses of Algolia is speed. Every time we’ve looked at how other search engines handle personalization, it always came at the cost of speed - we don’t think personalizing the search results makes sense if it makes the search slow. Who wants a slow, personal conversation?\nWe rolled up our sleeves earlier this year and got to work, and we’re proud today to announce two features that make personalization of search results a breeze, without compromising on the speed Algolia is known for. We call these two API features Optional Filters and Filter scoring - and here’s how it works.\nBecause the feature can place high demands on CPU performance, it is enabled by default only in our Enterprise plans. If you’re interested in trying it out on your plan, send us a note.\nGetting started\nLet’s take the example of searching through an index of movies. In this hypothetical website, users can add movies to their “Watch later” list - a familiar feature - and this information is added to the index, in the form of an attribute watch_later with an array of user IDs of every user that adds this movie.\nA Sample JSON entry:\n\nWhen we search this index, we want movies that are added to a watch list to appear before others.\nA regular search would look like:\n\nWith personalization, it would work like this:\n\nWith only one added parameter, suddenly the results are personalized: the search retrieves all the movies related to “karate”, but if a user has added one of… the movies related to “karate” to their watch list, this result will show up at the top of the results.\nGoing further: multiple levels of personalization\nWhat if you want to have multiple layers of personalization? For example, let’s say you’ve analyzed that a user is a huge fan of Action movies, watches a lot of comedies, but is not restricted to these two genres. You’d like the results to first show Action movies, then Comedies, and then the rest of the results. Here’s how you’d do it:\n\n...And you’ll get the results in the order that you expected! We added a new rule called filters in our ranking formula, which will rank the results according to the matching optionalFilters and filters scores.\nWe’ve been playing around with these features for a few months now, and a few of our customers already use them in production during the beta period, with amazing results.\nThe best part about these features is how versatile they are: there’s virtually nothing you can’t do when it comes to personalization of results:\n\nOn a professional social network, retrieving people that are part of your extended network first\nWhen searching on a shoe store, displaying brands or categories of products the user has bought from before first\nIn a CRM search, displaying leads I’ve contacted in the past first, or the ones that are assigned to me\n\nIf you want to know more about these features, we wrote a guide on personalization. We can’t wait to see what you’ll build with them!"
  },
  {
    "id": "5900-0",
    "title": "Solutions Engineers: Advocating for the Customer",
    "author": "Dustin Coates",
    "text": "We’re the Solutions Engineering team at Algolia. What does that mean exactly? We do a lot of different things, but they’re all tied together by one philosophy:\nWe advocate for the customer inside Algolia.\nBeing a Solutions Engineer requires flexibility above all. On any given day, we speak with customers and future customers, build out proofs of concept that show just what Algolia can do, and even add new features to the product itself. The tasks are varied and no two days are alike. Our priorities are determined by a single question: will this help our customers be more successful?\nWe’re proud of—and enjoy—what we do. We wanted to share it with you, to pull back the curtain and give you insight into this team you may have interacted with.\nA Variety of Job Requirements; A Variety of Backgrounds\nTo make our customers successful, we need to know what makes us successful. When we are evaluating a potential new SE colleague, what skills should they have?\nA successful Engineer on the Solutions Team should have the following qualities:\n\nProgramming aptitude\nCommunication\nUX design knowledge\nOrganization\nIdeation/innovation\nProactivity\nHigh standards\nTeamwork\n\nIt’s a decently-sized list and the qualities intersect well with each other. An Engineer on the Solutions Team won’t be able to communicate well with our customers and colleagues if she’s disorganized. And we consider good programming and UX knowledge to be siblings of a sort.\nWe’ve worked for startups, we’ve been at agencies, and we’re former educators. No matter how we got here, we use our experience to the benefit of our customers.\nWe Superpower Our Customers\nOn average, 60% of our time has a direct customer impact, with the other 40% spent on projects that look toward the long-term.\nSome work that might fall in the “direct customer impact” bucket:\n\nA well-known forum wants to search 100s of millions of posts and comments. What’s the best way to organize the data given our… infrastructure and their search needs?\nWhat’s the best UX for an ecommerce shop that sells books, toys, electronics, lawn equipment, sporting goods, food, DVDs, CDs, & video games?\nA potential customer wants to see what Algolia would look like on their website. Can you flex your technical muscles to blow them away?\n\nHere are a few examples of our 40% long-term work:\n\nThere’s a conference in Australia in front of 200 SaaS founders? Sign us up!\nNeed to redesign the dashboard? Let’s assemble a team and make it awesome!\nHow about an improvement to our instantsearch.js library? We’re on it.\n\nI asked my colleagues what they enjoyed most about their jobs. I thought Raph said something particularly interesting. He joined us in the spring of 2016 and he said he would tell people that:\n“I love being part of an awesome team that’s tackling a real problem: making content more accessible, and to see that it has a real impact.”\nAt the end of the day, that’s what makes our job so rewarding: we are making a real difference in our customers’ lives and their customers’ lives. Be it a well-known business or an e-commerce shop just trying to get off the ground, we are helping businesses be more amazing\nWe Superpower Our People\nOne of the overarching tenets of working at Algolia is that we hire really talented people and then empower them to do their best work. If you don’t like giving talks to big groups, but you love creating video tutorials, we’re going to encourage you to do what you are best at.\nEach person here cares deeply about the success of the others. Take a look again at the key qualities of our team members. Our teammates have high-standards and they value teamwork. We expect each person to look out for the others and to actively assist them when they need help.\nAnd that extends beyond our immediate team. We work closely with every team at Algolia. I often tell people that I’ve learned more in the past year about sales and recruiting than I ever… thought I would—or would want to. If I ever left Algolia to build a product, I’m in a much better place than I would have been in a strictly engineering role.\nDid I mention that we’re hiring?\nIf I had to sum us up in a pithy way, I’d say we’re engineers who obsess over our customers. It doesn’t quite fit on a business card, but it works.\nIf you read the above and thought to yourself, “That’s me!” then we want to hear from you. We are hiring in SF, NYC, and Paris and can’t continue to grow without talented people.\nE-mail us at solutions.recruiting@algolia.com and let’s chat."
  },
  {
    "id": "5868-0",
    "title": "Inside the engine part 7 – Better relevance via dedup at query time",
    "author": "Julien Lemoine",
    "text": "One of the most unique and most-used features of Algolia is the Distinct feature: it enables developers to deduplicate records on-the-fly at query time based on one specific attribute. We introduced this feature three years ago, opening up a broad range of new use cases like the deduplication of product variants - a must-have for any eCommerce search. It has also considerably changed the way we recommend handling big documents like PDFs or web pages (we recommend splitting each document into several records).\nThis post highlights the advantages of this feature and the different challenges it represents in term of implementation.\nDocument search: state of the art and limitations\nSearching inside large documents is probably the oldest challenge of information retrieval. It was inspired from the index at the end of books and gave the birth to an entire sub-genre of statistics. Among those methods, tf-idf and BM25 are the two most popular. They are very effective for ranking documents, but they don't handle false positives well - they push them to the bottom of the results.\nTo illustrate this problem, you can perform the \"states programming\" query on Google and Wikipedia. According to Google Trends, this query is as popular as the Rust programming language and seems to correspond to developers that search how to develop an algorithm with a state. The same query on Wikipedia is unfortunately not very relevant! The first issue probably comes from a stemming algorithm or a synonym that considers \"program\" as the same than \"programming.\" In Algolia, you can have an expansion of singular/plural without polluting results with stemming by using the ignorePlurals feature, which is based on a linguistic lemmatizer.\nThat said, even if you scroll through the first 1000 hits of Wikipedia search, you won't find the article which appears first in Google. There are hundreds of articles that contain both the word \"states\" and \"programming.\" Even the \"United States\" page… contains both terms and one is in the title! In this case, tf-idf and BM25 are not the most useful ranking criteria. The position of the two query terms in the document is more important, in addition to their relative distance (finding them close together is always better).\nStates programming query on Google and Wikipedia\nWhy do we split large documents\nOne of the best ways to avoid such relevance problems is to split big pages into several records - for example, you could create one record per section of the Wikipedia page. If a Wikipedia article were to have 4 main sections, we could create 4 different records, with the same article title, but different body content.\nFor example, instead of the following big record with the four sections embedded:\nView the code on Gist.\nWe could have 5 different records, one for the main one and one per section. All those records will share the same source attribute that we will use to indicate the engine that they are all component of the same article:\nView the code on Gist.\nWe can use this same approach for technical documentation, as there are often just a few long pages in order to improve the developer experience: scrolling is better than loading new pages! This is exactly why we have decided to split pages into several records in our DocSearch project; you can learn more about our approach in this blog post.\nIt might sound counter-intuitive, but the problem is even more visible when your data set is smaller! While the problem is only visible on well-selected queries on the Wikipedia use case, it becomes very apparent when you search inside a website with less content. You have a high probability of having false positives in your search that will frustrate for your users.\nThe need for deduplication\nSplitting a page into several records is usually easy as you can use the formatting as a way to split. The problem with several records per document is that you will introduce duplicates in your search results. For… example, all paragraphs of the United States Wikipedia article will match for the \"United States\" query. In this case, you would like to keep only the best match and avoid duplicates in the search results.\nThis is where the distinct feature comes to play! You just have to introduce one attribute with the same value for all records of the same source (typically an ID of the document) and declare it as the attributeForDistinct in your index setting. At query time, only the best match for each value of the attribute for distinct will be kept, all the other records will be removed to avoid any duplicate.\nIf we split the United States Wikipedia article by section and subsections, it would generate 39 records. You can find the first four records on this gist:\nView the code on Gist.\nThis split by section reduces a lot the probability of noise on large articles. It avoids the query \"Etymology Indigenous\" to match this article (which would typically match because there is a section called \"Etymology\" and another one called \"Indigenous and European contact\"). To improve the relevance, we have also divided the records into three different types that we order from the most important to the less important via the recordTypeScore attribute:\n\n\"main\", this is the first section of the article, including the title and synonyms (score of 3)\n\"section\": those are the main sections of the article (score of 2)\n\"subsection\": subdivision inside the sections of the article (score of 1)\n\nThe recordTypeScore attribute will be used in the ranking to give more importance to the \"main\" records than the \"section\" and \"subsection\".\nAnother good property of this split is that it allows linking to the right section of the page when you click on the search result. For example, if you search for \"united states Settlements\", you will be able to open the United States page with the anchor text stored in the record.\nHere is the list of the four Algolia settings we applied on this index:\nView the code on… Gist.\nYou can set those settings via the setSettings API call or directly on the dashboard:\n\nsearchableAttributes and customRanking can be configured in the Ranking tab\n\nAlgolia Dashboard: Searchable & Ranking Attributes\n\nignorePlurals can also be configured in the Ranking tab. For the moment you can only enable it for all languages in the dashboard (setting it to true), we will improve this setting to let you configure also the language.\nattributesForDistinct can be configured in the Display tab of the dashboard:\n\nAlgolia Dashboard: Group by\nEach of those settings is important for the relevance:\n\nThe customRanking uses the recordTypeScore to give importance to the main chapter in the case of equality on the textual relevance. In case of equality, we use the popularity attribute which represents the number of backlinks inside Wikipedia to this article (all records of the article keep the same popularity)\nThe order of attributes in searchableAttributes gives the following decreasing importance for the matching attributes, title > text > synonyms > sourceArticle\nThe attributeForDistinct is applied on the name of the article, only the best elements will be displayed for each matched articles\nWe set ignorePlurals to \"en\" to consider all singular/plurals form of English as identical without introducing any noise (you can also set this setting at query time if you have a search in a different language using the same index)\n\nWith those settings, you can easily request a set of results without duplicates with the query parameter distinct=true. You can go even further by requesting several hits per deduplication key. You can, for example, pass distinct=3 in your query to retrieve up to three results per distinct key - this feature is usually known as grouping.\nKeeping several records per distinct key\nSearch engines are also used for navigation or criteria-based search due to the fact that those queries are cheaper than on a regular database. Keeping several records… per distinct key makes it easy to bring a different display to search.\nFor example, if you want to build a list of job offers, you will probably let the user search via different criteria like the role, location, job type, etc - but you might also want to aggregate the search results per company instead of having each job display company information. This is what AngelList is doing when you search for a job on their platform. They display companies along with the three best job offers, this type of display can be built very quickly by:\n\nHaving one record per job offer with an attribute containing the company ID\nConfiguring the company ID attribute as attributeForDistinct in the index settings\nPassing distinct=3 in your query\n\nWith those three simple actions, you can build an interface like AngelList below (Note that SafeGraph has only two job offers that match the criteria, the value three is the upper-bound limit on the number of records we want per distinct key). Some customers also implement a \"Show more\" button on each company that allows to display all offers for this company. To implement it, you have just to filter on the company name and specify distinct=false on the query to retrieve all offers of this company.\nAngelList job search displaying several jobs per company\nThe Impact of Distinct on faceting\nFaceting can very quickly become a headache when distinct is enabled. Let's illustrate the problem with a very simple example containing two records sharing the same distinct key.\nView the code on Gist.\nThe first record contains the facet values \"A\" and \"B\" and the second record contains the facet values \"B\" and \"C\".\nLet's imagine that you have a query that retrieves both records and that faceting and distinct is enabled in your query. You can imagine three different behaviors in this case:\n\nComputing the faceting before the deduplication (using distinct). In that case, the possible facet values will be \"A=1\", \"B=2\" and \"C=1\". All categories are retrieved… but the count won't fit what you have on screen.\nComputing the faceting after deduplication (distinct). In this case, the result would be \"A=1\", \"B=1\" OR \"B=1\", \"C=1\" depending on the best matching record. In both cases, the result is not what you expect.\nApplying deduplication independently for each facet value. In this case, we would have A=1, B=1 and C=1 (for each facet value, we need to look at all distinct key and perform a deduplication at this level).\n\nThe third behavior is the holy grail as the result of each facet count would be perfect. Unfortunately, the computation cost is insane and doesn't scale as it directly depends on the number of facet values which match. In practice, it's impossible to compute it in real time.\nLet's look at the advantages and drawbacks of the two approaches which can be computed for each query in a reasonable time:\n\nComputing the faceting before the deduplication: all retrieved facets are valid but the problem is obviously the counts are misleading because they include the duplicates. That said, this approach works very well if you do not want to display the count, it handle any case including the fact you can have different facets in the records that share the same distinct key.\nComputing the faceting after the deduplication: this approach works well when you use distinct=1 and all records with the same distinct key share the same facet values, which correspond exactly to the use case of splitting big documents. In other use cases, the results will be disturbing for users as some facet values can appear/disappear depending on the record which is kept after deduplication.\n\nIn other words, there is no perfect solution solving all problems. This is why we have decided to implement both approaches but as the deduplication can be misleading and cause weird effect, we have decided to use the faceting before the deduplication as the standard behavior. We have a query parameter called facetingAfterDistinct=true that can be added to your… query when you are splitting some big records. In this case, you have a perfect result because the facets are identical in all records sharing the same distinct key.\nGreat features take time\nWe introduced the distinct feature in December 2013 and have improved it significantly in July 2015 by adding the support of distinct=N in the engine. The different faceting strategies have been a long-standing challenge and it took us a lot of time to find the best approach. The query parameter facetingAfterDistinct=true has been beta tested with real users for a few months before becoming public in December 2016.\nThis feature opened a lot of possibilities for our users and allowed a better indexing of big records that later gave the birth to our DocSearch project.\nIf you want to know more about the internal of the Algolia engine, we recommend to read the other posts in this series:\n\nPart 1—indexing vs. search\nPart 2—the indexing challenge of instant search\nPart 3—query processing\nPart 4—textual relevance\nPart 5—Highlighting, a Cornerstone of Search UX\nPart 6—Handling Synonyms the Right Way\nPart 8 – Handling Advanced Search Use Cases"
  },
  {
    "id": "5821-0",
    "title": "Introducing the Algolia Community forum and the Pioneer Badge",
    "author": "Josh Dzielak",
    "text": "Today we’re making a new resource available to the Algolia Community. It’s a modern take on one of the Internet’s most fundamental gathering tools: the forum.\nSee it live: the Algolia Community forum\nAs the Algolia Community has grown, it’s become very active on Github, StackOverflow and Twitter. As developers, we love these tools but there are gaps as we scale. Sometimes we want to have deeper conversations or conversations not specifically about programming. Other times we want to have larger, ongoing discussions and get input from a lot of people.\n\nDiscourse, the open source project we’ve chosen to power the new forum, is a chance to bring everyone together into one place and continue the conversation at scale. Discourse has customizable notifications, code highlighting, link expanding, badges and many other goodies right out of the box. It also has an excellent set of community moderation tools. Some very active communities run on Discourse including Github’s Atom, Twitter Developers, and Docker. We’re excited to join them as part of the larger Discourse community.\nCustomizations\nDiscourse is open source and has a powerful set of admin tools, so it's really a breeze to customize. One customization we've done so far is visual, by applying a minimalist theme that fits in with our overall community fonts, styles and colors.\n\nWe've also replaced some of the default badges with ones that match our theme. See them all on the Badges page, and keep reading to learn more about our first brand-new addition, the Pioneer Badge.\nCategories\nThe top-level organizational unit of Discourse is the category. Categories contain topics which have one or more posts. Notifications can be turned on and off at the category and topic level which means that you decide what you want to receive, not us. For example, if you're using the Shopify plugin, you shouldn't get emails about the WordPress plugin (unless you want to). Because the Shopify and… WordPress categories live separately on the forum, along with about a dozen others, it's possible for you to configure what you're watching at a fine-grained level.\n\nThe home page of each category has a menu like this where you can configure what you want to receive. For categories you're tracking or watching, you’ll get unread counts, so even if you don’t want notifications sent to you it's easy to see what you’ve missed just by coming back to the site. We've created a few categories so far, here's a summary of a few of them.\nThe Announcements category is one to keep an eye on. We’ll use it to post official announcements, including releases and new features, and also events and highlights from around the community. Introductions from each new member also live here inside of a special introductions topic.\nThe Projects category is a place to show off your Algolia implementations, projects and extensions. It’s ok to brag here! The community will love to hear how you configured your search and why you made the choices you did. You can also ask the community to try your search and give you feedback.\nThe Development category is the place to talk about the Algolia API, SDKs, and anything you’re using to get data into and out of Algolia. If you have questions about the architecture, technical or security design of your integration ask them in here. For anything else, you can get access to more viewpoints and potentially more depth by posting here.\nWe still encourage you to use StackOverflow for programming problems where you are expecting a single correct answer and Github Issues for repository-level bugs and feature requests.\nThe Design, UX and Relevance category is a place to talk about best practices for designing and implementing the user-facing aspect of your search. Not sure whether to use an autocomplete or instantsearch interface? Curious about what makes a search great on mobile? This is the category to ask in. Small changes to design and… relevance can have a big impact on the satisfaction of your users, and we encourage you to post in here with questions or lessons you’ve learned.\nThe DocSearch category is for the 200+ maintainers who are using DocSearch to power the search on their documentation. It's also for any of the teams we're backing via our sponsorship of Open Collective including webpack.\nThe Site Feedback category exists for you to ask questions and give feedback about the forum itself. All thoughts and questions about organization, moderation, and ways to improve are welcome.\nHow to get the Pioneer Badge\nWe want to reward Algolia developers and fans who join the forum in 2016 with a special badge. The entire Algolia team, and especially our developer advocates, really appreciate your help with beta testing and trying new things. To us, you're pioneers who are constantly pushing forward the frontier of search.\n\nHere is what you can do to get the Pioneer Badge.\n\nHead to the forum and click Log In. You'll be authenticated with your algolia.com account, or prompted to log into Algolia if you weren't already.\nIntroduce yourself on the introductions thread.\nTell the community about what you're working on in the Projects category.\n\nPlease complete all 3 steps by January 31, 2017. You will receive your virtual badge within a few days. We will follow up to send you a physical sticker for your laptop, along with information about extra badge-having benefits you can expect in 2017.\nBadges are fun but they also play another important role in community building—badges help members establish their reputation. Reputation helps new members identify experts and empowers experts to take more ownership. Reputation is also a way for developers to build their portfolio and strengthen their career. We expect the Pioneer Badge to be the first of many opportunities to build a valuable, market-recognized reputation as an Algolia developer.\nThanks for being a pioneer, let's explore… more frontiers together in 2017!\nBig thanks to Gianluca for deploying and configuring the Discourse instance, Antoine for creating the design, Jonas for implementing the design, Sébastien for creating the Pioneer Badge, Liam for reviewing this blog post, Raymond for onboarding our WordPress users, Matthieu for onboarding our Shopify users, and everyone at Algolia and in the community who has contributed something so far!"
  },
  {
    "id": "5796-0",
    "title": "Harnessing API’s with React: a different approach",
    "author": "Alexandre Stanislawski",
    "text": "Today we are launching react-instantsearch, a new library to build search interfaces with Algolia in React and React native. This new way of implementing Algolia brings ideas that go beyond the creation of widgets using React - we think they will fundamentally influence the way the JS community builds UI libraries!\nA year ago we launched instantsearch.js, in order to provide an easy solution for building search interfaces The project now has more than 1000 stars on github and 1500 users, far exceeding our expectations. With the release of instantsearch.js, we tried to help front-end engineers as much as possible and because the framework war was roaring hard back then, we went for the universal choice: Vanilla JS.\nIt was our first take at building a complete widget library for building search UI’s. With it, we tackled the problems that our users found when building a search interface, namely the lack of packaged options for search UX patterns, and the cumbersome mapping of concepts from the search realm to the UI realm.\nTo tackle those issues, we took the vanilla JS / framework “agnostic” approach. We created a light framework to be able to consolidate all the search parameters that each widget can set. Our final product was a drop-in search addon for all JS front-ends; however, this light framework is nothing without the widgets that are included. Each of them provides options to customise the search behavior and also their UI - it turns out, you can never provide enough options for UI and behavior.\n“You can never provide enough options”\nWe started with fixed markup, but this was not enough because you might want to use a specific CSS library with its own markup. So we added the ability to customize some parts of the UI with templates, but this wasn’t enough because users wanted customization across the board. We added the possibility to manipulate bigger chunks of the UI, but this created inconsistencies with our own expectations for the markup which… led to more options… in the end, the more options you provide, the more complexity you create.\nComplexity has two faces. First, it makes it harder to deal with inconsistencies and bugs. Second, it creates API noise. Dealing with inconsistencies and bugs is hard but it’s always a matter of how much you work on it, and eventually it’ll be fixed. On the other hand, the more options you have, the harder it will be to grasp what are the important or critical options. The amount of options creates an artificial sense of complexity, users get the impression that the learning curve is steep. They had to dig through all the options to find the one they need.\nOverall the API created for supporting UI options made the library harder to learn and use, and therefore the developer experience suffered from it.\nWe began to wonder ”What if we could separate the two, and provide options that are only meaningful for the search and limit the UI options?” - this is how react-instantsearch was born.\nDecouple search logic from rendering\nFrom the outside, react-instantsearch shares the same philosophy as instantsearch.js. It provides widgets, and each will handle a single part of the search UI. The combination of widgets makes up a search query to Algolia, which in turn updates the UI with new results, which in turn allows the user to further refine the search. Unlike its cousin, however, react-instantsearch handles customization by giving developers complete control over the rendering - the result is worth taking a look at:\nreact-instantsearch is made of a root component &lt;InstantSearch&gt; and other UI components that we call widgets. These widgets are the composition of a dumb react component with a business logic counterpart called connectors.\nAnd this is where the true originality of react-instantsearch is.\n\nConnectors are higher order components that provides a dual API. For the wrapped component, it exposes the minimal API necessary to let it do what it is supposed to… do: set and read the query if it’s a SearchBox, or set filters and get the filter values if it’s a RefinementList. For example, a minimalist implementation of a custom SearchBox would be:\nView the code on Gist.\nThe connector also provides an external API that let the developer set the search settings relevant to the concerned domain. For example, the `hitsPerPage` parameter is set on the Hits widget but is used by the connector to set the number of results to display on a single page.\nNow we have a clear and powerful API that provides all manners of customization for developers, and at the same time we can provide ready-to-use widgets that have limited options for UI tweaks. Our widgets are the results of our collective experience with search UX and what we’ve learned from users.\nThe end result is that beginner users are not left wondering what kind of experience they should implement, while experienced developers can adapt their search experience accordingly. Developers looking for a quick &amp; powerful solution will find exactly that with react-instantsearch - the deeper they dive, the more they will uncover its flexible underbelly by overriding the defaults we provide .\nreact-instantsearch is a dual-faceted API:\n\nOpinionated widgets that represent our vision of a good search UX. They have fewer options and therefore provide a faster execution\nCustomizable Connectors HOC’s which are UI-free, allowing you to fully adapt them to your desired search experience.\n\nA glimpse of the future for service based UI’s\nAlgolia is, at its core, a search engine. UI&amp;UX are very important elements that make for a great implementation and differentiate an unused search bar from a central navigation tool. Those improvements are very prominent when binding the search capabilities of an engine to the UI.\nThis new “flavor” of instantsearch provides more for the UI by channeling the capabilities of the search into abstract widgets. We provide our way of doing those… implementations, but we also let you implement new UI widgets through the prism of our experience on how to use our engine.\nIf you want to have a look at what the next version of instantsearch.js will look like, try react-instantsearch and let us know if we have the right balance of feature and customization (on twitter, gitter or by email).\nAs a final word, we would like to express our sincerest gratitude to Alexandre Kirszenberg who built the initial POC during his summer internship. Thanks!!!"
  },
  {
    "id": "5759-0",
    "title": "Inside the Algolia Engine Part 6 — Handling Synonyms the Right Way",
    "author": "Julien Lemoine",
    "text": "In any search engine, adding synonyms is one of the most important customization to introduce domain-specific knowledge. Synonyms make it possible to tune search results without having to make major changes to records.\nSupporting synonyms is no easy task especially for multi-word expressions, which introduce complexity in the handling of the proximity measure between matched words. We will describe in this post how we handle this complexity via a rewriting of word positions at query time.\nDifferent types of synonyms\nThe most common way to define synonyms is to use a set of expressions that are all identical. We call such a set an N-way Synonym because any expression of the synonym set is a synonym of another expression.\nFor example, if New York is unambiguously a City in your use case, you probably want to have a synonym set containing \"NY\" = \"NYC\" = \"New York\" = \"New York City\". This synonym set is configured as N-way, which means that:\n\n\"NY\" in the query also matches \"NYC\" or \"New York\" or \"New York City\"\n\"NYC\" in the query also matches \"NY\" or \"New York\" or \"New York City\"\n\"New York\" in the query also matches \"NY\" or \"NYC\" or \"New York\" or \"New York City\"\n\"New York City\" in the query also matches \"NY\" or \"NYC\" or \"New York\"\n\nLess common but still used are asymmetrical synonyms, in which you want to apply an equivalency only in one direction - we call this a 1-way Synonym. For example, \"smartphone\" = [\"iphone\", \"android\"] means that you want the \"smartphone\" query to be expended in \"iphone\" and \"android\" because you can have a record containing iphone without having the word smartphone. That said, you do not want the query \"iphone\" to returns all records containing the word \"smartphone\" - that wouldn’t be relevant for the user.\nThe Algolia engine supports both 1-way and N-way synonyms, and both types can contain any type of expression (one word or multiple words like \"New York City\").\nThe dangers of generic synonym sets\nWe are often asked by users when we will… include a standard synonym dictionary in the engine. The question is understandable - after all, we are taught in school that most words have synonyms, so it seems easy to package all this knowledge into one generic resource (per language). Unfortunately, it’s not that simple.\nThere are varying degrees to which two words or phrases are synonymous. For example, \"substitute\" is a stronger synonym to \"alternative\" than \"choice\" even if you can find both in a standard English synonym dictionary. Adding \"substitute\" as a synonym of \"alternative\" can be a very good idea for a website that compares technologies or offers; that being said, it can easily add noise on other use cases, especially if the name of a technology contains \"substitute\" in the name like the Substitute or jquery-substitute open source projects.\nPolysemy is present in all Natural languages, which means that a word can have different meanings depending on the context. For example, the word \"crane\" can be a bird or a construction equipment and the context is required to select the correct meaning. Our customers are all in a specific semantic context and the dictionary need to be specific to give the best results. If you are on a tech website you don't want to have a generic synonym of the word \"push\" because there is a strong meaning in tech and using synonyms for the verb or the noun \"push\" would lead to weird results.\nThe most useful synonyms are always very specific to your domain. This is the case of the synonym set [\"NY\" = \"NYC\" = \"New York\" = \"New York City\"] we mentioned before. This synonym set is valid if you only have cities in your use data set, it could introduce a bad relevance if you also have States!\nEven specific synonyms can have a different meaning in another use case. For example, I saw the synonym T-shirt = Tee on several ecommerce websites. This synonym makes sense as a lot of users are typing \"tee\" to search for a t-shirt. That said, this synonym is dangerous if you have a sports… website as you can have a query for \"tee ball\" or \"golf tee\" and won't like to return t-shirt!\nWith all those constraints in mind, it is close to impossible to package a set of synonyms that would make sense for all use cases as it is too much use case dependent. In practice it would lead to a bad relevance and results would be hard to understand for your users.\nThis is why we do not plan to package a generic synonym dictionary in the engine. That said, there are other resources that we already package in a generic way like the singular/plural forms of a language that we expose in the ignorePlurals feature. We plan to continue introducing such resource in the future that provides a lot of value without having the drawback described before.\nComputing synonym at indexing or query\nOne of the main question with synonyms is to make the choice between pre-computation at indexing time or expansion at search time. Both approaches have advantage and drawbacks:\nWith pre-computation at Indexing time, if you have a synonym \"NY\" = \"NYC\", you will index the document with both words when one of the two words is found in a record. This method is the best for performance as there is no OR between the two synonym words at search time. The biggest issue with this approach is to introduce the support of multi-words synonyms, which require introducing a new knowledge in the inverted list.\nFor example, if you have the text \"New York Subway\", the word \"New\" will be indexed as position 0, the word \"York\" will be indexed at position 1 and the word \"Subway\" will be indexed at position 2. If you have a synonym \"New York\" = \"NY\", you will have to introduce in your index the token \"NY\" at position 0 with a padding of 2 positions so that the query \"NY subway\" will have correct positions. This additional information has a big impact on the index size and performance that usually nullify a lot the gain of indexing in most use cases.\nConversely, with computation at search time, you will search for… all synonyms at query time with an OR operand between them. The big advantage of this approach is that you can compute the exact position of each matching synonym with a post-processing task (see next section for details). The disadvantage is that it requires more CPU as we have to compute the OR between several inverted lists.\nLastly, A hybrid approach is a mix of both approaches where all synonyms are searched at query time but some expensive computations are precomputed in one inverted list. Most of the time the expensive computation that is indexed is the search of consecutive words (phrase query). For example, the \"New York\" synonym would be indexed as one inverted list. The complexity of this implementation is to make it work with prefix search as we will describe below.\nThe implementation of synonyms in the Algolia engine is fully done at search time as we want to have an exact computation of the proximity (see next chapter for more details). We plan to implement a hybrid approach in the future where phrase queries will be pre-indexed in order to improve performance and have the best of both worlds.\nHandling multi-words synonyms is hard\nEven if our implementation is currently fully done at search time, we are still forced to rewrite the matched word position in order to have an exact computation of the relevance. We have explained in details this process in part 4 of this series in section 2.3.2.\nWithout this processing, having a synonym \"NY\" = \"New York\" would lead to different proximity for the query \"New York Subway\" on those two records:\n\n \nThe complexity is that \"New York\" is identified as position 1 and 2 in the first record, followed by \"Subway\" at position 3, whereas the second record contains \"NY\" at position 0, followed by \"subway\" at position 1. Rewriting this will ensure the proximity measure between the matched word will be identical for both records. This process essentially considers that \"NYC\" was composed of the two original words \"new\"… and \"york\" and will increase all following positions.\nThe following diagram shows the transformation done on the position of words to have an exactly identical proximity computation for records with a different number of words in the synonym (Note: words are indexed starting at position 0. For example, the first record is indexed with \"why\" at position 0, \"new\" at position 1, etc.)\n\nReducing noise in results\nSynonyms are just one of the various alternative expressions that are added on the queries. We also add typo tolerance, concatenation & split of query terms, transliteration and lemmatization. All those alternative expressions are described in the part 3 of this series.\nIn order to avoid adding noise, we do not apply the other alternative expressions on synonyms. This is the big reason why a search for \"New York\" won't give the same number of hits than \"NY\" when you have a synonym \"NY\" = \"New York\", records containing \"NY\" and \"New York\" will be retrieved in both cases but the record containing \"NewYork\" for example will be only returned for the \"New York\" query via the concatenation expression.\nThe following diagram explains the way we add alternative expressions, all steps are computed at the same time and are not re-entrant. In other words, we do not use the output of one alternative expression to feed the input of another step.\n\nPrefix match on synonyms\nHandling synonyms correctly in an instant search use case is pretty complex. Most engines fall short in this respect and have a bad user experience because of this processing. For example, let's say you have 5 records containing \"NY\" and 5 records containing \"New York\" and have defined the synonym \"NY\" = \"New York\":\n\nthe query \"n\" will match all words starting with \"n\", so records containing \"NY\" and \"New York\" will be matched: the 10 records are found\nthe query \"ne\", \"new\", \"new y\", \"new yo\", \"new yor\" will only match the 5 records containing \"New York\" as the synonym is not triggered\nThe synonym will only… be triggered when the query contains \"New York\", returning the 10 records\n\nIn order to avoid this flickering of search results, we consider that the last word of an expression is allowed to match as a prefix. So the synonym will be triggered as soon as the query contains \"new y\" (of course the synonym will also be triggered for the query \"new yo\", \"new yor\" and \"new york\"). This approach reduces a lot the problem of flickering as the synonym is added as soon as the last word is started and deliver an intuitive behavior:\n\nIt would be very complex to understand that the query \"ne\" match \"NY\" because we match a prefix of the expression \"New York\" (would be the case if we allow to match the complete synonym as a prefix)\nIt is pretty easy to understand that the query \"new y\" is matching as a prefix of \"New York\"\n\nThis is why, in the Algolia engine, the final word of a synonym expression can be matched as a prefix. Of course, we continue to add the synonym when the expression is only a part of the query, so the query \"New York subway\" will still match the synonym \"New York\" = \"NY\" and records containing \"NY\" and \"subway\" will match. This recognition of expressions at any position of the search query is not performed by most engines.\nDifference between original terms and synonyms\nMost users want to consider synonyms exactly identical to the search terms but this is not always the case, as we described before there is several degrees of synonyms and some synonyms are closer than others.\nWe have several tunings available in the engine to let you configure the way to define the expressions that should be considered as identical or not:\nalternativesAsExact is an index setting that specifies the type of alternatives that should be counted in the \"exact\" criterion of the ranking formula. As we explained in detail in part 4 of this series, it counts the number of terms that exactly match the query. By default, this settings is set to [\"ignorePlurals\", \"monoWordSynonym\"], which… means that the singular/plural alternative and the synonyms containing one word are considered as exact but a synonym containing several words won't be considered as exact as they are often semantically farther. You can consider them as exact by adding \"multiWordsSynonym\" in the array.\nThe synonyms dictionary also contains alternative corrections that are different semantically and should be considered as one or two typos. The spelling can be very far from the original words and so won't be cached by the typo detection, so you can define them manually. For the moment this feature is limited to single-word expressions.\nImplementation details matter!\nThe configuration of synonyms is a key aspect of the search relevance and should be configured in most use cases. We have seen in this post that there are a lot of implementation details in the way synonyms are handled that have a direct consequence on the user experience or the ranking.\nAs with most of the features we introduce in the engine, we have spent a lot of time designing synonyms and how they should behave. This is why we introduced their support progressively: we started with support of single-word synonyms in May 2014, followed by multi-word synonyms in April 2015 and a dedicated API to handle synonyms in June 2016.\nWe took the time to think about these implementation details and to our knowledge we are the engine that goes the deepest in terms of supporting as-you-type synonyms and multi-word synonym positions to have a perfectly identical ranking. We still have improvements to the way we handle the synonyms in our roadmap - like the hybrid indexing of multi-word synonyms to improve performance.\nIf you have any other ideas of improvements, we would be very interested to discuss them with you!\nWe recommend to read the other posts of this series:\n\nPart 1 – indexing vs. search\nPart 2 – the indexing challenge of instant search\nPart 3 – query processing\nPart 4 – textual relevance\nPart 5 –… Highlighting, a Cornerstone of Search UX\nPart 7 – Better relevance via dedup at query time\nPart 8 – Handling Advanced Search Use Cases"
  },
  {
    "id": "5697-0",
    "title": "How we tackled internationalization in our Zendesk integration",
    "author": "Matthieu Dumont",
    "text": "Zendesk customers are worldwide, coming from every continent. It’s no surprise that their Help Centers support multiple languages out-of-the-box. When we launched our Algolia for Zendesk integration, it shipped with English support by default, and you could extend it to handle other languages. Today, we’re proud to announce that our integration supports 30 languages.\nAlgolia has always been language-agnostic. You can search in an English, Arabic or Chinese text without touching the parameters; but each integration comes with some specific features. Searching in help articles is a specific use-case, and since we’re providing some front-end features (e.g. an autocompletion menu), we also had some text that needed to be translated ( e.g. “10 results found in 2ms”).\nOur initial release had some flaws that we quickly discovered by talking with our first integration users. We got great feedback from pretty big Zendesk users that needed multiple language support out of the box, like Dashlane, whose Help Center is available in English, French and Spanish.\nTL;DR\nI’ve learned the hard way that there’s no magic bullet. Languages are too different to expect being able to simply replace parts of your text easily. Some things aren’t obvious - for example, how one thousand is written numerically in English (1,000) vs. French (1 000)) - some languages have multiple plural forms and you can’t expect that the sentence construction will be the same in any other language. As soon as you want to have dynamic content in a sentence, you’ll need to use some form of templating logic.\nHow did it work before?\nWhen you call our front-end library, you can pass some parameters. We had exposed a simple  parameter, in which each key held either a string with the desired value for all languages or an object associating a locale with a translation.\n\nWe then decided that we wanted to embed a few languages directly inside the application by default, to ease our users’ life.… That’s when we’ve learned that the simple solution I had developed wasn’t sufficient at all.\nFlaws of the previous solution\nWe used Gengo to get our sentences translated in a first batch of 5 languages. Satisfied with the results, we ordered a total of 30 translations (most of the ones supported by Zendesk). Some languages had a grammar similar enough to the English one that the integration was straightforward. Others brought up issues with the current solution, at three levels.\nDifferent ways of displaying the same information (dates, numbers)\nWe have some great tools to display the same information in multiple languages.\nYour browser ships with  and . Unfortunately, those methods by default use the user’s localization. ECMA2015 has added the support for a new  parameter, but its support by browser vendors is still too low for us to confidently use it for an integration targeting our customers’ end-users.\nAt that time, we simply ignored the different ways of displaying numbers depending on the language. However, for dates, we used the great  library.\nPlural forms\nThis one is a very basic one, but I didn’t think about cases where a plural form was not needed in English but needed in other languages (and I’m French):\n\n\n\nForm\nEnglish\nFrench\n\n\n\n\nSingular\nFound in\nTrouvé en\n\n\nPlural\nFound in\nTrouvés en\n\n\n\nSomething that came as a real surprise to me though was that languages also have multiple plural forms:\n\n\n\nAmount\nEnglish\nCzech\n\n\n\n\n1\nresult\nvýsledek\n\n\n2 and 3\nresults\nvýsledky\n\n\nmore than 4\nresults\nvýsledků\n\n\n\nSentence construction\nSometimes it’s just the sentence construction which is completely different. In some languages, the words positioning can be totally different:\n\n\n\nLanguage\nTranslation\n\n\n\n\nEnglish\nNo results found for “query”\n\n\nGerman\nKeine Ergebnisse für “query” gefunden\n\n\nJapanese\n“query” の結果が見つかりませんでした。\n\n\n\nAnother small difference between languages can simply be on the punctuation side. You… have different quotes in different languages:\n\n\n\nEnglish\nCzech\n\n\n\n\n“query”\n„query“\n\n\n\nIt’s with all those cards in hand that we’ve started realizing we needed a better framework.\nWhat we ended up with\nWe kept the same logic for the translation object, but this time added the ability to have logic on top of it.\nThe static standalone sentences are still simple translated strings:\n\nOthers now are functions. Those function are called with access to the other translations () and take the dynamic parts as parameters:\n\nFor each locale (e.g. ), you can either override the root translation by using the  key, which will change the translation for every english-speaking locale (i.e. , ,  and ), or by providing a locale specific translation by using the full locale as a key (e.g. ).\nThe only exception for this is Chinese, where Simplified Chinese and Traditional Chinese are too different to have a common root.\nGoing further\nOnce this was done, we started thinking about how we could improve the relevance in each languages.\nThe first thing we thought about is related to the type of queries the users might send. On a Help Center, it’s not unusual to have users type “how to do …”. The issue with that type of query is the potential noise related to words as common as “how”, “to”, “and” or “my”, that are called stop words in natural language processing.\nA simple example speaks thousand words. Consider two articles:\n\n\n\nChange your password\nHow to delete your account\n\n\n\n\nJust follow this link to change your password\nClick on “Delete my account”. In case you change your mind, your old login/password will still work during 14 days.\n\n\n\nFor the query “How to change my password ?”, without any special handling of those frequent words, the Change your password article would not even show up because of the lack of the words “how”, “to” and “my” in the article. How to delete your account would instead show up.\nThat’s where stop words… handling kicks in. Algolia offers an automatic stop words removal feature, with the query parameter . You can either use it with  to remove all the stop words in all languages or limit yourself to a specific language. Activated with the example above, Change your password would come back first in the results list and How to delete your account second.\nWhile this works great for full queries, its behavior can be a bit strange when used with prefix search, because the words are used as part of the query while they’re used as a prefix (the query “ho” for instance would match “how”, but the query “how to delete” would only match “delete”). That’s why we chose another solution.\nAlgolia also provides another query parameter called , which can be used to specify which words aren’t required for a record to match. At each keystroke, we’re now looking at which stop words in the current language the query contains, and add them as optional words to the query. How to delete my account ? would still show up first because it matches more query words, but Change your password would show up in the results list.\nThis solution in the end brings a good balance between no stop words handling and the usage of , in terms of relevance but also user experience. This works especially well because the dataset we’re searching in is usually in the 100s of articles, so there aren’t that much relevant results for a query.\nNext steps\nNow that we have this framework, what could we improve upon?\nThe first thing would be to have a way to use language specific tags. We’re indeed displaying all the tags on the search result page by default, you can hide them with three lines of CSS, but having only localized tags could be an even better solution.\nAnother big improvement we will do will be to have one index per locale. This would allow us to move the whole stop words logic from the front-end to the Algolia index settings instead, which would save some space in the… front-end library.\nFeel free to contribute, the whole code is open-source, and we’d be happy to look at any feature you’d like!"
  },
  {
    "id": "5598-0",
    "title": "Powering Shopify store search with Algolia's search-as-you-type experience",
    "author": "Matthieu Dumont",
    "text": "Algolia's plug-in for Shopify store search\nToday, we’re excited to release Algolia for Shopify, our all-in-one Shopify store search app that brings Algolia to your store. We've built Algolia for Shopify with simplicity in mind, which means that we automatically keep your Algolia indices up-to-date with your store, and we've provided shop owners with a beautiful UI that works out of the box.\nOur mission at Algolia is to make great search available to everyone - website & application builders, and also their end-users. We've been working hard on our Shopify search integration to make sure it brings everything we've learned with our 2,000+ customers and makes it available to the 300,000+ Shopify store-owners at the click of a button.\n\nHow it works\nFor e-commerce websites, search is mission critical. Indeed, a user will often browse a website first to see what’s available, then search for the products he/she got interested in and checkout. By easing up the store search process, our aim is to improve the end-user satisfaction, increasing conversion rates and return rates of our customers.\nWe’ve worked hard to provide you with an easy to use, easy to install Shopify search application.\nOnce you've downloaded our App from the Shopify Marketplace, you'll create/login to your Algolia account & we’ll start indexing your products and collections right away. Those indices will be updated in real-time using Shopify webhooks, so you’ll never need to worry about indexing again.\nOn the indexing side, we’re simply creating Algolia indices, which means that you get all of the features Algolia provides by default: typo-tolerance, relevance tweaking, synonyms handling, analytics and a rock-solid infrastructure.\nOn the front-end side, you can either go ahead and create your own store search implementation following one of our guides or use the built-in front-end experiences provided with the app.\nAn auto-completion menu\nThe auto-completion menu will be available on any page… of your store, in any search input:\n \nAn instant-search page\nYou can also replace your current search page with an Algolia powered advance store search that features instant search.\n\nAs with all our integrations, it brings the speed, relevance and customizability of Algolia to your end-users, wherever in the world with our Distributed Search Network.\nBut don't take our word for it, try it live!, or even better, install it on your Shopify Store!\n\nGoing further\nWant to learn more? We have a few resources for you!\n\nInstall Algolia for Shopify\nCommunity page\nEcommerce Shopping Search\nMarketplace entry\n\n "
  },
  {
    "id": "5709-0",
    "title": "Searching camelCased parameters in API documentation: how we handled it",
    "author": "Maxime Locqueville",
    "text": "As you may already know, we love having great search on documentations we use daily. That’s the main reason we built DocSearch.\nDocSearch is live on hundreds of documentation sites, including our own. We believe DocSearch is the easiest and fastest way to navigate technical documentation. Because of that, we invest a lot of time in making it better and better by improving it on a daily basis.\nImproving your DocSearch\nOnce you setup DocSearch, there are three main areas for improvement:\n\nThe structure\nThe content\nThe search itself (indexing and/or querying)\n\nThe list is ordered by importance, meaning that if you find a relevance issue on a DocSearch implementation it’s usually due to either the structure or the content. Then, in very few cases, it’s due to the search itself.\nThe camelCase issue\nWe just came across one of those search issues: camelCased words.\nCamel case is the practice of writing compound words or phrases such that each word or abbreviation in the middle of the phrase begins with a capital letter\nIf you are an Algolia user you know that all our api parameters are camel cased: see our list of parameters.\nSearch for parameters is working but we found it far from perfect.\nLet me explain why.\nLet’s take, for example, one of the parameters from our doc: snippetEllipsisText\nIt’s 1 word but you understand 3 different words: “snippet Ellipsis Text”\nLooking at it split up, it makes sense to expect the search engine to be able to return results for the following queries:\n\n“snippetEllipsisText” (original name)\n“snippet Ellipsis Text” (split name)\n\nBut also:\n\n“Ellipsis” (middle word only)\n“EllipsisText” (two last words, not split)\n“EllipsisTex” (prefix query of “EllipsisText”)\n“Ellipsis Text” (two last words split up)\n“Ellipsis snippet” (split up, inverted first and second word)\n…\n\nThere is a few queries where you are not expecting results:\n\n“EllipsisSnippet” (not split inverted first and second… word)\n“TextEllipsis” (not split inverted second and third word)\n...\n\nIn plain words we want to match:\n\nThe exact parameter name (Because people might copy/paste it from their code to know more)\nAny combination of sub-word of the parameter name split up\nExact parameter name omitting 1 or more starting sub-words\n\nOne of the great features of Algolia is the highlighting. We describe in detail how it works in a previous blog post.\nSo we also expect, when searching for camel case, to have highlighting working correctly, meaning that if I search “ellip” I expect to see “snippetEllipsisText” in the result\nFor now we were handling only:\n\n“snippetEllipsisText” (the basic one)\n“snippet Ellipsis Text” because the engine tries to concatenate the query.\n\nThere will be a few search inputs like the one just bellow along the blog for you to try and understand the process. Those inputs will search inside all Algolia parameters (at the time of the writing)\nWorking queries: \"snippetEllipsisText\", \"snippet Ellipsis Text\",\n Not working queries: \"Ellipsis\", “EllipsisText”, \"EllipsisTex\", \"Ellipsis Text\", \"Ellipsis snippet\"\n\n\n\nAs you can see from the examples above, that’s 2 out of 7 working, which we can agree is bad.\nWhy we get those results\nUnderstanding why we are handling so few queries out of the box is the key to fixing it properly - let’s dive in. Algolia is doing prefix matches only (more details in this article). It’s one of the reasons Algolia is able to search so fast, but for our camel case use case it’s preventing us from searching in the middle of the word. So we had to find a way around that.\nThe iterative process to fix it\nIndexing the splitted content\nSince we want to be able to search the middle of our camelCaseWords we knew we had to index it as “camel Case Word” so basically “uncamelizing” the content.\nSo we started to look for existing librairies doing that (in python because the DocSearch scraper is built with python.\nWe… found the stringcase library which has a sentencecase function wich does the job of “uncamelizing” but there is two issues with such library:\n\nIt’s working too well :), what I mean by that is it’s going to uncamelize everything, like “API client” is going to become “A P I client”, we don’t want that to happen as the brains reads and understand it as “API client” not “A P I client”\nA camelCasedWord in the context of a documentation is usually surrounded by text and it’s not allowing us to know which words got uncamelized in the process (more on why we need that information bellow)\n\nSo we had to write our own:\n\nif a letter is preceded by an non-capital alphanumeric character we add a space, fairly simple.\nWith this in place:\n\n“snippet Ellipsis Text” gives the expected results\nwe can now search in the middle of the camelCasedWord\n\nBut:\n\nwe now have a display issue when looking for “snippet Ellipsis Text”\n“snippetEllipsisText” is not returning results anymore\nwe are still not able to have results for “EllipsisText”\nwe can know exactly which word in a sentence was camel cased\n\nWorking queries: \"Ellipsis\", “EllipsisText”, \"Ellipsis Text\", \"Ellipsis snippet\", \"snippet Ellipsis Text\"\nNot working queries: \"snippetEllipsisText\", \"EllipsisTex\"\n\n\n\nThat’s 5 out of 7 working, better but still not perfect\nFixing the remaining issues\nThe display issue\nAs mentioned, we now have a display issue. The content we show on the search result for the query “snippet Ellipsis Text” is not the one that you can see in the content and expect in the search result: “snippetEllipsisText”.\nWe came up with a nice trick. We looked for an invisible unicode character: \\u2063 (there are others but this one does the job) to put as a replacement for the space. This make the engine still considering snippetEllipsisText as several words while displaying snippetEllipsisText because the separator is not visible in a browser.\n\n\n\nThe _uncamelize_word… function code now looks like:\n\nLast but not least: the no result issue for “snippetEllipsisText” and “EllipsisT”\nSearching for “snippetEllipsisText” does not bring any result anymore since the index does not contains anymore the word snippetEllipsisText.\nSearching for “EllipsisTex” does not work because the word “EllipsisText” is not indexed, we indexed “Ellipsis” and “Text” but not “EllipsisText”.\nNote that EllipsisText is returning the expected result because it’s one typo away from “Ellipsis Text”, same for “EllipsisT”. It’s better but we would rather have the engine considering it as 0 typo\nFortunately the Algolia engine has a handy synonym feature.\nFirst thing first: “snippetEllipsisText”\nWe can just add a 1 way synonym:\nsnippetEllipsisText => snippet Ellipsis Text\nThen for “EllipsisT” in the end what we want is to have another 1 way synonym:\nEllipsisText => Ellipsis Text\nBut we need this to be generic. If we summarize we want to:\n\ncreate a synonym for the complete name,\nremove the first sub-word and creating a new synonym\niterate until only 1 sub-word remains.\n\nThe following schema should help you understand:\nLet’s consider “snippetEllipsisText” as “A B C”, we are going to create the following 1 way synonyms:\nABC => A B C\nBC => B C\nC => C we actually don’t need this one as it’s already handled by the initial splitting\nYou can have a look at the final code here.\nFinal result:\n\n\n\nHandling camel case seemed like an easy thing, but after having to handle it I can fairly say it’s not that simple after all, because it implies a lot of edge cases. The work we did here is improving a lot the search for parameters in our doc, and the search for all already live DocSearch implementations.\nOne area where DocSearch doesn’t shine yet is searching in generated api documentation from code like JavaDoc where camel case is omnipresent. This work is is a big step forward it making it available."
  },
  {
    "id": "5587-0",
    "title": "Inside the Algolia Engine Part 5 – Highlighting, a Cornerstone of Search UX",
    "author": "Julien Lemoine",
    "text": "Visually highlighting search results is a must-have component of a great search experience. This is even truer when you start to do advanced processing on the query (synonyms, typo tolerance, concatenation/split of query words, etc.), like we presented in the third installment of this series.\nA search result that is considered as weird by a user without highlighting can become smart just by explaining the processing done to retrieve it and by making it easy for the user to check if this is exactly the content they were looking for. In this article, we’ll show you in detail how we have implemented our highlighting to make sure it always provides a great user experience.\nOn this Google query, the first hit shows the standard highlighting done by Google. We removed the highlighting manually on the second hit - the result is much more difficult to understand.\nDifferent approaches\nHighlighting tends to be a subject that appears easy at first glance; however, the reality is much more complex, namely because it is a different process entirely than that of matching & ranking your objects. There are three main ways to implement highlighting:\n\n1. Take the query and the text to highlight and imitate the job of the search engine. This approach is partial as you don't have access to the processing done by the search engine like the extraction of synonyms. Usually this means that you just highlight the query terms, which will be misleading for users as they will not see why a record was found.\n2. Apply the query interpreter on the query to extract all possible extensions like synonyms and use that information to highlight a text. This approach will give you a good visual result as you will have all the alternatives. But you will have to test a lot of expressions that do not match against your text. There is a lot of waste of performance here.\n3. Apply the query in the search engine as usual but keep the matched terms for each result. This list of matched terms will be used by… the highlighter to process a record. This approach offers the best of two worlds: you have exactly the expected highlight whereas the highlighter remains fast and only focuses on the expression that is in the record.\n\nThe big advantage of the last two approaches is that your highlighter will be easier to implement. You don’t have to worry about alternatives like synonyms or typo-tolerance as it will already be resolved by your query interpreter. In other words, if your record matches with a typo, you will have the word with a typo as an input of the highlighter.\nIn the Algolia engine, we have used the third approach since day one. It was actually one of the many reasons to redevelop the Algolia engine from scratch. We had already developed several highlighters in the past and we knew from experience the third approach would be the best; however, we had to to keep all matched terms for each record, which needs to be done in a very efficient way in order to not create a bottleneck in term of CPU or RAM.\nDifferent expressions we highlight\nThere are four types of expression that the highlighter can highlight:\n\n1. A word: in this case, we need to find all tokens in the text to highlight that are identical to this word (with an accent and case-insensitive comparison).\n2. A prefix: in this case, we need to find all tokens in the text to highlight that start with this prefix (again with an accent and case-insensitive comparison). Usually, this word corresponds to the last query term that is matched as a prefix; however, it can also contain a typo (as we support typo tolerance on prefixes).\n3. A phrase: in this case, we need to find a sequence of words in a specific order in the record (also with an accent and case-insensitive comparison).\n4. A prefix phrase: identical as a phrase, except that the last word of the phrase can be matched via a prefix.\n\nAll those expressions come from the search engine and are an input for the highlighter, for example the user query… \"searchengine\" contains only one term but will also add the alternative phrase \"search engine\" which is the result of our split of query tokens processing (described in the third article of this series).\nExplaining the result using query terms\nThe highlighter is not just the process that adds some tags around matching expressions, it plays a bigger role in the user experience. You have potentially dozens of attributes in your objects used for search, displaying all of them would give too much information to the user. You only have to show the relevant one to explain the result.\nFor example, if you are typing the query \"Twilio IPO\" on your favorite news site, you will have several objects that will match. Some with both terms in the title like this one:\n\nAnd some with only one term in the title like this one:\n\nOn the first one, the highlighter will give you the information that all query terms were found in the title attribute (via the `matchLevel=full`), which allows you to consider a specific display of this article in the UI as only the title is required to explain the result.\nHere is the highlighter information on the title of the first article:\n\nOn the second article, the highlighter will give you the information that the title attribute is partially matching the query (\"matchLevel=partial\").\n\nThe highlighter gives you all information needed to explain the query, you can scan all attributes in the highlighter and only select the ones that \"explain\" one query term that no other one explains. Most of the time, you don’t have enough room to show every title and its content, in this case the highlighter will help you to show the content only when it's relevant to explain the result. This approach of explaining search results plays a big role in user engagement and improvement of your ROI on search.\nAn example of a search query where several attributes are required to explain the result: the movie title & the actor name.\nComputing a snippet\nWhen the text of an… attribute contains more than a few words like the content of a news article, you want to summarize it by keeping only the important sections. The result of this process is called a snippet and there are a lot of different approaches, so every snippet computation is different.\nIn Algolia, the snippet computation relies on the highlighting.\nThe first step of the process is to flag each token of the text to snippet with the corresponding query term that matches it. Then the second step is to find the window of N tokens that maximise the number of different query terms matched. You can have several windows of N tokens that contains the same number of highlighted terms, in this case we prefer to leave some text before and after the matching terms to give some context to the user.\nAlgolia lets you customize the number of words in a snippet, as this parameter depends on the UI you are building. In the example below, we will use 10 words for the  description of the two articles:\n\n\nThe two snippets actually return the 10 first words of the content as there is no 10 words window that contains both terms.\nYou can note that we do not return matchedWords attribute in the snippet as the result is partial. You need to use the highlighter to fully explain a result but you can, of course, request to have both the highlighted version and the snippet version.\nHow the engine identifies matching words efficiently\nOur highlighter is exhaustive while having very good performance, a big part of the hard work is actually done in the search engine itself in the identification and storage of all matched words for an object.\nDuring the query processing, we compute all expressions to find and create a description object for each of them that contains the expression to highlight and the link to the original query token. At the end of the query processing, we have a vector with all those alternative expressions.\nThen, when we create the boolean query that will be applied to the index, we keep… the link to the original expression. For example if you have the query \"Hotel NY\" with a synonym \"NY\" = \"New York\", the expression vector would be:\n\nWe would execute the following query:\n\nWhen a record matches, we know which part of the expression matched (list of integers). For example:\n\n\n\nA record containing \"hotel\" and \"NY\" will have a vector containing [0, 1]\nA record containing \"hotel\" and \"new york\" will have a vector containing [0, 2]\nA record containing \"hotel\" and \"NY\" and \"new york\" will have a vector containing [0, 1, 2]\n\n\n\nThis example is very simple as we have a very small number of expressions to match. In reality, we usually have hundreds of expressions to try because of typo tolerance and it becomes critical to identify only the one found in the record.\nWe finally keep this vector of integers for each result to be able to produce the list of terms to highlight and the list of matched words.\nWhy search without highlighting is bad\nAs soon as you have advanced query interpretation,highlighting becomes essential to a good  user experience. Having a great experience is more than highlighting the most important attribute, it is searching and displaying all attributes that are important for the user to understand why the result was displayed. This is key to help the user quickly decide which result they will choose first. Without this aspect, you will leave your user disappointed as they will inevitably choose a bad result.\nI hope this explanation has underscored why highlighting is such a complex and important topic of any search engine!\nWe recommend to read the other posts of this series:\n\nPart 1 – indexing vs. search\nPart 2 – the indexing challenge of instant search\nPart 3 – query processing\nPart 4 – textual relevance\nPart 6 – Handling Synonyms the Right Way\nPart 7 – Better relevance via dedup at query time\nPart 8 – Handling Advanced Search Use Cases"
  },
  {
    "id": "5574-0",
    "title": "Unveiling Algolia’s WordPress search plugin - blog search at the speed of thought",
    "author": "Raymond Rutjes",
    "text": "Today we are very excited to announce our Search by Algolia plugin for WordPress which will bring relevancy, instant search and autocomplete capabilities to almost 30% of the internet which is powered by WordPress.\nWe believe that every website or blog, irrespective of size, deserves good search so that users can easily navigate and find relevant content. \nWith our new plugin, every WordPress user will be able to leverage Algolia in just a few clicks, and replace the default WordPress site search with a blazing fast, highly relevant search engine.\nToday’s default WordPress search and most other search plugins out there are based on simple SQL queries, which aren’t high on the relevancy quotient, adds unnecessary load to your hosting environment, and slows down your site. They weren’t designed to handle complex user needs, such as typo tolerance, and they weren’t designed to scale with your content (in fact, they do just the opposite). We think the Search by Algolia plugin will solve these problems.\nEasy Set-up and Automatic Updates\nSince Algolia is a completely hosted service, you initially need to send the data. The plugin handles this automatically for you and lets you index the following content types by default:\n\nPost-Types: Including Posts, Pages and custom registered post types,\nTaxonomies: Including Categories, Tags and any custom registered taxonomy,\nUsers: All the contributors of the website.\n\nEvery time you update Posts, Taxonomy terms and Users, we automatically synchronize the data for you. \nNo matter how many blog posts, categories, tags, or users you have, the plugin’s built-in queue mechanism allows it to handle any amount of data.\nAlgolia makes it easy for you to customize ranking and configure indices in order to achieve optimal relevancy. When you use the Search by Algolia plugin for WordPress, we take care of everything for you, so you get the best search experience out of Algolia without any of the setup hassle.\nOut-of-the-box… Autocomplete and Drop-down menus\nMost WordPress themes display a search bar on every page of the your website by default. By enabling autocomplete, we add instant results at each keystroke in every one of your search bars.\nHere is what it looks like on SaaStr\nOur plugin provides you with a beautiful default layout for your dropdown results, and the templates can be completely customized, of course.\nBringing instant search results page to your WordPress\nOur plugin provides you with a fully featured instant-search results page. You simply have to enable the instant-search feature in the plugin and it will replace the default search page with a search page that includes filters and displays results as-you-type!\nThe newly provided search page will allow users to filter the content of your website based on content types, categories, tags or even authors.\nOf course the default appearance and behavior can be fully customized with just a little knowledge of CSS and JavaScript. If you're not comfortable customizing your WordPress UI, we’re confident you’ll be happy with the default design!\nOur Plugin is designed with the WordPress developer community in mind\nHere is what it looks like on ForEntrepreneurs.com\nThe whole plugin has been built with extensibility in mind and can be used as the foundation for every Algolia search experience on WordPress websites.\nHere are a few highlights:\n\nEasy to override templates (without requiring editing of the actual plugin files),\nOver 30 WordPress action & filters to customize and extend functionality,\nBuilt-in logging system to help understand everything that happens when building your own search experience,\nEvery feature can be enabled/disabled so that you can leverage the features that matter for your project,\nLast but not least, the plugin is thoroughly documented here.\n\nThe plugin is open sourced on GitHub. We invite anyone to submit feature requests or bug reports by opening issues on the repository:… https://github.com/algolia/algoliasearch-wordpress\nWe haven’t forgotten about you, WooCommerce users!\nIf you are a WooCommerce user, we’ve got you covered. We’re about to release an extension to this plugin, so stay tuned for the Search by Algolia for WooCommerce plugin by subscribing here (We will only send you 1 email to inform you of the release) \nWe wanted to mention beta testers and developers without whom this plugin wouldn’t be out yet!\n\nGaadiKey: https://blog.gaadikey.com\nFinanzdiskurs: http://finanzdiskurs.de/\nWooNinjas: https://wooninjas.com/\nMagnolia: https://magnoliamarket.com/\n\n\nTom Witkowski: https://github.com/Gummibeer\nRahil Wazir: https://github.com/rahilwazir\nAndrei Serban: http://andreiserban.com"
  },
  {
    "id": "5546-0",
    "title": "Why we're returning to Mind The Product for the second time this year",
    "author": "Liam Boogar",
    "text": "Earlier this year we participated in our first Mind the Product event in San Francisco as an exhibitor, and we’re excited to be returning this month to their London edition. We first discovered Mind the Product when we were looking at ways to get better acquainted with the product manager community, and we’ve been utterly blown away by our experience.\n\nThe event itself brings together some of the greatest minds behind product innovation today - as you can see in the recap of their San Francisco event above (in addition to some nice Algolia love at the end of the video), the event brings together an increasingly important (& large) section of the technology workforce, a workforce that we’ve been increasingly working with as we roll out Algolia to larger & larger companies.\nFor sponsors, the Mind the Product team is very committed to making sure that we get the most out of our experience, without compromising on the overall experience of the event. The speakers are hand-picked, and there’s plenty of time to attend sessions and meet with fellow attendees.\nAt our first edition, we made 100+ meaningful connections - entities as big as the Staples & Wells Fargo as well as burgeoning companies. Product managers are increasingly becoming champions inside their teams for new technologies, approaching product development from the user’s perspective. We’re committed to great Search UX, and product managers know how important that is.\nOne of the things we love about Mind the Product is that, as an exhibitor, we feel so special, because there are only a handful of select partners who exhibit. While we had originally planned on balancing team members at our stand and team members moving around the event to see what else was going on, we ultimately ended up with so many people on our stand that we had to keep all four of us on the stand to field questions and make new friends.\nFor this edition, we’re sending four people again - over 1,000 attendees came to San… Francisco, and the London event already sold out as well - our goal will be to have 100 meaningful conversations. In addition, we’ll be making the most of our time in London by speaking at a number of other meetups in London, like the Elastic London User Group & UX Connect at Google Campus London.\n\nA few tips to optimize your experience\nWhen we talk about Algolia, we often dive right into the technical aspects of search - the advantages & disadvantages of various implementations & options - however, we figured out quickly at MindTheProduct that it was much more important to focus on the impact that better Search UX has on your core business - conversion, retention, user experience - as we were mostly speaking with product manager & VPs of product, who have a much more high-level look at the evolution of their product.\nFor PMs and VPs of Product, Algolia often serves as a partner in helping them get their engineering team excited about using Algolia. Understanding how product managers fit inside of development teams is key to becoming a partner, and ultimately in helping them improve their product.\nA final tip is to give attendees something they can put in their pocket (or wear) and remember you by. Edible goodies are good for attracting crowds, but if you want to stay in their mind, give them something branded that they can use for weeks to come - mugs, bottle openers, even a backpack - and you’ll stay with them through at least breakfast (for the mug, of course!).\nIf you’ll be attending MindTheProduct or will be in London and want to connect with us, you can find us on our stand or ping us @Algolia and we can talk about Search & the future of Product Development!"
  },
  {
    "id": "5487-0",
    "title": "Search Party recap: 60+ attendees, 10 projects, 1 \"magnificent\" photo booth",
    "author": "Josh Dzielak",
    "text": "Several weeks ago we welcomed a group of Algolia developers, customers and partners into our SF office to have a conversation about search. It was also an opportunity to say \"Hey, thanks for working with us!\"\n\nFireside Chat\nThe evening kicked off with a fireside chat between Algolia co-founders Nicolas Dessaigne and Julien Lemoine. Some fun facts came out, like Nicolas and Julien having a combined 27 years of experience working on search. The conversation touched on company history, culture, and a few technical topics including our new premium SLA and the assistance we provide to customers for relevance tuning.\n\nDuring the Q&A someone asked \"How many records is Algolia storing today across all applications?\" “We don’t know” answered Julien, the candid response getting a nice chuckle from the crowd. He added “Today we count overall operations but not records, but maybe someday we will count those too.”\nCommunity Projects\nAfter the fireside chat I dismissed the crowd for a break—to get up, stretch their legs and grab a drink from our (deserted) island-themed bar. This was actually a mistake :/ It took several minutes to get people to quiet down and return to their seats. Lesson learned for next time!\nOnce things settled down I gave a presentation called 10 Community Projects in 15 Minutes. At Algolia we believe search is a conversation, a dialogue that helps you learn what your users want. Each of these 10 projects helps facilitate that conversation in some way:\n\nAlgolia Places\nDocSearch\ninstantsearch.js\nAlgolia JS Helper\nJekyll plugin\nZendesk plugin\nMagento plugin\nWordPress plugin\nOval Quotes\nSearchstone\n\nWhat do these projects have in common? They either use Algolia or help you use Algolia, and they're all looking for feedback, beta testers or contributors.\nPhoto Booth\nWe handed out brand new community-themed stickers and t-shirts—and then there was the photo booth 🙂 The booth was stocked with every search-related… knickknack we could get our hands on, as well as a few heavy-duty magnifying glasses I found at Pottery Barn.\nWho needs Snapchat when you have old school photo filters:\nThat's Yonas from StackShare trying to hide back there\nWhat are *you* looking at?\n \n\nLast Call\nPhotos of the event are available on our Facebook page in two different albums: Search Party #1 and Search Party #1 Photo Booth. If you couldn't make it this time don't worry, just stay tuned to hear about the next one.\nBig thanks to everyone who came out and helped make it a very special evening. All of us at Algolia really enjoyed hanging out and hearing from you. Until the next #searchparty! ??"
  },
  {
    "id": "5464-0",
    "title": "Bringing Advanced Search to Magento 2",
    "author": "Jan Petr",
    "text": "When it comes to eCommerce, the goal of search is to read the customer’s mind. Thatls what we set out to do with out Magento advanced search extension. When we released Algolia for Magento 1.0 last year, that’s exactly what we wanted to do. Algolia for Magento 1.0 has been starred over 100 times, and thanks to our amazing community, we learned a lot about how we can improve the shopping experience on Magento.\nToday we’re releasing our search extension for Magento to the growing Magento 2 community, bringing the same benefits of Algolia search to Magento 2 stores.\nWhat we learned from Magento 1.0\nSince releasing Magento 1.0 last year, our Magento community has grown to over 190 customers making 6 million queries per day. We’ve also seen a lot of interest from our very active Magento community on how we can make our extension even better. \nThrough a series of feature (and pull) requests, we were able to refine and iterate on Magento 1.0 to make it the best way to improve search in your store:\n\nWe cleaned up some code and tried to make the extension as developer-friendly as possible\n\n\nWe worked on our extension’s ability to synchronize with Algolia in real-time\n\n\nWe implemented the most common Algolia features directly onto the Magento dashboard so that customers could manage their Algolia settings in the same familiar environment they're used to. For instance our Synonyms feature is available directly on the Magento dashboard.\n\nWe also received a lot of requests for an extension for Magento 2. And we thought, why not? Today, we’d like to introduce you to the all new Magento 2 extension.\nUpgrading the search engine for Magento advanced search\nOur Algolia extension works with Magento 2 just as well as Magento 1.0 -  it replaces Magento’s default search engine, which uses simple SQL queries, but lacks basic features like autocomplete, fuzzy search or partial matches. We make it easier for shoppers to find what they are looking for.\nThe… Magento advanced search plugin also scales to any size of store, so you can have instant results even with very large inventories. Some of our customers are searching millions of SKUs.\nMagento 2 is completely synchronized with Magento 1.0 with regard to features, so if you’re looking to migrate your store onto the newer platform, the transition will be seamless.\nHere’s a look at some of the features we’ve incorporated onto our Magento advanced search extension:\nOut-of-the-box autocomplete\nAutocomplete or \"find-as-you-type\" search experience always makes the search more intuitive and goes a long way in shortening the distance between the end user and relevant content on the website. \nHow could we NOT include auto-complete for Magento 2?\n\nInstant-search results page\nWouldn’t it be awesome if search results were automatically updated for your customers with every subsequent letter of their search query? That's exactly what instant search gives you.\nInstant search not only refines search results with each keystroke, but also updates filtering options such as price range, stock availability, colors, sizes and many others. With every letter they type, shoppers get constantly refined results.\n\nSynonyms\nHave you ever tried looking for a TV but come up with absolutely nothing because it was called “Television”, not TV? \nWe know the feeling. \nThat’s why we integrated Algolia’s Synonyms feature directly into the extension - you can manage your synonyms right from your store’s admin interface!\n\nIndexing your Products\nIndexing your products is as easy as 1, 2...3. After you install the extension and synchronize your indices for the first time, you won’t have to worry about synchronizing data ever again. The extension uses Magento’s hooks and every time you add / update / delete a product in your Magento dashboard it will automatically be synchronized with the indices.\nInstalling the Magento 2 extension takes under 10 minutes with a just few lines of… code - or you can upgrade your store search directly through the Magento Marketplace.\nJoin the Algolia + Magento community\nWe’re on a constant quest to improve the quality of our products here at Algolia, and like last time, we’d love your feedback and support to take the Magento advanced search experience to the next level.\n\"The Balance Internet team love using and recommending Algolia Search-As-You-Type extension on all our Magento 1 and Magento 2 projects. The extension is easy to install, configure and customise for our clients. The search functionality delivers a fantastic user experience that is lightning fast and relevant. Our customers love being able to include CMS pages, categories and products in their Magento site search. In the past 7 years, I haven't seen another Magento search extension / platform come close to Algolia from a performance, functionality and price perspective.\"\n-- Kieran Smith, Solutions Architect @ Balance Internet - Australia's most experienced Magento Partner\nIf you are a Magento store owner, developer, system integrator... or just someone interested in our Magento advanced search extension, we want to hear from you! You can also try a demo with our extension here.\nDrop us a line at magento@algolia.com to become a member of the our Magento community and get exclusive priority access to trainings, newsletters and even workshops focused on our latest Magento related news and updates.\nThe extension is completely open-source and on GitHub. Feel free to fork the extension, send a pull request or open an issue. Any tweaks you suggest are highly welcome. Let's make the Magento world more navigable for shopaholics everywhere!"
  },
  {
    "id": "5415-0",
    "title": "For SLAs, there’s no such thing as 100% Uptime - only 100% Transparency",
    "author": "Adam Surak",
    "text": "With the advent of cloud computing and its ubiquity over the past 10 years, the SaaS model has, brick-by-brick, revolutionized the way businesses all over the world operate - from processing payments to processing paychecks in HR, from measuring marketing ROI to boosting sales efficiency, the modern enterprise’s go-to reflex when looking at improving how it operates is to turn to SaaS.\nWhile SaaS is undeniably an improvement over the former software licensing model, which came with bulky infrastructure, on-site maintenance, and less global scalability, SaaS is a global light switch that can instantly turn your business on and deliver it to literally billions of customers. But light-switches go both ways, and, as any enterprise that has ever suffered an outage knows, SaaS can cost big-time.\nJust recently, Amazon’s own search went down for a few hours - even the most conservative estimates suggest that the outage cost them tens of millions in revenue. Outages like these are inevitable; however, the strongest SaaS providers today invest heavily both in strong uptime and having a strong SLA, so that customers know that, in the event that an outage impacts their business, that they have insurance.\nToday, I’d like to talk a little bit about our brand new SLA policy, and how it came to be.\nWhat’s an SLA?\nBefore getting started, just to make sure we’re all on the same page, let’s get our vocabulary straight. \nA Service Level Agreement (SLA) is essentially an insurance policy in case the service you paid for doesn’t operate as advertised. In general, the more critical a service is to a company’s operations, the stronger the SLA will need to be to satisfy customer worries. Likewise, a stronger SLA is a stronger commitment from a service provider to ensure flawless service.\nAs a SaaS player and a customer of several external services for both our search platform and our operations, as a company we’ve seen (more than) our fair share of SLAs. As we evolved and… improved our own SLA over the past two years - providing an increasingly strong & transparent promise to our customers - we began combing through SLAs with a fine-toothed comb. It turns out, not all SLAs were created equal.\nBusting the myth of the 100% SLA\nIn recent years it’s become fashionable for companies to include 100% uptime guarantees in their SLA - and, in some cases, even more than 100%, despite its mathematical impossibility.\nNow, don’t get us wrong - all service providers have an obligation to put 100% of their effort into keeping their service running like a well-oiled machine; however, the detection of an outage itself can sometimes even be impossible… until it’s too late, of course.\nBeing a SaaS provider on the internet implies dozens of dependencies on intermediary devices and networks, which themselves have downtime. When you promise 100% uptime, every millisecond of downtime counts - however, what if you can’t detect the outage? How can one tell if the issue comes from your connection dropping data, the service provider, or any of the dozens of intermediaries in between?\nTo resolve this issue, SLAs define a minimum outage necessary in order to be triggered. The market standard is typically 1 minute; however, 1 minute of downtime per month means 99.9977% uptime - so what exactly is 100% uptime then?\nLet’s take a look at an example of a “100% SLA” from a SaaS provider on the market today:\n“credit of 5% of the fees paid for the month in which we fail to provide the stated level of service for every 0.05% of such a month, during which you are unable to transmit information up to an aggregate of 50% of the monthly service billing”\nThis SLA only kicks in after 0.05% downtime in a month, which is a little over 20 minutes of downtime - and yet, that same SLA claims 100% uptime guaranteed.\nHow we define our Enterprise SLAs\nWhen we set out to design our SLA, we had three goals:\n\nMake it Simple - you only ever look at an SLA before… purchasing and when there is downtime, and, in both instances, simplicity is key. \n\nMake it Transparent - no one wants unexpected surprises, especially if there is downtime. The easiest way to lose a customer is to let them down.\nTrust our platform - we trust the the system we built, and we want an SLA that speaks to that trust.\nAt Algolia, we currently have two different setups for our customers:\n\nEnterprise: we replicate your search on at least three different machines hosted by two different providers in different datacenters and autonomous systems\nPremium: we replicate your search on at least three different machines hosted by three different providers in three different data centers with three autonomous systems using at least two different Tier1 upstream providers.\n\n(Did we mention how inevitable dependency on intermediary services are?)\nThese setups are not different just on paper but they’re also different in terms of infrastructure and come with two different SLAs:\n\nEnterprise: 99.99% uptime, each minute of downtime would make you eligible for 100 minutes of refund, up to a cumulative value of 100% of the monthly service billing.\nPremium: 99.999% uptime, each minute of downtime would make you eligible for  1,000 minutes of refund, up to a cumulative value of 600% of the monthly service billing over a year.\n\n \n\n \nOur outage detection starts at 30 seconds (0.001% of a month) instead of 1 minute. This is so granular that it can’t be measured with traditional monitoring architecture, so we built our own monitoring network that continuously monitors our API infrastructure, that gives us a fairly unique ability to detect downtime this fast.\nHere’s what our refund policy looks like in practice:\n \n\n \n\n\n\nSearch down time\nTotal refund of the monthly service bill\n\n\nEnterprise SLA\nPremium SLA\n\n\n30 seconds\n0%\n1%\n\n\n1 minute\n0%\n2.3%\n\n\n5 minutes\n1%\n11.6%\n\n\n30 minutes\n7%\n70%\n\n\n45 minutes\n10%\n100%\n\n\n1 hour\n13.8%\n138%\n\n\n2 hours\n27.7%\n277%\n\n\n4 hours\n55.5%\n555%\n\n\n8… hours\n100%\n600%\n\n\n\nAs you can see, with our Premium SLA, if our service is down 45 minutes, we refund you 100% of your monthly bill - it doesn’t get much simpler than that. \nSLAs are more than just insurance\nMost people don’t really see SLA as much more than a form of SaaS insurance - at Algolia, we see it as something much greater, a way to remind our customers of our reliability. We back our Premium SLA with our reinforced infrastructure and our goal is to make sure we provide the best service in the market - we don’t want downtime any more than you do, and we put our money where our mouth is. We incentivize ourselves to do everything possible to ensure that the probability of an outage is as close to zero as possible! \nIt has been a year since we introduced our three provider set-up, and, with it, we’ve been able to placate the worries of even the most cautious of customers. \nOur setup has been extensively tested - with outages of entire datacenters and networks - and we’ve still been able to maintain 100% uptime.\nTo the best of our knowledge, our Premium SLA is unique to the market - in terms of simplicity, transparency & refund guarantee -  and we’d love to tell you more about it if you have any questions, or would like to see how your current SLA stands up against ours!"
  },
  {
    "id": "5418-0",
    "title": "Behind the Scenes: Algolia Places",
    "author": "Sylvain Utard",
    "text": "All of us have at some point or the other been stuck in the middle of nowhere, desperately trying to locate an address, with our “fat fingers” for company; or abandoned a purchase in our virtual shopping baskets because it was too much trouble to fill out our entire address. \nAlgolia being Algolia, we wondered how we could reduce instances of this in our own way and Algolia Places was born.\nIn case you haven’t heard, Algolia Places allows you to create beautiful address auto-completion menus with just a few lines of code; it can also be personalized and customized as per your use-case, not to mention enriched with additional sources of data. Today, we’d like to share the story of how we built Algolia Places with you. \nHow did we do it?\nStep 1: Data Collection\nTo build Algolia Places we relied on the open-source datasets provided by OpenStreetMap &amp; GeoNames. These datasets are actually very different from each other and we chose them for precisely this reason.\n\nThe OpenStreetMap dataset contains map data: it basically constitutes of the geo representation (polygons, lines &amp; points) of about 200 million geographical features.\nThe GeoNames dataset contains geographical names of about 9 million features: it’s a  regular list in the TSV format and contains names of every single city/country/place associated with some meta-data (population, zip codes, ...).\n\nStep 2: The Indices\n\n\nThe city &amp; country index\n\n\nIn order to build the city and country search experience, we exclusively used the GeoNames dataset - it’s a pretty exhaustive list and is quite simple to parse.\nFor every single city &amp; country name on the TSV files, we created an Algolia record. These records not only include the variations and translated names of countries &amp; cities (if available), but also some meta-data such as associated postcodes, populations, location and various tags &amp; booleans we use for the ranking/filtering capabilities of our API.\nThe result? ~4 million… records.\nThe address indices\nTo build the address search experience (including countries, cities, streets &amp; notable places), we used both OpenStreetMap and GeoNames. \nThe OpenStreetMap initiative is wonderful but the underlying data is not always on point. Why?\n\nIt may not always be exhaustive (especially for non-famous places)\nIt may sometimes include erroneous values (from our experience, postcodes used to be wrong)\nIt may contain duplicates\nIt may not always follow the same conventions across countries\nIt may lack some internationalisation/translations\n\nTo convert the OSM map data into Algolia records, we imported the whole OSM planet (40GB compressed) inside a PostgreSQL+PostGIS engine. The resulting database was a whopping 600GB!\nWe then used Nominatim to export this huge database to an XML format, rebuilding the hierarchy of places. This actually brought an interesting problem to light: the raw map data of OSM doesn’t have any hierarchy, for instance you don’t have an obvious hierarchy/link between San Francisco, California and the United States. Nominatim was used to rebuild that hierarchy in the exported format so another tool could easily process it.\nThe resulting XML file weighs 150GB and looks somewhat like:\n\nWe also built a SAX parser to process the file and rebuild the features hierarchy by following the “parent_id” and “parent_place_id” XML attributes.\nIt turned out to be a tiny bit more complex than expected\n\nA lot of features were associated with duplicates and inconsistency:\nSome cities have both “boundary/administrative” and “places/city” features, some have just one of them\nA street is composed of several segments, so you might see several features representing the same street\nThere are some street names which are common to several cities, so we needed to have multiple records within Algolia\n\n…..some hierarchy couldn’t be resolved\n\n\nSome features did not have any parents\nSome streets were attached to their countries… but not cities\nSome cities are also states, so the hierarchy was very confusing\n\n\nand some features also lacked some metadata.\n\n\nThey weren’t associated with the corresponding population\nSome counties didn’t have postcodes\nSome cities didn’t have translations\n\n\nDeduplication is generally super easy, but when you need to deduplicate 150GB of data, it leads to all sorts of new problems - in our case, we never seemed to have enough RAM! We also wanted to make sure that parsing was fast enough in order to avoid waiting days to build the Algolia records ... after all, milliseconds matter!\nSo we tried to leverage the previously built GeoNames index as much as possible to fix the missing data we could hit with OSM; but since these 2 datasets didn’t share the same IDs, it was obviously way more complex to aggregate.\nFor performance reasons (more on that under “Search strategy”), we decided to build multiple indices from those records:\n\n1 index for the whole planet (~20M records)\n1 index per country (~6M records for the US, ~1.5M records for France, ….)\n\nThat’s about 60GB of indices.\nJust so you have an idea, the overall parsing + record generation + indexing, takes ~12 hours at a time, today.\n The record schema\nHere’s what our final record schema looks like:\n\n&nbsp;\nStep 3: Index configuration\nWe constructed the underlying Algolia indices with the following configuration:\n\nSearch works with all localized names, but considers the default name as the most important\nThe names are considered as more important than the city, the postcode, the county, the administrative area, the suburb, the village and even the country.\n\n\n\nWe make sure to rank countries above cities and cities above streets, ensuring that we get the highest populated places first.\nIn case the population is not set, we fall back on the OSM’s importance field; which reflects the importance of the underlying place.\n\n\nStep 4: The search strategy\nWe built a custom API endpoint to query the indices… and implemented a custom querying strategy:\nIf the query is performed from your backend (and therefore doesn't specify any  query parameter) or your source IP address isn't geo-localized, the results will come from all around the world because we target the “planet” index.\nOr, if the query is performed from the end-user browser or device (and hence specifies an  query parameter) or has a source IP address that is geo-localized), the results will be composed of:\n\nNearby places\n\nPlaces around you (&lt;10km): this is a query using our  feature,\nPlaces in your country: this is a query targeting the specific country index,\n\n\nPopular places all around the world using a query targeting the “planet” index.\n\n\nSpecifying a country query parameter will override this behavior, restricting the results to a subset of the specified countries only.\nNumerical tokens in the query string are considered as optional words to make sure we always find the address even if the postcode &amp; house number are wrong.\nWe also defined a list of stopwords and tagged every stopword of the query string as optional.\nWe queried the underlying indices multiple times to ensure that:\nPopular places will always be retrieved first.\nNearby places will always be better than places in your country, which are better than world-wide results.\nIf both a city and an address match the query, the city will be retrieved first.\nIf the query doesn't retrieve any results, we fallback on a degraded query strategy where all words are considered optional and we only target cities.\n\nThe Result\nWe’ve had a great time building Algolia Places and the results today make all those coffee-fueled sleepless nights absolutely worth it. Here’s a quick look at them:\n\nWe’re hosting the Algolia Places infrastructure on 1 Algolia Cluster + 9 DSN replicas\n\n3 main servers in Germany\n4 DSN servers in France\n4 DSN servers in US\n1 DSN server in Singapore\n\n\n\n\nOur Algolia Places infrastructure is currently processing 60 queries… per second and provides answers in 20-30ms on an average.\n\n\nOur infrastructure is still at &lt;3% of its capacity.\n\n\nWe’ve reached 3200 stars on the GitHub repository of the frontend JavaScript library and a bunch of positive feedback.\n\nWant to use it?\nGo for it, it could be FREE!\nWe’re providing a non-authenticated free plan limited to 1,000 queries a day and we increase the free plan usage to 100,000 a month if you signup.\nTry Algolia Places now!"
  },
  {
    "id": "5393-0",
    "title": "Algolia (and Burgers) at PolyConf",
    "author": "Gianluca Bargelli",
    "text": "Have you ever had a conversation with your friends which goes something like this?\nYou: Let’s go grab some burgers for dinner tonight!\nFriend 1: Not burgers, anything else is fine.\nFriend 2: Thai?\nYou: Meh \nUs too.\nFood is something we’re very passionate about here at Algolia (along with coding). In fact, there’s an animated discussion about where we should go for lunch every single day - pretty much like the conversation described above.\nSo, in typical fashion we decided to build an app, aptly named Burger, that can help tackle this problem. And then took it to PolyConf 2016, held in Poznan, Poland.\nWhat Did We do at PolyConf?\nPolyConf, now in its eleventh edition, is a tech conference for programmers interested in a polyglot approach to software development. This year’s edition was held in Poznan and we had the chance to enjoy three intense days full of awesome talks about programming languages and technology. We also had the opportunity to host a workshop (more on that later)!\nThis edition was particularly focused on functional programming languages such as Clojure, Haskell, Oden, Crystal and Erlang / Elixir with a sweet spot for immutable data structures and a look at JavaScript’s interoperability with ClojureScript and Elm - both heavy sources of inspiration for Redux (In case you missed it we wrote a detailed blog post about Redux here).\nBack to the workshop. With Algolia being a polyglot programming company itself (we have over 15 API clients!), it wasn't very hard for us to work with a topic which could interest programmers with a large variety of skillsets. We decided to host a workshop which would teach participants to build a full-stack application using different technologies that we use daily.\nIntroducing Project Burger\nAfter a couple of discussions we decided to build Burger, an app which can be used to find lunch venues with people who have similar culinary tastes - a sort of meetup app for meals if you will; we wanted to mainly… demonstrate how to integrate different technologies and languages to build a conceptual -yet plausible- app from the ground up.\nHere’s a small demo of the App:\n\nWe implemented the app by using React, Redux and a REST API Service by using Scala, a JVM functional / Object Oriented language; we also indexed all the venues in Poznan by using FourSquare’s API in order to make sure it had some Algolia magic.\nThat way we had all the ingredients for our workshop: a little bit of Back-end, with some Front-end, topped off with an overview of how to integrate these technologies with a 3rd party API such as Algolia. Et Voila! Burger was born.\nIf you’re curious you can find the complete app source code on GitHub and some slides we used for the workshop introduction.\n\nWhat We Learnt Along the Way\nIt was our very first time organizing a 3-hour workshop and we learnt quite a bit along the way. Here’s a brief recap:\n\n Keep the scope small\n\nWe didn't quite realise while building Burger that it would probably be a bit hard to build an entire app from scratch in 3 hours. We ended up almost running out of time and just about managed to cover all the topics we needed to in order to have a functional application at the end. This is also because we tried to cover too many topics without accounting for enough time for the participants to apply them.\nIf given the chance to redo it, we’d probably narrow the scope: go for choosing either of the two - front-end or back-end.\n\n Focus on concepts and material\n\nWe took for granted that most of the workshop attendants were familiar with React and JavaScript Development in general; in reality we had some people interested in Scala and some others in the Redux portion, and so, consequently, some of the participants didn't get a chance to explore their favorite part in depth. \nIf we could go back we’d probably spend more time preparing material and exercises to keep participants engaged and involved until the end without focusing too… much on the implementation details.\nIntegration examples are great\nIntegrating a third party API into an application is something that every developer knows by heart; it’s a great example of how leveraging existing libraries can help achieve speed and productivity in such a short amount of time.\nWe wanted to demonstrate something similar in our workshop by wrapping our Algolia JavaScript client in a React component and use it for our app, saving the time and effort (out of the scope of the workshop) needed to implement an autocomplete component from scratch.\nWhat Else?\nPolyConf was a great experience for us and we were really lucky to find ourselves amidst coders with very varied skill-sets - in fact, after our workshop, we attended several sessions on functional programming and we found ourselves newly enriched with knowledge of Clojure and Elm - we actually hacked a possible first draft of an Elm Algolia Client!\nApart from the conference, we also got a bit of time to do some sightseeing and of course ate our fill of Burgers:\n\nHow could we not? \nOur Favorite Part\nOrganizing a successful workshop is no easy job but we believe we've learned some important lessons for the next round; we also hope that some of the attendees will try to apply the ideas we exchanged in their work or in their personal projects (and let us know, please!).\nAs developers, we absolutely loved PolyConf: interesting talks, nice food and a lot of ideas coming from different coder communities. It was a pleasant experience and we’d definitely like to thank the organizing crew, particularly Zaiste and Martyna. Thank you for making it happen! See you next year!"
  },
  {
    "id": "5299-0",
    "title": "How We Tackled Color Identification",
    "author": "Léo Ercolanelli",
    "text": "No matter how well crafted a search engine may be, if the query it is given is incomplete or inaccurate, the search results will always seem a little off.\nE-commerce often falls prey to this because large catalogs and innumerable filters complicate curation. Since relevancy is at the very top of our priority list, let us introduce a little experiment of ours, designed especially to enhance the search experience when it comes to image-driven content.\nThe Problem?\nThe goal we set for our computer vision experiment was to detect the color of the dominant object of a picture. Searching by color is a common occurence when it comes to e-commerce websites, and while the nature of the object is usually clear in the description, its color is often more of a problem. As an example, let us consider an article containing several colors in its description (for the sake of the example: white and blue), but having a picture clearly showcasing a blue shirt. While searching for a white shirt this article may show up and may even rank very high, because the relevant color - white, is present in the description. However correct this may be, having a number of images showing a blue object while searching for a white one does not give the best impression of relevance to the end user. The idea is certainly not to remove these results (because they are still correct), but to boost the ranking of the ones which more closely represent objects of the searched color.\nSome colorful context...\nWe are obviously not the first to attempt to address this problem, but we’ve taken a slightly different approach from the current solutions in the market. We can divide the applications trying to solve this problem into two different groups that provide different experiences:\n\nCommercial applications like vue.ai\nOpen source scripts like josip/node-colour-extractor or lokesh/color-thief\n\nThe open-source scripts listed are going for a different and simpler problem. They extract the palette composed of… the principal colors of an image. While they do the job well, they can’t be applied out of the box to fashion images because we are not interested in the colors in the entire image. Using them directly would result in the color of the background being detected as the main color of the given image. Moreover they do not provide a word for the extracted colors, and indexing RGB values would not improve the search experience.\nVue.ai showcases some impressive results and provides many more features than our experiment. We wanted to provide a quick, easy way for users to enrich their records a bit and improve their ranking, especially if they have to deal with poor descriptions in their records. This script doesn’t need you to register or anything: all you have to do is download, install dependencies, enjoy.\nOur approach \nChosen method\nThe problem we try to address is a relatively complex one and can be solved by a number of different ways. Most state-of-the-art computer vision frameworks these days adopt the Deep Learning path by using techniques such as Convolutional Neural Network to classify images. This is a path we decided not to take. Neural networks offer astounding results, but to do so they also need large datasets in addition to dedicated hardware which shortens the computation time. We started this project as an experiment and we began with simpler methods to see where those would take us. We always knew that we would eventually open-source this project allowing anyone to run it and add information to their records: deep learning frameworks can be somewhat hard to setup and run; they also require a great deal of computing time. The sequence of algorithms we chose was able to process images reasonably fast on a machine with a classic CPU and can be installed as simply as any other python package.\nPre-Processing\nBecause we only cared about the color of the dominant object in the picture, a high level of detail was likely to confuse the algorithms. Moreover,… smaller images meant less computing. After performing tests on several images we observed that a size of 100x100 was a good compromise.\nWe really had the use-case of a fashion e-commerce website in mind while designing this tool, and it led to the idea of an additional step to improve our color detection tool: cropping.\nThe object we are interested in is highly likely to be centered within the image, cropping allows us to reduce the size of the background image. Although cropping does yield better results overall, keeping less than 90% of the original image doesn't work well with background removal when considering a heterogeneous data set. This is due to the main object, sometimes touching the corners of the picture as a result of which it gets considered as  the “background” for subsequent algorithms.\nResizing and cropping (original on the left)\nExcluding background\nBe it plain white or complex, the background isn’t something we want to interfere with, while detecting the color of the main object of a picture. Separating the “foreground” and the “background” is not an easy task unless you know that it follows a simple convention (i.e. plain white), or exactly where the targeted object is placed in the picture. We make none of these assumptions to allow for broader use cases, and combine several simple computer vision algorithms to handle reasonably complex backgrounds.\nThe first step can be viewed as a thresholding over the image. We take the pixel values at the four corners, identify all pixels that are close in terms of color-shade and consider them to be a part of the background. We use the  metric to determine the distance between two colors. This step is particularly useful on more complex backgrounds because it doesn’t use information such as edges, but can’t discard gradient or shadows efficiently.\nGlobal thresholding struggling with shadows\nThe second step uses flood filling and edge detection to remove shadows and gradients. The… technique is largely inspired by this article on the Lyst engineering blog. The main goal is to remove the background- generally indicated by the absence of sharp edges. To achieve this, several steps of smoothing are used to maximize the efficiency of the Scharr filter used. Contrary to the first step, this step doesn’t behave well on complex backgrounds that contain edges themselves, or in cases where the object and the background don’t have sharp edges separating them.\nShadows are handled by edge detection and flooding\nBy using both of these filters we try to get the best of both worlds. However, this is not always achievable, sometimes one of the two steps does not accurately separate the background and the foreground and ends up removing way too many pixels. When this happens, the result of the step is simply ignored.\nExcluding skin\nA lot of pictures include models and if the targeted object is a swimsuit, the number of pixels representing skin can be greater than the number of pixels we are actually interested in. For cases like this, we implemented a really simple method to discard additional pixels.\nDetecting skin is an open problem and the possible solutions range from testing if each pixel color can be considered as skin to training a complex artificial intelligence model. We went for the lower end of the spectrum of complexity: a simple filter using only the color of the pixels to classify them.\nThere are a lot of skin filters out there, and each has their pros and cons. We chose one based on its false-positive rate. Indeed most of the filters only consider the color of the pixel - in fact it is very common (unfortunately) to see orange-tinted clothes entirely labeled as skin. Choosing a low false-positive rate reduces this risk but doesn’t remove it, and because all data sets don’t contain skin, we made this step completely optional in the final script.\nFor now, a single filter has been implemented which tries to be as versatile as possible,… however we definitely plan to implement several narrower ones to be able to target certain skin types more efficiently.\nDetecting skin pixels\nClustering pixels\nWe have no interest in the multiple shades of colors that images may contain, but only for the canonical, definitive names. To achieve this we use a clustering algorithm to group similar pixels together.\nClustering a gray T-shirt (from left to right: original, background, skin, clusters)\nAll pictures are not equal in terms of number of colors, and even if we want to group together different shades of the same color, we don’t want to say mix blue and green pixels. Because of this property, we can’t build the same number of clusters regardless of the image. We try to detect the best number of clusters, using the “jump” method for which a simple description can be found here on Wikipedia. This technique is relatively easy to understand. The error in a particular case of clustering can be measured using the following formula where  is the center of the cluster that  belongs to:\n &nbsp;  &nbsp; \nThis formula says that the greater the distance from a particular point to its attributed cluster’s center, the higher the error. This value will always decrease when clusters are added, until it reaches the point where  which gives an error of 0. However, this error won’t decrease at the same rate every time a cluster is added. At first, each added cluster will decrease the error by a huge amount, and at one point, the gain will become much smaller. The goal is to find the moment when adding a cluster is way less significant.\nThe algorithm uses a slightly more complex computation of error, but the idea behind it is the same. This way we adapt the number of clusters to have colors close to one another grouped together but still enough clusters not to group distinctly different colors on multi-colored items.\nRainbow clusters (from left to right: original, background, clusters)\nAttributing color names\nLast but… not the least we have to return readable names in a given language, say English, in the results and not just RGB values. This problem is harder to solve than it looks: the color space is huge (~16 million possible values), and different colors have very heterogeneous sizes. Defining each possible color range by hand is very tedious and yields poor results in a lot of cases. We turned to a K-Nearest-Neighbors algorithm to give color names to RGB values, thanks to the XKCD Color Survey. The XKCD survey consists of 200,000 RGB values labeled with 27 different color names (e.g. black, green, teal, etc.) that we use to train a scikit-learn KNeighborsClassifier.\nThis method is still far from perfect; for example, it is not able to handle shades of grey. We are working with the RGB system, and the grey colors form a plan in this colorspace. Because of this topology, there are never enough neighbors (using a euclidean distance) to categorize a color as grey. To solve this we added an additional step dedicated to these colors. By computing the  distance between RGB values and their projection on the grey plan, it is possible to tell how “close” the color actually is to grey. We then apply an arbitrary threshold to classify the color as grey, or continue with the classifier.\nThe overall process still has drawbacks. For example it doesn’t provide enough insights to be able to put a color in multiple categories (sometimes it is hard to tell if a color is green or blue for instance, and we may like to label it both green and blue).\nThe final result\nToday this computer vision experiment is available here on github as a stand-alone python script that anyone can run to enrich their records containing an image URL with the extracted color tags. Both a library for more advanced usage and a CLI are available for use as well.\nThis is as simple as it gets using the CLI:\n\nWe ditched the idea of a dedicated search engine optimized for images for to a simple reason: Algolia… already has a state of the art search engine, and not taking advantage of it would be a pity!\nA self-serve script fulfills another criterion close to our hearts: configurability. Our script uses sane defaults, but all steps described above can be tweaked to yield better results on a specific catalog. These tweaks range from making certain steps more or less aggressive to deactivating them entirely. We also provide a way to support any language: the dataset used by the script to give color names to images can be changed by the user, thus making room for non-english color names.\nOur script doesn’t use any kind of machine learning yet and we’re aware that results could be greatly improved by going down this road. For this first release our tool features ~84% accuracy. While this is still low, our tool isn’t designed to be used on its own, but in conjunction with your textual searches by boosting images whose colors match what is searched for."
  },
  {
    "id": "5333-0",
    "title": "How Algolia Is Powering Up ProdPad's Help Center",
    "author": "Pragati Basu",
    "text": "Most Saas companies today are still figuring out how to get customers to move towards self-service, especially when it comes to support. But one company has figured out how to make it easy to find all the answers.\nCustomers at ProdPad are consistently amazed at how lightning fast their customer support is, especially when they learn that the company is still a startup.\nTheir secret sauce? A blazing fast help center search, powered by Algolia.\nAndrea Saez, Head of Customer Success at ProdPad, tells us how ProdPad is using Algolia to provide consistently fast support to a growing customer base.\nWhat does ProdPad do?\nProdPad provides product management software that helps product teams collect ideas, identify priorities, and build flexible product roadmaps.\nHow are we handling a fast growing customer base without an army of customer support agents?\nWith the help of our incredibly fast knowledge-base powered by Algolia.\nSince we started using Algolia on our Zendesk Help Center, we’ve seen self-service become the preferred method of support for our customers.\nWhy?\nBecause they’re now finding answers on their own instantly, without having to go through the entire process of contacting our support team.  Even though our response times are surprisingly quick and our live support brings a 100% satisfaction rating, our customers prefer to help themselves.\nSelf-service is the best way for us to give our customers what they want - and Algolia is helping us make this a reality.\nHere’s how Algolia is helping us support over 500+ customers without breaking a sweat.\nOur Secret Sauce\nYou don’t know how much time you’ve been wasting until you use a search bar that just lights up with exactly the right article, even before you’ve finished typing.\nOur Help Center has always performed well - we worked hard to create a well-organized support center with useful guides and self-help resources. Usually, as a user, you might need to click around for a bit, try a few different… queries and multiple searches but you’d always find the right article.\nBut with our Algolia search, that same content is now instantly discoverable.\nThe impact  was immediate - we saw a dramatic drop in live support requests around the basic and recurring questions that used to make up a significant part of our support queue.\nWe’ve been able to create an experience our users absolutely love. They’re now finding support articles in just milliseconds.\nOur new users notice it the most because they’re usually digging around and setting up different areas of ProdPad during their magically extending free trial - and our search helps them find docs and instructions in less than a hot second!\n\nFast, helpful support is something we’ve become known for, and now Algolia is helping raise the bar again.\nWe have more control over our support experience\nConsumers generally try to help themselves before reaching out to live support. Today we’re able to monitor what they’re searching for so we can improve our overall product experience. \nWith our support traffic now divided between our knowledge-base and incoming support tickets, we’re constantly studying our data to understand how we can clarify and simplify information for our customers.\nOur goal is to reduce as much friction as we can during our customer’s support journey.\nWe’re using Algolia and Zendesk analytics together to dig for answers to questions like:\n\nHow many times do they need to contact support to fix a problem? \nWhich searches result in tickets being opened?\nHow long is it taking our customers to get the answers they’re searching for?\n\nFor example, we found that some users were searching for “google docs” and weren’t finding any results that contained that term. \nThey were looking for more information about our Google Apps integration, but couldn’t find it because we hadn’t included that specific keyword. It was an easy fix. I simply went in and added “google docs” to that… article to ensure that from then on, the right article would pop up straightaway.\nA quick change like this saves time and provides more bandwidth to answer individual support tickets. It also saves our customers from the hassle of opening up a support ticket for minor questions.\nAs we continue to make these improvements from our end, we’re making the support experience better for both our customers and ourselves.\nWe’re learning exactly what they’re trying to achieve\nWe can track queries in our Algolia analytics, so we know the kind of questions our customers are keying in and how they’re thinking.\nTake a look at our top queries:\n\nThis tells us what people are looking for, how many results come out of that search, and how many times users click on a given result.\nAs product managers, this kind of data is a treasure chest for us. What could we be explaining better? What are customers trying to do within ProdPad? Where are they getting stuck?\nWe can use it to understand what areas of our product are pain points for our clients. Conversely, we can use it to see which areas they’re most interested in. We can and do spot search trends which help us identify what our customers are trying to achieve.\nThis has not only helped us make improvements to the app itself, but the way we communicate our features and functionality across touchpoints, from our in-app messaging to our blog.\nThe Takeaway\nAs the overall number of support tickets in our queue drops, we’re seeing a greater percentage of customers reach out for help with setting up custom and increasingly sophisticated processes in ProdPad. \nThis is an excellent sign for us. It means they’re becoming increasingly proficient in ProdPad and are finding it worth their time and effort to invest in our more advanced functionality.\nThat’s how we’ve become the product management tool that everyone loves.\nSmall team, big impact - Algolia has made a big difference.\n "
  },
  {
    "id": "5278-0",
    "title": "DocSearch: 150 sites, 75k searches per day",
    "author": "Rémy-Christophe Schermesser",
    "text": "In December 2015 we released DocSearch, an easy way to make your software or API documentation searchable.\nFast forward 6 months later and DocSearch is powering the documentation behind 150 projects including React, Symfony, Play framework and Meteor.\nWith just a little configuration, DocSearch can automatically crawl most documentation websites and then provide a nice autocomplete search box to add to documentation pages.\nToday we have some exciting news for the DocSearch project:\n\nAn improved design is here with the release of DocSearch.js v2\nWe are open sourcing the full DocSearch crawling code!\n\nA better default design\nWith several iterations, we upgraded the DocSearch default design incorporating all the feedback we received from the community. The new design is available today with the release of DocSearch v2. \nHere's what it looks like on the Middeman website:\n\nFrom 0 to 75k searches a day\nDocSearch is now powering 150 documentation searches. Most of them are in English, but we also have sites in Turkish, Chinese, French, Japanese, etc.\nIn total we’ve indexed around 1.5M records and are performing 75k searches per day and growing. An interesting trend we’ve noticed is that most of these searches are made during weekdays. Who ever said developers worked on the weekend?\n\n\nThe last bar, starts on the 26th of June, that’s why it’s smaller.\nThe DocSearch open source stack\nDocSearch is composed of three different projects. as of today they will all be open source :\n\nThe front end of DocSearch (which was already open source): https://github.com/algolia/docsearch\nThe scraper which browses & indexes web pages: https://github.com/algolia/docsearch-scraper\nThe configurations for the scraper: https://github.com/algolia/docsearch-configs\n\nWant to try DocSearch on your documentation? Just follow these steps: https://github.com/algolia/docsearch-scraper\nThe scraper is a collection of submodules, each in its own directory:\n\ncli: A command line tool to manage… DocSearch. Run `./docsearch` and follow the steps\ndeployer: Tool used by Algolia to deploy the configuration in our Mesos infrastructure\ndoctor: A monitoring & repairing tool to check if the indices built by the scraper are in good shape\nplayground: An HTML page to easily test DocSearch indices\nscraper: The core of the scraper. It reads the configuration file, fetches the web pages and indexes them with Algolia.\n\nFuture of DocSearch\nWe want DocSearch to be easy to integrate and customize for everyone. To do that we are building a visual configuration tool that will help you auto-generate the DocSearch configuration and the styling of the autocomplete menu.\nRequest an awesome documentation search\nWe’re always looking for websites that are missing a great documentation search and are happy to help create it. We’ve seen DocSearch improve the search experience of many types of documentation and we think it could improve yours too.\nRequest your DocSearch now!"
  },
  {
    "id": "5252-0",
    "title": "How we reduced boilerplate and handled asynchronous actions with Redux",
    "author": "Alexandre Meunier",
    "text": "At Algolia, we do a lot of front-end JavaScript, because for us UX and UI are key components of a great search – we want the search experience to be perfect across the entire stack.\nRecently, we’ve been playing quite a lot with React in our projects, especially in Instantsearch.js, an open source library for building complex search UI widgets. So far our experience has been very positive; it was so good that we eventually decided to introduce React onto our dashboard, along with Redux to handle shared state across components.\nWe're really happy with the benefits that both have brought to our stack; in particular, we found that Redux brought quite a few cool things to the table:\n\nTestability: Because view components essentially hold no logic, it is very easy to test them – just provide them with data and spy functions, and check that it renders the way you want;\nSimplicity: We noticed that using Redux led us to write very clean and elegant code: no obscure features or leaky abstractions means that we always know what's going on and virtually never run into cryptic, framework specific errors like some of AngularJS's errors;\nDeveloper experience: There are great developers tools (i.e. check out Redux Devtools, it's awesome!) out there which allows you to easily inspect Actions and State mutations; on top of that you can also rely on very cool features such as hot reload and, even better, time travel: fixing bugs it's just a matter of rewinding / forwarding a set of actions, find the state change which caused the error, fix it and finally replay the actions.\n\nIf you are not familiar with React or Redux as yet, there are many great resources available online for both.\nReact and Redux give you great powers, but also great responsibilities ! You are free to handle many things exactly the way you want. However, to really harness their combined potential, we've found that setting up and enforcing conventions is crucial.\nOrganising your single page… application\nWe're using the following fractal directory structure. Each page is an almost self contained app, asynchronously loading its own subpages.\n\nThere’s nothing particularly controversial here, with the exception of one convention we’ve chosen to impose: To collocate Redux action creators and reducers in the same files, following the Ducks proposal. Our “actions” files look something like this:\n\nThis allows us to use default imports for the purpose of creating the combined reducer when creating the store, while still being able to used named imports in containers to import just the actions that are needed.\nReducing boilerplate in ...reducers\nWe found that whenever we were writing reducers and action creators, we were often writing duplicate action creators and reducers, doing little more than updating a subset of the reducer’s state, for instance:\n\nStrictly speaking, action creators are not required since components can directly dispatch actions by themselves. However, we found that working with the more abstract action creators, allowed us to write more modular components, which would literally need no knowledge of how the reducers work or which action types are used. Instead, we simply need to pass them data and (wrapped) action creators.\nTherefore, we looked into how we could simplify the reducer side of things, instead of the action creators. After a few iterations, we ended up with redux-updeep, a strongly opinionated createReducer implementation which assumes that the majority of the actions will result in deep updates of the reducer’s state. It allowed us to write the following code:\n\nHow does it work ? As mentioned before, it handles all unspecified action types in the same way, using updeep to immutably merge the action’s payload into the reducer’s current state. While it is still possible to explicitly specify action types and reducers, we still haven't felt the need to do so!\nGranted, when using redux-updeep, it becomes more… complicated to compose reducers, which is idiomatic Redux. However, we've made it such that it is very easy to write factories that allow us to parameterize the action type's namespaces as well as the path at which the update is merged:\n\nThen, it becomes possible to use the initial action creator deeper into a reducer state:\n\nWe're pretty happy with our current set up, so we thought that we would share it with the world. In fact, we've just open sourced it ! Find it on GitHub.\nHandling asynchronous actions – and reducing more boilerplate\nUsing thunks ?\nBy default, Redux does not care how you handle asynchronous actions. The redux-thunk middleware is available but enforces no rules: it simply enables you to dispatch multiple actions from a single action creator. Using it, you can for instance easily dispatch loading/success/error actions in response to a Promise:\n\nThis is a great start, but the code still relies on a lot of boilerplate. If you have as many simultaneous asynchronous actions as we do, and want to handle them all in the same way (e.g. how to keep track of the pending state, how to handle errors), it rapidly becomes a tedious task.\nLess boilerplate, more magic!\nThere is a lot of middlewares in the Redux ecosystem designed to handle asynchronous actions and/or promises, but we couldn’t find one that would handle a few conventions that we had initially defined for handling asynchronous actions and asynchronous data in the components:\n\nReducer states can have several keys that will ultimately be assigned a value (let’s call those “eventual values”);\nWe want to be able to enquire whether a particular value is ready or not, without resorting to isXXX boolean flags;\nWe wanted to be able to dispatch eventual values like normal values in action creators.\n\nSo we decided to create one, the redux-magic-async-middleware, and we've just open sourced it! It is optimised to work with redux-updeep, and enables dispatching promises in payloads like… synchronous values:\n\nThe middleware will automatically extract the promises from the payload, and replaces them with eventual values. When a promise is resolved, it will trigger a success action which will update the resolved data into the reducer states, thus resolving the eventual value.\nIn combination with this, we have created the eventual-values library, very much inspired by another eventual values library. This library is extremely simple, but allows us to encapsulate and abstract the behaviour that we desired around asynchronous values. It allows to you write code like this:\n\nWhat's next ?\nWe're still experimenting with those concepts and tools, and are gradually introducing flow typing in our codebase. Watch this space for more updates on how we use React, Redux and JavaScript in general!"
  },
  {
    "id": "5220-0",
    "title": "Inside the Algolia Engine Part 4 — Textual Relevance",
    "author": "Julien Lemoine",
    "text": "The way we search has changed a lot in the past decade. The original function of the Enter key was to begin a search, today it’s used to select a result that’s already been displayed. The type of information that people search for has changed too.\nToday’s search engines are used for much more than just web sites and documents—they’re also used to find specific items like people, places and products.\nThe Algolia engine was designed with both of these trends in mind. Algolia’s sweet spot is providing instant, relevant access to millions of small, structured records (and now from anywhere in the world). In this blog post, we’ll talk about how the Algolia engine computes textual relevance and why it works well for exactly these kinds of records. By textual relevance, we mean the letter and word-based aspect of ranking query results. Textual relevance does not include business relevance, which is used to provide a custom ranking based on non-textual information like price, number of followers or number of sales.\nIn a previous post we discussed how our ranking formula works, specifically how it uses a tie-breaking algorithm to rank each component of textual relevance one-by-one. In this post, we’ll look at each of the five individual components it uses in detail: number of typos, number of words, proximity of words, importance of attributes and exactness.\n1. The five components of textual relevance\nEach of these components address a unique relevance challenge while also playing their part in the global relevance calculation.\n\n1.1 Number of typos\nThe number of typos criterion is the number of typos in the query when compared with text in the record. This criterion allows you to rank results containing typos below results with the correct spelling. For example, in a search for \"Geox CEO\", you would prefer to have the \"Geox\" result (a typo-free match) displayed before \"Gox\" (a 1-letter-off typo). This example has a twist—”Gox” is not actually a typo in… the usual sense (it’s a correctly spelled proper noun) but it’s still possible that a user could have mistyped “Geox” when searching for it.\n\n\n\n1.2 Number of words\nThe number of words criterion is the number of query words found in the record. This criterion applies when the query is performed with all words optional (a.k.a. an \"OR query\"), a technique commonly used to avoid displaying no results. Here’s a simple example. If you search for \"supreme court apple\" you would prefer to have the first record before the second one as it contains all 3 of the words in the query.\n\n\n\n1.3 Proximity of words\nThe proximity of words criterion is the distance between the matched words in the result. It’s used to rank records where the query words are close to each other before records where the words are farther apart. In the example below, you can see two different results for the query \"Michael Jackson\". In the first record, the proximity criterion is 1 - the terms ‘Michael’ and ‘Jackson’ are right next to each other. In the second record, the proximity criterion is 7 because there are 6 words between the matching terms. A lower value is better, i.e. records with a lower proximity value will be ranked about records with a higher value.\n\n\n\n1.4 Importance of attributes\nThe importance of attributes criterion identifies the most important matching attribute(s) of the record. Since records can contain more than one attribute, it’s not uncommon for one query to match multiple attributes of the same record or different attributes of different records. However, a match of one attribute might be more important than another when it comes to ranking, and that’s precisely why we have the attribute criterion. An example is shown below. Given a query for “Netflix” in a collection of articles, you’d want the title attribute match to carry more weight than the description attribute, hence the title-matching record is displayed first.\n\n\n\n1.5 Exactness\nThe exactness… criterion is the number of words that are matched exactly. This criterion is very important for building quality instant (search-as-you-type) experiences because it’s important to display full-word matches before prefix matches. See the example below. If you search for \"Prince\", you want the exact word match to appear before the match where “Prince” is being considered as a prefix of “Princess”.\n\n\n2. How the criteria are computed\nWe’ve just seen what each criterion does and how it can be used to influence the ranking of search results. Next, let’s look at how the five criteria are computed and what we’ve done to make their computation efficient.\n\n2.1 Computing the number of typos\nIn our previous post about Query Processing, we explained how alternative terms are added to queries. For each alternative term, we also have an associated number of typo criteria (which corresponds to the edit-distance). This value of the overall typo ranking criterion is the sum of all typos used to find the match. For example, if the record is found via 2 words having 1 typo each, the typo criterion value will be 2.\nIt’s a very good practice to have this criterion at the top of the ranking formula. However, there is one exception: typosquatting. If you operate a service with a lot of user-generated content, you might already be familiar with this problem. Let’s take Twitter as an example. If you search for \"BarakObama\" on Twitter (note the typo), you would want the official @BarackObama account to appear before the typosquatting account @BarakObama. However, today it does not (as of June 2016). Typosquatting is a complex and difficult challenge.\nOne way that we recommend to handle typosquatting is to add an isPopular boolean attribute to the record of popular accounts and then to put it in the ranking formula before the typo criterion. On Twitter, isPopular could be set to true if the account is verified. This approach ensures that the official account with a typo… will be displayed before any typosquatting accounts, no need to hide any results. For this approach to work well, the number of records with isPopular set to true should be a relatively small percentage of the dataset (less than 1%).\n2.1.1 Typo tolerance tuning\nThe Algolia engine provides several different ways to tune typo tolerance for specific words and attributes:\n\nminWordSizefor1Typo and minWordSizefor2Typos: Specify the minimum number of characters in a query word before it can be considered as a 1 or 2 typo match.\nallowTyposOnNumericTokens: Specify if typo tolerance is active on numeric tokens (numbers) present in the query.\ndisableTypoToleranceOnWords: Specify a list of words for which typo tolerance will be disabled.\ndisableTypoToleranceOnAttributes: Specify a list of attributes for which typo tolerance will be disabled.\nAlternative corrections: Specify a set of alternative corrections with the associated number of typos.\nadvancedSyntax: Add quotes around query words to disable typo tolerance on them.\n\n\n2.2 Computing the number of words\nComputing this criterion is relatively easy, it’s just the number of words from the query that are present in the record. Each word might be matched exactly or matched by any alternative form (a typo, a prefix, a synonym, etc.).\nThe position of the words criterion in the ranking is important and will impact results. For example, the query \"iphone case catalist\" with all terms as optional will have two different ways to match the record \"Catalyst Waterproof Case for iPhone 6/6S Orange\":\n\nIf words is before typo in the ranking formula: The ranking will be set to words=3 and typo=1 as matching more words is important to the ranking (even if Catalyst is found via a typo).\nIf typo is before words in the ranking formula: The ranking will be set to words=2 and typo=0 as less typos are more important to the ranking (in this case it’s better to not match the Catalyst term and therefore not introduce a typo).\n\n\n2.2.1 The… removeWordsIfNoResults parameter\nBecause of situations like this, we usually recommend against performing queries with all terms optional. Instead, we recommend using a specific query parameter that we’ve added to the API to help with these types of situations. The removeWordsIfNoResults parameter instructs the engine to first test the query with all terms as mandatory, then progressively retry it with more and more words as optional until there are results. This parameter gives us the best of both worlds (predictable behavior and always showing results) and also does not pollute faceting results. When all terms are optional there is a large increase in the number of matched records, as only one matching word is required to add the record to a facet. For example, in a query for “iphone black case”, any black item will be matched which will have a big impact on the quality of the faceting.\n\n2.3 Computing the proximity of words\nThe proximity criterion’s value is the sum of the proximity of all pairs of words that are next to each other in the query. Computing the proximity of \"iphone release date\" is done like this: proximity(\"iphone\", \"release\") + proximity(\"release\", \"date\").\nThe engine stores word positions at indexing time in order to efficiently compute proximity at query time. We define the distance between two words A and B as the shortest distance between any A and B in the record (A and B may appear more than once). We increase the distance by 1 if the match with the shortest distance is not in the queried order.\nThe computed proximity between two words is capped at 8. If the words \"apple\" and \"iphone\" are separated by a distance of 10 or 100 words, the distance will still be 8. The goal is to avoid creating different proximity values for words that are not used in remotely the same context. In practice, 8 words is a sizeable distance between any 2 words, even when it’s mostly common words between them. Capping the maximum proximity has two other… advantages:\n\nIt helps our tie-breaking algorithm. A maximum distance of 8 means that we don't know if a distance between words of 10 is better than 100, and usually we don’t want to (it’s just noise). Capping the proximity at a small number prevents introducing false positives into the relevance calculation. More importantly, it lets the other criteria in the ranking formula take over when proximity values are not meaningfully differentiated.\nIt enables a lot of optimizations on the storage and computation side, as we will see.\n\n\n2.3.1 How word positions are stored\nAs we index a document, we assign an integer position to each occurrence of each word. For example the string \"Fast and Furious\" would be indexed as:\n\n\"Fast\" at position 0\n\"and\" at position 1\n\"Furious\" at position 2\n\nAdditionally, we assign positions to some word prefixes as described in our blog post about indexing prefixes.\nThe position is also used to efficiently store associated information:\n\nWe use a single value to store the position of each word in the text and which attribute it came from. We keep the position of the first 1000 words inside each attribute and associate an number to each attribute (the first attribute will have positions 0-999, the second attribute will have positions 1000-1999, etc.)\nFor multi-valued string attributes, we need to ensure that the last word of a string isn't considered to be next to the first word of the next string. To do that, we increment the first position of each string by 8 each time we move from one string to another. For example if a record contains the following “Actors” attribute, we will store the word \"Diesel\" with a position of 1 and the word \"Paul\" with a position of 10 (+8 compared to the value in the same string). This will generate a proximity of 8 if you type \"Diesel Paul\", which is the highest value for proximity.\n\n\n\n2.3.2 Bonus challenge: multi-word alternatives\nComputing a proximity distance can be very complex when you support… alternative word matching, e.g. synonyms. If you have a synonym that defines \"NYC\" = \"New York City\" = \"New York\" = \"NY\", you would like to consider a record that contains \"NYC\" as strictly identical to a record that contains \"New York\". We solve this tricky problem by rewriting all word positions at query time.\nTime for an example. Let's say that you have two different articles that you want to be searchable via a query for the \"New York subway\". You would like the proximity distance calculation to be the same between them. Here are the article titles:\n\n\"Why New York Subway Lines Are Missing Countdown Clocks\" \n\"NYC subway math\"\n\nWhen this query is run, the first record will be matched via three words (\"new\" at position 1, \"york\" at position 2 and \"subway\" at position 3) and the second record will be matched via two words (\"nyc\" at position 0 and \"subway\" at position 1).\nIn order to avoid this inconsistency, we rewrite positions using a multi-word expansion. For the \"NYC subway” query, we apply the following transformation:\n\n\"Nyc\" at position 0 became:\n\nVirtually \"new\" at position 0\nVirtually \"york\" at position 1\n\n\n\"subway\" at position 1 became:\n\n\"Subway\" at position 2 because we introduced \"york\" word at position 1.\n\n\n\nAfter the transformation, we have exactly the same proximity measure between the two records. This rewriting is entirely cumulative. For the \"New York\" query, if a record contains \"NYC\" at positions 5 and 15, it means that all original positions greater than 15 will be increased by 2 and all original positions between 6 and 15 will be increased by 1.\n\n2.3.3 Attribute-aware proximity\nWhen computing the proximity score, we don’t want to be misled when different parts of the search expression are found in different attributes of the record.\nSay you’re looking for \"soup of the day\" and you have a record with \"soup\", \"of the\" and \"the day\" separately in three different attributes. In this case we do not want to compute a total distance of 10… (distance(soup, of)=8 + distance(of, the)=1 + distance(the, day)=1) because it would mean “of the day” are close together, which is not the case. Instead, we want a total distance of 17 (distance(soup, of)=8 + distance(of, the)=1 + distance(the, day)=8). The distance of 8 between \"the\" and \"day\" tells us that \"day\" was found in a different block than \"of the\". We use a dynamic programing algorithm to do this computation efficiently with a cost of O(N) (N being the number of matched positions).\n2.3.4 Proximity computation tuning\nThe minProximity index setting allows to change the minimum proximity to something that fits your data better. If you set minProximity=2, it would mean that the query \"New York City Subway\" will have the same proximity score whether the record contains \"New York City Subway\" or \"New York City's newest subway\". The distance would normally be 1+1+1 for the first record and 1+1+2 for the second, but setting minProximity=2 means that all pairs of words with a distance less than or equal to minProximity are counted for 1. In this example, both records are given a proximity criterion score of 3.\n\n2.4 Computing the most important attributes\nAs a reminder, the attribute criterion lets us say that a query match in some attributes is more important than others.\nEach attribute is indexed as ordered or unordered (this is configured in the index settings). Indexing an attribute as ordered means that the position of the best matched word inside the attribute is taken into account during ranking. So earlier words will matter more. Indexing as unordered means that the position within the attribute is not taken into account. Word position is not important.\nAn example—if you index the title attribute as ordered and description attribute as unordered, the query “Netflix” will return the following 2 records in the order displayed.\n\n\n2.4.1 When proximity is before attribute in the ranking\nThere are two strategies to compute best-matched attributes and… they depend on the relative position of proximity and attribute criteria in the ranking formula. Let's illustrate them by using the query \"VMware CEO\" to retrieve the following record:\n\n\nIf proximity is before attribute in the ranking formula, we first find the location with the best proximity in the record and we then compute the best attribute on this location. On the record below, the query “VMware CEO” will have an attribute criterion of 1000 as the best proximity is found in the description attribute instead of the title attribute.\nIf attribute is before proximity, the best-matched attribute will be decided by the position of the best-matched word in the record. In this case, the query “VMware CEO” will have an attribute criterion of 0 as the best word is matched in the title attribute.\n\nWe recommend keeping the proximity criterion before attribute criterion. Proximity usually leads to a better identification of the best-matched attribute.\n\n2.5 Computation of the exactness criterion\nThe exactness criterion represents the number of words that match entirely, not one as a prefix of another. This criterion works well when the query contains several words but the one-word case requires a modified approach. In an instant search use case, if a user has typed “was”, there is a greater chance that he or she might be in the middle of typing a search for “Washington” and not just stopping at the verb “was”. Displaying “was” before “Washington” in the results at that moment would not be ideal.\nWe have implemented three different ways to compute the exactness criterion for a single word query, each specified via the exactOnSingleWordQuery parameter:\n\nexactOnSingleWordQuery=none: The exact criteria is set to 0 for all one-word queries.\nexactOnSingleWordQuery=attribute: All records that have an attribute string exactly equal to the query will have exact=1. This option is the default as it works well for databases that contain names, brands,… companies, etc.\nexactOnSingleWordQuery=word: All records will have exact set to 1 if there is a word that matches exactly and is not in our stop word dictionary (see the removeStopWords option for more information). For the query “prince”, a record with “prince” will have exact=1 and a record with “princess” will have exact=0. For a query “was”, both records containing “was” and “Washington” will have exact=0 because “was” is in the stop word dictionary.\n\nThere is unfortunately no magic option that works well with all use cases, which is why we introduced the exactOnSingleWordQuery parameter. We recommend using exactOnSingleWordQuery=attribute if you have a database that contains lots of one-word entries like brand names, account names, movies, etc. We recommend using exactOnSingleWordQuery=word when you have content like titles that have a low probability of matching an entry using only one word.\n\n2.6 Designed for small records\nThe five components of textual relevance we’ve discussed are uniquely designed for small, structured records. A few of them require some fairly complex processing, like the query-time rewriting of word positions for multi-word alternatives, but we think that having an approximation-free, exact value computation is worth it.\nIf you don’t automatically have small, structured records—i.e. if you’re starting with documents or long blocks of text—we recommended splitting them into multiple, shorter records. One record per paragraph is a good rule of thumb. This is the approach used by our DocSearch project, which crawls web pages containing technical documentation and can generate many records per page. DocSearch is now powering the search of 150+ open source projects, so there are lots of places you can try it out.\nWe’re constantly seeking more ways to customize relevance for advanced use cases. Some of the parameters presented in this post have been added to the API recently and several new parameters are… currently being beta tested.\nA few weeks ago we were given a challenge: to index millions of articles of a popular news website and provide relevant results about the artist Prince’s death, even for queries with typos like \"Prinse\". The query “Prinse” matches article with \"Prince\" in the title (typo=1, words=1, proximity=0, attribute=0, exact=0) but the results were being displayed below other articles about \"Sharon Prinsen\" (typo=0, words=1, proximity=0, attribute=1000, exact=0) because it matches a prefix without typo. To solve this issue, we introduced a new option to consider all prefix matches as a typo: now all articles with “Prince” in the title will be served first (typo=1, words=1, proximity=0, attribute=0, exact=0) and articles mentioning \"Sharon Prinsen\" will come after (typo=1, words=1, proximity=0, attribut=1000, exact=0). With this improvement, we were able to address the relevance problem of prefixes while maintaining a high quality instant search experience.\nRelevance is our passion and we love to be challenged. Feel free to contact us if you have a complex use case with relevance issues, we’d be happy to help!\nWe recommend to read the other posts of this series:\n\nPart 1 – indexing vs. search\nPart 2 – the indexing challenge of instant search\nPart 3 – query processing\nPart 5 – Highlighting, a Cornerstone of Search UX\nPart 6 – Handling Synonyms the Right Way\nPart 7 – Better relevance via dedup at query time\nPart 8 – Handling Advanced Search Use Cases"
  },
  {
    "id": "5189-0",
    "title": "Hello, Hi, Hey from our brand new Synonyms API!",
    "author": "Julien Lemoine",
    "text": "At Algolia we’re big on choice. We appreciate that different people search for the same things using different queries and keywords. Therefore incorporating synonyms has played a crucial part in making our search “intelligent”.\nOver the last two years, we’ve refined and reworked our synonyms feature to make it one of the most comprehensive in the market. It now supports almost all types of expressions: multi-word expressions, abbreviations, expressions containing symbols and more. It even supports advanced features such as placeholders. Today, Algolia is excited to announce the release of a dedicated API for you to manage synonyms!\nWhy a new API?\nWe’ve made significant improvements to our synonyms feature since its inception, but we also know that the configuration methods for it need work. Up until today, you had to configure synonyms via an index setting. This approach worked perfectly well if you needed to add just a few synonyms but it became harder to manage with regard to speed if a lot of synonyms needed to be on the list. There were three main drawbacks:\n\nAdding more synonyms had become a challenge (you could not use the dashboard to configure a large set of synonyms but had to write a few lines of code to push all of them as a new index setting)\n\n\nLong lists of (thousands of) synonyms impacted the performance of the getSettings API call and the performance of search to a certain extent (settings are read when opening an index)\n\n\nConfiguring the same synonyms on replica indices was a time-consuming process and left something to be desired for\n\nWhat will I get with this API?\nWe’re primarily developers and fully comprehend the importance of backward compatibility. You can still use your previous methods of configuration via an index setting and decide when you want to use this new API.  In other words: there are no breaking changes! Your production code will continue to work as-is with support from the old API, which is still very much in… existence and receives TLC on a regular basis. \nWe’ve merely introduced a small change in our new API clients: the getSettings method does not return synonyms and you need to use our new specialized search Synonyms method without any query, in order to retrieve all of them.\nThis new API is already supported by all of our API clients and you can start using it today. The documentation is available on this page. The major improvements you can benefit from are:\n\nEase of use: one API endpoint for 99% of the use cases; you can just upload your synonyms in one big batch via the batch API call, using the parameter, replaceExistingSynonyms boolean, set to true.\n\n\nScalability: this new endpoint supports millions of synonyms without impacting dashboard and search performance\n\n\nEnhanced Dashboard: we built a new interface in the dashboard to search and edit synonym sets\n\n\nEase of Reproduction: you can use the forwardToReplicas boolean parameter to forward the synonyms to all replica indices (no more painful replication of synonyms!)\n\nAvailable now!\nThis new feature has already been deployed and is available to all of our users. We hope you love this new API and have as much fun with it as we did while making it.  As always, we eagerly await your feedback, which we value deeply and and use to help improve the product!\nHappy coding!\n "
  },
  {
    "id": "5169-0",
    "title": "Introducing Algolia Places",
    "author": "Vincent Voyer",
    "text": "Today we’re excited to release Algolia Places, a fast and easy way to turn any HTML input into an address autocomplete.\nSee it live • Read the docs\nOne of our core missions at Algolia is to improve search-related user experiences. Address forms were high on our list because they're difficult to build and often represent a critical step in user flows like sign up and check out. Our goal with Algolia Places is to provide something that’s both easy for developers to integrate and enjoyable for visitors to use. It’s also highly customizable and can be enriched with additional data sources.\n\nAlgolia Places is packaged as a simple, lightweight JavaScript library and is available on npm and jsDelivr. You can find comprehensive documentation and live examples on the Algolia Places website.\nBeautiful by default\nAt Algolia we try to build tools with sensible design defaults. The Algolia Places JavaScript library displays a modern and clean dropdown menu from the moment you install it. A few lines of CSS are all you need to customize it to the exact look and feel of your site. All of the typo-tolerance & highlighting capabilities of the Algolia search API are already built in.\n\nEasy to use & easy to extend\nHere’s a snippet of code you can use to add Algolia Places to any  element on your site. No prior setup or API key required.\nView the code on Gist.\nIn some cases, you’ll want to customize the type of data that is returned and how it is presented. Algolia Places exposes additional configuration options that allow you to:\n\nRestrict search to specific countries.\nRestrict search to specific types of places (e.g. country, city, address).\nSearch for and retrieve localized place names.\n\nCombine places with search results\nAlgolia Places is compatible with the autocomplete.js library. You can easily build interfaces that combine address search results with other results from autocomplete.js data sources, including Algolia indices. Here’s an example. The Cities… section contains results from Places and the Vacation Rentals section contains results from a conventional Algolia index.\nAlgolia Places is also compatible with instantsearch.js, a library for building full-page search-as-you-type experiences. See the advanced examples to learn more.\nTry it now\nAlgolia Places is free up to 1,000 requests per day. This should be enough to get familiar with the service and deploy smaller applications to production.\nContact us at places@algolia.com if you’d like to use Algolia Places on a larger scale. We’re happy to discuss increased quotas or chat with you about custom plans.\nLike many of our projects, Algolia Places is open source. The Github repository has everything you need to open issues, have conversations and submit pull requests. Contributions from beginners and experts are all welcome.\nWhat’s next?\nWe’re continuing to enrich the underlying data and add more features. We’re working on a detailed blog post about how we built Algolia Places with OpenStreetMap and GeoNames.\nIn the meantime we’re excited to get your feedback and hear about your experiences!\nTry it now"
  },
  {
    "id": "5150-0",
    "title": "How Algolia Increased TradePub.com’s Search Conversions by 70%",
    "author": "Pragati Basu",
    "text": "Content is the hottest thing on the internet today. It’s a medium which is simple yet flexible in terms of how it can be used – for marketing, sales, information discovery, and even, at times, to look cool at parties! It’s also a medium which is heavily dependent on search functionality. In fact, that’s what brought TradePub.com to Algolia. \nDavid Fortino, SVP, Audience & Product at NetLine Corporation, recently reached out to share TradePub.com's Algolia story with us. Here’s what he had to say.\n\nWhat does TradePub.com do?\nLiving in an on-demand world is an amazing thing. Satisfying that demand though is a different ball-game altogether. TradePub.com is one such player that satisfies the demand in the “content” sphere by curating resources on behalf of some of the world's largest and most influential companies. The TradePub.com research library is one of the top destinations for professionals to access free research, white papers, reports, case studies, magazines, and eBooks.\nTradePub.com is owned by NetLine Corporation, a top B2B content syndication lead generation platform, reaching 125 million business professionals across 300 industry sectors.\nHow We Do it\nAs a top research destination for professionals, TradePub.com is built on sophisticated recommendation algorithms that dynamically match content with users throughout their onsite experience as well as beyond the on-site experience via 1-to-1 personalized email newsletters. However, one feature that had not received much TLC or investment in terms of time, was Search. At NetLine, our Product Team was focused on content discovery and recommendation modules on TradePub.com, and didn’t have much bandwidth to optimize the onsite search feature...but then again, how important was search if TradePub.com’s content recommendation technology perfectly did what it was supposed to?\nWhy Search?\nWe knew that TradePub.com’s search functionality needed work. The accuracy and relevance of search… results constantly received a poor rating from internal parties; however, search only accounted for < 2% of monthly page views — therefore, upgrading the search functionality was difficult to prioritize on the project roadmap.\nWhile we had a hard time directly attributing search to a significant percent of page views and revenue, we did realize that it contributed to the onsite experience of thousands of TradePub.com users daily. Therefore, a correlation could be made that thousands of users were receiving a poor user experience, and we were definitely not satisfied with this statistic.\nHistorically, our search feature was internally built and supported by NetLine’s Engineering Team. To improve search results and implement additional features such as auto-complete, the tool would require a complete rebuild.\nAfterall, In-House Tool Rebuild = Time + Resources\nWhat Sold Us on Algolia\nWe recognized that though search was not a top priority, it was still an important feature which added to the TradePub.com UX. With this in mind, we started looking for 3rd party solutions that could bring TradePub.com up to industry standards quickly, without pulling significant time and resources away from high priority items.\nNetLine had several key features on the “required” list when interviewing Search providers: fast results, responsive design/display, auto-complete and data analytics. As such, an exploratory process commenced that focused on the evaluation of various search technologies in the market. It was through this process that we discovered Algolia. While there was a wide assortment of customizable features reviewed throughout the evaluation process, the following key components moved Algolia to the top of the list:\n\nAuto-Complete Drop Down: Users could avoid the Search Results Page and preview immediate results.\n\n\nInstant Results: Users that reached the Search Results Page received dynamically generated result sets as they type search queries. Try… it!\n\n\nFaceting: User facing search refinement options were available, for example: by content topics.\n\n\nFully Responsive: Users could benefit from the fact that it worked perfectly across all devices.\n\n\nHuman Centric Error Handling: Automatically handled typing mistakes, concatenation, and split (\"front end\" vs \"front-end\") queries.\n\n\nAnalytics: Easy dashboard reporting complete with:\n\nTop Searches\nTop Searches to yield zero results\nTop refinements\nGeo\n\n\n\n\nOnce the t’s were crossed and the i’s dotted by my colleagues on our cross-functional Product Development team, we moved quickly to begin the implementation phase. Search was implemented between other top priority projects; but from product discovery to completion, the process was fully implemented within two months.\nThe Bottom Line:\nFollowing the implementation phase, we captured and published two months of search data. Initial results comparing the new Algolia Search Feature with 6 months of data from the original in-house search tool are as follows:\n\nAverage Monthly Search Queries increased more than 200%\nAverage Monthly Conversions driven by Search increased more than 70%\nAverage Monthly Revenue driven by Search increased more than 30%\n15% of Search conversions are driven by the Search dropdown menu versus the Search Results Page.\n\nNeedless to say, our team at NetLine is thrilled with the early results – and the entire TradePub.com audience is reaping the benefits of the new advanced search experience.\n "
  },
  {
    "id": "5131-0",
    "title": "Algolia's top 10 tips to achieve highly relevant search results",
    "author": "Julien Lemoine",
    "text": "As a hosted-search engine service, we discuss the relevance aspect of search with our customers and prospects all day long. We now have more than 1500 customers and have seen a large variety of real-life search problems. It's interesting to note that more often than not, these problems are in some way connected to the R word. Relevance.\nRelevance is a well understood concept in search engines, but is pretty complex to measure, as a high degree of subjectivity is implicit in the notion of relevance. In fact, we've seen time and again that too many people spend too much time trying to control their relevance.\nIn this post, we'll share our top 10 tips to help people achieve good relevance in their search results.\n1) Structure your data\nIt might seem like we're stating the obvious, but the first step to having good relevance is structuring your data correctly. Having a long string that contains all your information concatenated won’t put you on the path to good relevance.\nYou should have an object structured with different attributes and different strings to help the engine associate the importance of matches in your preferred order. Avoiding string concatenation to list values also ensures that the proximity measure will not be reflected inaccurately because the last word of one value is close to the first word of another value. (Proximity is an important element of the relevance that measures how close the query terms are in the matched document.)\nHere is an example using a movie title:\n\nWhich works better than:\n\n2) Handle typo-tolerance\nBefore doing advance tuning, there are a lot of small checks you can do to achieve a decent level of textual relevance. For example you should be able to find a record that contains  for the query  and  or find a record containing  with a query . You should also check that you handle abbreviations and are able to find a record containing  with the  query and vice versa.\nTypos are very frequent, especially because of the… small virtual keywords of mobile devices (and the famous fat-finger effect). If your record contains , you should be able to find it via  or . Users also love as-you-type search experiences, so you should ensure that you are able to tolerate typos on the prefix. For example, the code should be such that the  query  is considered as a prefix of  (because mikca = one typo of micka).\nAll these cases are automatically handled for you by the Algolia engine.\n3) Don’t fix issues locally, think global\nRelevance is not a one day job, and you will discover specific queries with relevance issues over time. You should try not to optimize your relevance for a specific query without having the big picture in mind and having a setup that allows you to efficiently test your modifications on the entire set of query logs ensuring you won't degrade the relevance of other queries. A good non-regression test suite is mandatory.\n4) Stop thinking about \"boost\"\nWhen we talk to different search teams, we see that they are all used to configuring  and it comes as a reflex for them. By boost they mean integers that they configure in their ranking formula like \"I configured a boost of 5 on my title attribute, 2 on my brand attribute and 1 on my description attribute\" which is kind of code for \"The attribute title is 5 times more important than description and brand is twice as important as description\".\nUnfortunately these  are not so great. Changing one from a value X to Y is the most common source of issues we see everyday! No engineer is able to predict what will happen because this boost will be combined with a lot of other factors that make the modification unpredictable (you can see it as one integer mixed with other elements in a big mathematical formula). In other terms, even if you have non-regression tests, a change of boost will just totally change your result set and it will be close to impossible to say if the new configuration is better than the previous one or not.\nThis… is actually why we designed a different way to handle search relevance with a predictable approach.\n5) Always explain the search results\nSometimes your search can be good but the user can perceive the results as bad because there is no explanation of the match.\nThe best–and most intuitive–way to explain a search result is to visually highlight the matching query terms in the result, but there are also two cases that are often not well handled:\n\n1. When you are searching in multiple attributes but only display one of them statically: in this case it would be better to display the matching attributes. This is, for example, the case if you return \"Elon Musk\" for the query \"Tesla.\" This is great, but it would be even better to add a sub-line with \"CEO: Elon Musk\" to make sure someone who doesn't know Elon Musk would understand the result.\n\n\n\n2. When you are matching via a typo: it can sometimes be non-intuitive to see where the typo is, and highlighting the matched term is very useful to help the user quickly understand the match.\n\n6) Think twice before removing stop words\nAs obvious as it seems, before trying to use a recipe you should be sure that your use case is compatible with it. This is the case with stop word removal (removing the most commonly used words in a given language like , , , , , …).\nBut those words can be very useful and removing them sometimes hurts the relevance. For example, if you try to search for the \"To Beta or not to Beta\" article on hacker news without stop words, the engine will end up with the query, Beta.\nThere are even worse queries, like if you want to search the artist \"The The.\" In this case you would just have a no-results page!\nOf course, there are cases where removing stop words are useful. If you have a query in natural language or if you are trying to search for similar content. But those cases are more of an exception than the norm. Be wary of removing stop words!\n7) There are entries that are complex to find\nThere will… always be some specific queries that can be complex to handle, this is the case for the TV show called \"V.\" This query is particularly challenging in an instant search use case:\n\nFor the sake of good UX, you should launch the search from the very first character typed (we’ve tested it many times, and have found that any heuristic that launches the query after N letters leads to a poorer UX and conversion rates)\n\n\nYou should have a preference for the exact match over the prefix because there are probably a lot of words that start by  in your data set.\n\nAnother type of corner cases is the usage of symbols, this is the case if you are looking for the band \"!!!.\" We encounter such problems with symbols in almost every use case.\n8) Be careful with language specific techniques\nNatural languages have a lot of variety that can cause your records to not be returned. For example, if you are using a singular word in your query and your record contains the plural word. There is some language specific heuristics that help to address this problem. The most popular are:\n\nStemming: Reduction of a word to the simplest form called a stem. Most of the time by removing the suffix of the word, for example transforming  in . The most popular open source stemmer is Snowball and is based on a set of rules per language.\n\n\nPhonetization: Compute a replacement of the word that represents its pronunciation. Most phonetic algorithms are based or derived of the Soundex algorithm and only work with English language.\n\n\nLemmatization: Reduction of all different inflected forms of a word. This is similar to the stemming approach except it is based on a dictionary developed by linguists, and it usually contains mainly nouns and verbs.\n\nThe major drawback of these approaches is that they only address one language. We see in practice very few cases when there are only words from one language and those techniques can produce noise on proper names such as last name or brand. You can, for example, think… about a search of people on a social network, where those approaches can introduce bad results.\n9) Use business data in your relevance\nThe first eight tips target the textual relevance, but you should also include business data in order to have good relevance. It can be just a basic metric like the number of page views or something more advanced like the number of times a product was put in a cart.\nIt can even be an advanced metric which relates to the query like \"the number of times a product was bought when searched with a particular query\".\nFrom our experience, the addition of business data makes a big difference if the textual relevance is good. That said, the business relevance should not bypass the textual relevance or you risk loosing all the benefits of the hard work done on relevance! Textual relevance should (almost always) go first and in case the textual relevance doesn’t help to decide whether one hit or the other should go first, then the engine should use the associated business data.\n10) Personalize the ranking for each user\nPersonalization of search is the final touch to get the perfect relevance and is the part that most people don’t really see. Let's take a simple example: if you search for \"milk\" on your favorite grocery store that applied all the previous tips, you will find the most popular milk bottle. But if you are a regular user of this store and have already bought a particular bottle of milk several times in the past, you’re likely to expect this one first. This is the ultimate way to make the user love the search result and avoid the perception of a bad relevance. In other words, it's the icing on top of the cake!\nNot an exhaustive list\nWe hope this list of advice will be useful to help you get a better search functionality on your website or app. This list is unfortunately not exhaustive as relevance is a pretty complex domain and there are a lot of specific problems that we do not cover in this list.\nOur team is dedicated to… help you have a better relevance, fill free to contact us at contact(at)algolia.com to share your problems and we will be happy to analyse them with you."
  },
  {
    "id": "5111-0",
    "title": "Our Post Mortem of the DNS DDoS which took place on Monday May 16th",
    "author": "Rémy-Christophe Schermesser",
    "text": "Milliseconds and transparency matter at Algolia- both inside and outside our company. So we thought we’d share the details and our post mortem of the DNS DDoS incident which took place on Monday, May 16.\nIt all started when we received an alert from our monitoring system - the globally geo-routed endpoint to our API was not available. Despite our fallback plan where we’d designed our API clients to target another endpoint if the geo-routed one is unavailable, the problem still occured. We immediately started investigating the issue because it could slow down the speed of search queries for our end-users which we absolutely could not have.\nIdentifying the Cause\nEarlier that day, we had performed an hour-long maintenance procedure on our monitoring system and verified that it was working correctly on our performance dashboards. When the alert about our main endpoint being unavailable came, we saw a similar drop in the number of queries reaching our API and we first suspected a monitoring error. Some local tests showed that the endpoint was responding but was incredibly slow. Our worries were confirmed when we saw that the DNS resolution was actually consuming the majority of the query time. A few minutes later, we received an email from our primary DNS provider informing us of an incoming DDoS attack. The root cause had been identified - it was time to update our status page and prepare. \nThe first wave of the DDoS came around 14:00 UTC and lasted till 17:30 UTC. The second one came at 19:30 and lasted till 20:30. At the peak of the attack we lost 25% of the traffic, as can be seen on the following graph.\n\nLearning from our mistakes\nThis was not the first DDoS attack that either Algolia or our primary DNS provider has seen. In fact, we’d already put some measures in place for just such an eventuality. Last year, we updated our API clients with what we call the “DNS Fallback”. This allows our API to operate on two independent DNS networks and fallback from… algolia.net to algolianet.com in case there is a problem.\nOur DNS providers too have DDoS mitigation solutions in place and have a lot of capacity to handle attacks. When forced to explain this new problem, we realised something was not working correctly in our DNS retry strategy. Despite our efforts, we noticed that 25% of our requests dropped. We immediately suspected two sources: usage of outdated API clients (without the DNS fallback) or buggy handling of DNS timeout in some of them.\nEven when DDoS mitigation is triggered quickly, it takes minutes to get rid of the simplest attacks. This is long enough to affect our users’ search availability. That's why we're tuning the timeouts of all the requests in our API clients itself in order to bring the impact close to zero.\nThe Good, The Bad and The Ugly\nAlthough we had introduced the DNS fallback a year ago, we still see usage of very old versions of our API clients. During this year, we tried to eradicate the usage of our old clients by sending notices to the impacted users and introducing a warning in our dashboard. Unfortunately we did not manage to remove all instances of old client usage - there were probably a couple of components missing in our messages since we’d not discovered a good enough incentive to get people to upgrade an API client that worked just fine, as there hadn’t been any outages. On the bright side though, when most people using old clients (without fallback support) came to us, we asked them to upgrade their API clients which resolved the issue instantly.\nBut we also discovered during this attack, that we trusted our fallback implementation a bit too much. We started to test all API clients’ implementation by replicating the conditions of the DDoS attack. For these tests, we created a new testing domain algolia.biz. This domain timeouts all the requests due to non-responding name-servers.\nWe officially support 15 API clients and here is the overview of what did (or did not)… work.\nThe Good\nRuby (also Rails), Python (also Django), Go, Swift, C#, and Android API clients passed the new test with flying colours.\nThe Bad\nThe bad one turned out to be the JavaScript API client. In the recent versions we introduced a bug that seems to have disabled the DNS Fallback. That bug was triggered when the DNS browser timeout or the DNS system timeout was triggered before our internal API client timeout.\nFortunately, this didn’t occur for every browser or OS. When a browser fails at resolving a DNS server on time, there’s no timeout exception raised by the browser but rather an error on the underlying XMLHttpRequest object. Internally, we use XMLHttpRequest errors to decide to use JSONP in case XMLHttpRequests are blocked. A recent fix on that JSONP fallback introduced a bug when facing a DNS resolution error.\nWe advised our clients to use the last confirmed working version if they were experiencing issues: version 3.13.1.\nThe good news is that we’ve now reworked the fallbacks and the latest client is working perfectly today.\nThe Ugly\nJava and Scala clients using the standard Oracle JVM had unexpected results with our new algolia.biz testing domain. While the new test worked locally, it kept failing on Travis CI which we use for testing all of our libraries. After carefully tracing the application calls, we discovered 2 things:\n\nThere is no way to set a timeout on DNS resolution, but there is a workaround: http://bugs.java.com/bugdatabase/view_bug.do?bug_id=6450279. The JVM uses the timeout of the underlying OS. That explains the difference between our workstations and Travis CI.\nThe JVM has a DNS resolution cache that is enabled by default (more information on networkaddress.cache.ttl )\n\nAlthough the second one was not an issue during the DDoS, depending on the OS the first one could have been.\nThis implies that some work needs to be done for both the Java and the Scala API clients. For the Java client, we will do it in the upcoming v2. For the… Scala client, we need to upgrade the underlying HTTP client, which will take some time as we need to change the underlying architecture of the client.\nFor PHP,  the situation is even trickier. We are using the CURL library to perform all the requests. Unfortunately, the CURLOPT_TIMEOUT and CURLOPT_CONNECTTIMEOUT options do not include DNS resolution time and PHP uses the timeout of the OS. Luckily, if you have the “ares” extension installed it sets a CURL_TIMEOUT_RESOLVE that handles DNS timeout.\nIn Closing...\nWhen we implemented the DNS fallback strategy earlier last year, we were confident it was the very last required piece of code to implement the high-availability of our API. Testing such a DNS fallback strategy is complex and it turns out that not having the ability to perfectly reproduce all the conditions of the attack - be it the OS configuration or weird behavior of the underlying HTTP library you don’t understand- was more of a handicap than we thought.\nToday we have a dedicated domain name and robust tests to ensure that our fallback code is working in order to alleviate this problem in the future.\nAnd finally, if you are an Algolia customer, please ensure that you are using the latest version of our API clients in order to avoid such impact in the future."
  },
  {
    "id": "5067-0",
    "title": "Inside the Algolia Engine Part 3 — Query Processing",
    "author": "Julien Lemoine",
    "text": "Search engines and query processing are not recent in Computer Science: this field known as Information Retrieval has a pretty vast set of state of the art practices. Today most search engines on the market come with a large set of features that developers can use to create their query processing pipeline, but this task is far more difficult than it seems and most people never manage to achieve a good result. In this post, we will cover the classical approaches and how we are handling it in the Algolia engine.\nIntroduction to query processing\nTo be able to find results, a search engine has to be able to understand deeply what was asked. That’s the role of the query processing: to analyse the query, and eventually transform it to make it easier to process by the search engine.\nAnd to do so, a search engine will process the query in two big steps, both of which can be very challenging.\n\n1) Detect words in the query: this process is called tokenization and identifies what is a word by answering questions like: \n2) Search for alternatives on this set of words: the goal of this step is to be less rigid by adding alternative corrections or synonyms, to avoid missing results that don’t contain exactly what was searched, but something equivalent. It decreases the need to reformulate the query to find what you want.\n\nWhy is tokenization complex?\nThe process of detecting words involves a set of rules to determine what is a word. It is applied both on the query and when indexing the words, to ensure both have the same format of words, which makes them easier to match.\nThis process seems simple at first – at least for alphabet-based languages – but as often, complexity creeps in when you consider the edge cases. And you soon realize that it’s way harder than it seemed, with no easy solutions.\nTo give you an idea of the complexity of tokenization, here is a non-exhaustive list of tokenization issues:\n\nSymbols: You cannot simply ignore symbols. C, C++ and C# are… different words. And there are expressions that are composed of symbols only like the “!!!” band!\nCompound words: some languages like Dutch and German can aggregate words to form one expression. For example in Dutch, the word  means  and is composed of 3 words: arbeid (labour) + ongeschiktheid (inaptitude) + verzekering (insurance). Of course you would like to have a document containing this long expression match on the  (insurance) query\nAgglutination: some languages like Japanese, Chinese and Korean don’t  have separators between words (i.e. spaces). Those languages require a statistical approach that helps to detect words in the sequence of ideograms, the search engine also needs to be reliable to an error of word recognition. As with compound words, you want to be able to search for a document with just one of the words.\n Acronyms with Punctuation: you want to find a record containing U.S.A with the USA query and vice versa\nCommon words containing periods: sometimes the period can have an importance like for domain names (.com, .net, .org), titles (mr., dr.) or abbreviations (st.)\nHyphenated words: the hyphen is not an easy case as it can be sometimes considered as a separator like in  or  but sometimes it is important like in \nApostrophes: they can be used for clitic contractions (we're ⇒ we are), as genitive markers (Carl's haircut) or as quotative markers\nCamelCase: pretty common practice of writing compound words or phrases such that each word or abbreviation begins with a capital letter. You probably want to find your  tag via the  query\nOthers: HTML entities, dates, time, measures, phone numbers, etc.\n\nTo know what to search for, the engine needs to know which words compose the query. But all of these exceptions make the process of detecting words pretty challenging. Not an easy problem to solve.\n\nWhy is searching for alternatives also complex?\nOnce the query is tokenized, you have a set of words that will be searched in your documents. We will… now enrich the query by searching for alternative words: by expanding the set of words we’ll be able to find matches that are close to the query but not exactly identical.\nThis process is only performed on the query and not when we index the record. (In a future post, we’ll explain why doing so on indexing is a bad idea.).\nIf you do not apply this process, you’ll have two problems:\n\n1) You will miss some results if there is a small difference between the words in your records and the query (for example a singular versus plural). You can also have typos or errors in your query that cause those records to be unmatched.\n2) The words in the record can be different than the query and lead to a different segmentation. For example, a record that contains 'hispeed' will produce one token whereas the query 'hi speed' will produce two tokens. This difference is pretty big as we cannot simply search for records containing the query terms anymore.\n\nThat’s when it becomes complex. As you can see in this last example, searching for alternatives can potentially change the number of words in the query, which is always a bit of a mess to manage!\nWe can distinguish four different types of alternatives that can be added to expand the query:\n\n1) An alternative word on a query word: your query contains  and your record contains .\n2) A multi-word alternative on a query word: your query contains  and your record contains  or if your query contains  and your record contains .\n3) A word expression as alternative of several query words: your query contains  and you search for a record containing  or if your query contains  and want to search for a record containing \n4) A multi-words expression as alternative of several query words: this is the most complex one to handle, this is the case if you add a synonym set containing . In this case, if your query is , it will be extended in:\n\n\n\nThe first type is properly handled in any search engine as it is just an OR between several terms… but very few engines handle natively the last three cases which often require a lot of custom code.\nAlgolia’s way of tokenizing\nOur approach of tokenization was always to keep it as simple as possible. In the end, most challenges of tokenization can be solved via an alternative added at query time! That said, there are some cases which require a special processing at indexing time to be perfect:\n\n1) Some abbreviation & apostrophes: If you have  in your record, it is very unlikely that you want to have it match for the query . Which means you just want to have the token  to avoid noise that could break the relevance. It is the same for the  expression, in this case you just want to have one token. There are of course exceptions, for example  (French expression meaning ) needs to be found as  and we do not want a token \"lhotel\". All those cases are handled automatically by our engine without having to configure anything!\n2) Agglutinative languages: For languages such as Chinese, Japanese and Korean we use a word dictionary with frequencies to split a sequence of ideograms in several tokens. We also detect the change of alphabet as a new token (this is useful, for example, in Japanese to produce a new token when we move from Hiragana to Kanji). All this processing is done automatically and we improve this segmentation all the time based on our user feedback.\n3) Symbols (&, +, _ ...): This part is the only one that is configurable since it depends on the use case and cannot be packaged in a generic way. We have a configuration setting to let you specify the symbols which are important for you. For example we used this approach on a website that helps students. They have to index mathematica formulas and make the difference between  and . In this case we have indexed all mathematical symbols (addition, subtraction, multiplication, …)\n4) Compounded words and camel case: A part of this processing has to be done at indexing if your record contains a compounded word like… and you want to be able to retrieve this record via the  query. That’s the only step that we do not have today in production but which is on our roadmap. (You can expect to see this in the future!)\n\nAt first sight, the process of tokenization can be perceived as simple. But as you have seen it is pretty complex in practice and requires quite a lot of work. We constantly work on improving it.\nWe only address a few of the tokenization challenges described at the beginning of this article. We actually solve most of them via alternatives added after tokenization. The next section describes those alternatives in details.\n\nAlgolia’s way of searching for alternatives\nWhen we launched the engine in 2013, we had only typo tolerance as an alternative. But since then, we have improved the ability of the engine to find your records even with complex mistakes. \nToday the engine expands a query via six different types of alternatives:\n1) Typo tolerance on words: after tokenization, we look for potential approximations of each query word in the index dictionary (it contains all words used in this index). In practice, we compute a Damerau-Levenshtein edit-distance between the query word and each word of the dictionary and accept the alternative if the number of typos is acceptable.\nThis Damerau-Levenshtein edit distance represents the number of letters inserted, deleted, substituted or transposed to change the query word to the dictionary word. For example the distance between  and  is 1 as there is one transposition between  and  (transposition corresponds to a common typing mistake, two letter are inverted in the word).\nIn practice, we simplify the process. By looking at the maximum distance that is acceptable for each word, we apply a recursive scan on the dictionary (represented as a radix tree). Then we prune the branches: optimizing the scan by re-evaluating on the fly what doesn’t need to be scanned. \nBy default we accept a maximum distance of 2 if the query word… contains at least 8 letters and a maximum distance of 1 if the query word contains at least 4 letters (although those values are configurable).\nLast but not least, the typo-tolerance function is able to correct a prefix, which is important in an instant search scenario. For example  will be considered as one typo of  which is the prefix of  (In this case the dictionary does not contains the word  but only the word )\nIn our engine, typos are tolerated on words and prefix of words natively, without having to configure extra processes like ngram computation. This native approach has also the big advantage of being very efficient at indexing and producing an exact computation of relevance. You can learn more about the way we store our dictionary and prefixes in part 2 of this series.\n2) Concatenation: Typo tolerance on a tokenized query is already a challenge for performance but this is unfortunately not enough: the query tokenization can be different than the one performed at indexing time. This is the case if the query contains  and the record contains  and you would like to have an alternative with the concatenation of query terms! We automatically compute a concatenation for all pairs of terms plus a general concatenation of all the query terms. Which means, for example, that the query  will generate the query:\n\nIn order to avoid polluting the results by doing an approximation on an approximation, we do not apply typo tolerance on those new alternatives; they need to be identical in the dictionary. In this query the typo tolerance is only applied on each words of the  part.\n3) Split of query tokens: Concatenation is a very good added value of the engine but it unfortunately cannot catch all the cases. The word can be concatenated in the query and split in the record, and in this case you have no other choice than splitting the query term in several words. You have this issue if the query contains the hash tag #searchengine and your record contains . The Algolia… engine will automatically try to split all your query terms in two words using the dictionary. In this case we know the frequency of the \"searchengine\" term and the frequency of all possible splits. We look at all the possible ways to split the words and select the one that has the best score for . In this case we take the best score between: , ,  … , , … , . It is obvious that the maximum will be obtained for the split in  +  and we will extend the query term  with a phrase query between  and . (Phrase query means that the terms  need to be just after  in the record to match.)\n4) Transliteration: All of the approaches we have described before work well for alphabet-based languages but can’t be applied on ideogram-based languages such as Chinese, Japanese and Korean. For these languages we use an extension file of the unicode standard called  that contains links between ideograms. For example those two lines contains a mapping between a simplified Chinese and traditional Chinese character:\nU+673A  kTraditionalVariant     U+6A5F\nU+6A5F  kSimplifiedVariant      U+673A\nThis unicode file also contains a Z-variant of the same ideogram (share the same etymology but have slightly different appearances), for example:\nU+3588 kZVariant U+439B\nU+439B kZVariant U+3588\nThe unicode format goes even further by containing ideogram with overlapping meanings, for example:\nU+349A kSemanticVariant U+7A69"
  },
  {
    "id": "5031-0",
    "title": "How we re-invented our office space in Paris",
    "author": "Pragati Basu",
    "text": "We take a few things quite seriously at Algolia—search, fried chicken, cookies and change for the better. In fact, at the rate our product and team are growing, we believe that Algolia is a new company every six months.\nBut when people in our Paris office started taking conference calls from the supplies room and kitchen, we realised that our space hadn’t exactly grown with us and it was time to find a new home.\n\nAlgolia on the move\nLike everything else at Algolia, the big move was a collective effort from the get go. Our office manager and CTO spearheaded months of site visits and negotiations, sharing everything from proposed locations to paint colors with the team. After months of searching, we found a space in a vibrant neighborhood in the heart of Paris, just next to the lovely River Seine. At roughly 8600 square feet, the location would allow us to host meetings and workshops as well as accommodate our fast growing team. In fact, once we found out about the wide choice of restaurants to try out during lunch, there was no other address for us but 88 Rue de Rivoli.\nRe-inventing our space...at the watercooler\nThe build-up to moving day was a bit chaotic.  The primary discussion at the water-cooler was anything related to the office map - lighting, seating to even our highly anticipated, brand new Playstation! We even had a mini “Hunger Games” situation at the office, with people laying claim to their desks and favorite spots beforehand.\nAt Algolia, we’re not very particular or rigid about work spaces. People pick their own spots or organize themselves depending on where they’re most comfortable. After much discussion we decided to design the new office keeping this in mind with adjustable standing desks, large “Fatboys” and lots of meeting rooms. \nIn order to avoid a situation where people had to resort to taking calls from the kitchen, we also made sure there were lots of little call rooms for privacy. Of course we’re thinking non-stop about… newer ways to make our space more comfortable even today.\nWhen nostalgia set in\nOf course there was the matter of a zillion questions which needed to be answered. Where were the lights going to go? How many new desks did we need? Who scheduled the office ski trip the same weekend as the move? There were a lot of questions that needed answering. But as the date drew nearer and we confirmed all the logistics, the nostalgia set in.\nThough we had been talking about the move for months, actually packing our things into boxes and getting ready to leave the old office behind wasn’t easy. It was here that Algolia had tripled in size. It was here that Algolia had held its first Thanksgiving. It was here that we had our very first “Garden”—where every member of the Algolia team sat together and had lunch each day. But now we had the opportunity to create new traditions and memories...starting with moving into the new office at 8am on a Saturday morning!\n\nNew traditions in the making...\nOne of the first things we did after moving to the new office—after laying claim to our desks in no particular order (yes, the “Hunger Games” had been a futile, but fun exercise), was map out all of the interesting cafes and restaurants we could go to for lunch—after all, milliseconds do matter.\n\nIt’s been just a few weeks in our new office and, to put it simply, we’re loving it. Several team-mates have personal projects ranging from painting the walls to purchasing plants (over and above the 20 odd we already have) to even buying games for our Playstation which occupies pride of place in our new ‘Garden’.\nSome things of course, are better left unchanged.\n "
  },
  {
    "id": "4856-0",
    "title": "Search your knowledge base quickly and easily with Algolia for Zendesk",
    "author": "Matthieu Dumont",
    "text": "We’re excited to announce the release of our new Zendesk integration as a part of our community package. This integration adds as-you-type search to your help center in just a few clicks and can be integrated at no cost.\nWhy do you need a help center?\nTo quote Zendesk, you need a help center to:\n1. Increase customer satisfaction by providing better service and meeting the needs of customers who prefer self-service.\n2. Reduce costs and increase efficiency by eliminating repetitive costs so agents can focus on more strategic tasks.\n3. Grow your business community and build deeper connections between your company and customers.\n6 Tips for Building a Thriving Help Center by Zendesk\n\nWhat if we told you that with this release, we can help you with both 1 and 2 on this list?\nThe Algolia Zendesk usecase\nAt Algolia, the whole technical team handles support because it gives us more insight into the frequently recurring questions and forces all of us to acknowledge our user's pain points. By sharing their pain, we end up doing everything we can to help them find answers as quickly and easily as possible. Every email not written to support because the users were able to find the answer on their own is a win, for us and for them.\nThat’s why our documentation and FAQ are both searchable. We even embedded them in our Need help? widget that's accessible from anywhere on the website.\nWhy is knowledge base search important?\nWhen users visit your help center, they rarely want to browse it. Instead, they want to find answers to their question(s), as fast as possible. And a good search is an obvious way to help them achieve this. Zendesk’s own help center has a huge search bar to invite you to search before opening a ticket. Unfortunately, with a “type-keywords-press-enter” strategy, you often have to try multiple times before finding the answers you're looking for.\nThat’s where Algolia comes into play. By providing as-you-type results, Algolia gives your users… instant feedback on the keywords they’re currently typing, allowing them to quickly and easily narrow their search and find the information they need. Reducing friction for your users not only makes them happier but also helps your agents by reducing the number of repetitive questions they have to answer.\nHow to get started\nThe Algolia Zendesk integration is easy to install and only requires a few clicks and one copy/paste.\nThe integration automatically exports your Zendesk articles into your Algolia account and keeps them up-to-date. Not a single line of code needed!\nOn the front-end side, it adds:\nAutocomplete to every search field in your help center\n\nInstant capabilities to your search page\n\nBut don’t just take our word for it, try it live!\nAnd just in case you missed it the first time—this integration is available for free!\nGoing further\nThis integration is actually creating an Algolia index and keeping it up-to-date, which means you can easily add an autocompletion menu using this data anywhere in your website/app without having to manage the indexing.\nFinally, the whole code (crawler, JavaScript library and community website) is open-source under the MIT license. Your contributions are very welcome!\nRelated links:\n\nCommunity page\nDocumentation\nGitHub repository\n\nGet started"
  },
  {
    "id": "5003-0",
    "title": "Designing a developer friendly API client in Scala",
    "author": "Rémy-Christophe Schermesser",
    "text": "The Algolia Scala API client is our 14th API client and was released earlier this year. As with every API client we release, we made it as easy to use as possible and want to give you some insight into our building process.\nIs it developer friendly?\nHave you ever struggled to use an external library or even just find some simple getting started documentation? What about a clear enough API that you don’t actually need to read the documentation?\nAs developers, we know how painful it can be to use external libraries, so we wanted to make the Algolia experience as easy as possible. I remember when I used Algolia for the first time; I used the ruby gem and loved the experience. It took me only 10 minutes to index and start searching some data, and this included a bundle install!\nEvery time we develop a new Algolia API client, our main goal is to finish with something that strongly leverages the language's strengths and, at the same time, is easy to use and feels productive from the very first minute.\nRTFM\nFirst, the documentation. It should be easy to read, but it also should be easy to write. As we support 10+ languages, we need the documentation to be clear enough that someone can just jump in, regardless of the language. For now, we use a custom project that generates Markdown files. It’s quite simple—a template, some code in each programming language, some ifs to handle special case. This way, every time we add a feature to our API, we don’t need to completely rewrite the documentation. Furthermore, we use the custom project to generate the README.md on GitHub and the documentation on the website.\nSecond, the code. As with the documentation, it should be pretty straightforward for a developer to use. We want each API client to use the idioms/syntax of its programming language. But, at the same time, we want to use the same vocabulary across each of our own API clients so a developer can move seamlessly from one to another. For example, we refer to a document… as an object, so this term should be used in every API client.\nWhat principles are unique to Scala?\nScala is a very powerful language, and because of that developers are used to powerful libraries that can do a lot in just a few lines. Furthermore, with the non-blocking trend, we are used to asynchronous libraries. These two things were the core design principles we had in mind when we developed our Scala API client.\nA few lines of english\nOne great thing about Scala is its flexibility. For example, you can omit parentheses and dots when calling method, with some rules. For example:\n\ncan be written as\n\nIt can also be extended further.\n\ncan be written as\n\nWith this simple functionality, and with some clever choice of words, you can have a fluent API that looks like English sentences. As you might have guessed, we used this a lot to provide a really straightforward API:\n\nThere are some limitation to this. First, it only works with 1 parameter methods. Second, some keywords are reserved in Scala, so you can’t use them as method names (for example: object).\nThe first limitation is not really a limitation as you can always find a way to get around it. If you need 2 or more parameter methods, you can find ways to express it differently in English. Another way is to use `object` for drop-in words. Let’s take this code:\n\nIf you add the parentheses, it becomes:\n\nAs you can see, the methods `synonyms` and `and` take an `object` as parameters. The first one `of` is just for show, and the second one is a real parameter that will be used in the API.\nThe real issue is for 0 parameter methods. For example, to list all the indices of your account:\n\nIt’s possible to write\n\nbut it’s considered unsafe.\nThe second limitation can be worked around with the backtick notation:\n\ndoes not compile, but\n\ndoes.\nFor example, indexing an object:\n\nDon’t bore me with simple stuff\nAnother thing that could help developers is taking care of repeating tasks for them. For example, we decided… to map as many things to object/classes as possible. The objects you index are `case class`, type safe and much easier to use. To have as much type safety as possible, a lot of parameters were transformed to case objects. This way you have the compiler working for you. For example:\n\n\nThat’s not much, but it can save a lot of time in the case of a typo. Our search is typo tolerant, not our settings. 😉\nSecond things second: Asynchronous\nIt seems as simple as we should just return `Future[_]`. But there are two things to consider:\nFirst, users might want to specify their own `ExecutionContext`, so we added it as an implicit parameter:\n\nBut when we wanted to implement `browse`, we stumbled upon a limitation. This method allows you to get all the objects of an index. It uses a cursor, as in a SQL database. The first call returns a list of objects and a cursor, then you use this cursor to call the API again so you can get the next objects, and you repeat that until the cursor is empty.\nOf course the first call to `browse` in Scala would return a `Future[Iterable[_]]`. But we need to call it multiple times to have all the results. So our final method would have returned a `Iterable[Future[Iterable[_]]`. Not nice.\nFurthermore, each `Future` needs the result from the previous one to be able to run (remember the cursor thingy). And it would have been nicer to have, as a result type, something like a `Stream[_]`. It’s achievable with an `Iteratee`, but it would have needed to add a dependency. As we want to keep our API clients as small as possible, we chose not to implement it in this client. For more on `Iteratee`, see here.\nWhat could we improve?\nOf course there are always improvements to be made. For example, we would love to simplify the way batches are made (see: https://github.com/algolia/algoliasearch-client-scala/issues/67).\nAs of today, if you want to have a batch, you need to call the method `batch` that takes a list of operations:\n\nSimple enough, but these… pesky ending commas could be removed, and there is a lot of repetition (index into \"indexToBrowse\" `object`). It would be better to do something like this:\n\nThis feature is planned, but we are not sure if this new syntax is needed.\nLong story short, I tried to keep in mind the ease of use while developing the Scala API client. Hopefully you have some better insight into this process now. I would love to hear your feedback and any suggestions you have for improving it. Feel free to leave a comment or open a GitHub issue to contact me directly."
  },
  {
    "id": "4814-0",
    "title": "Reducing precision in custom ranking values",
    "author": "Dustin Coates",
    "text": "Search relevance is always top of mind at Algolia. It is one of our differentiating factors, and we are always pushing ourselves to make sure our search engine is as relevant as possible. \nOne of the ways we ensure relevance is with our custom ranking feature, which is a very powerful tool if you know how to use it. One issue you may run into, however, is that custom ranking attributes that span a wide range of values may have too fine a granularity. Think for example of the number of views a photo might have. If you want to take multiple custom ranking attributes into consideration in order to get a good mix of results, you need to reduce the precision of this attribute or the other attributes may never be used.\nTo understand why, it's important to revisit Algolia's tie-breaking algorithm. Every time multiple records have the same score, we bucket them out based on the currently examined ranking factor and then create smaller and smaller buckets until we have exhausted our ranking factors.\nIf records have gotten through all of the textual relevance factors and are still tied, we take a look at custom ranking factors. Let's say that our custom ranking is set up like this:\n1. Photo views (descending), with our most popular photos having millions of views and new photos having 0\n2. Number of likes (descending), with values ranging from 0 to thousands\n3. Date added (descending)\nSince we want the most popular photos to be displayed first, we will achieve this with our first factor. But this will, in most cases, be the only factor considered because the values for this attribute are so precise. Think about this—we have six videos tied in textual relevance with the following custom ranking attributes:\n\nIn this case, the photos would now be ranked descending, based on number of views (9768, 5341, 1000, 1000, 25, 10). And since only two of them are tied in the same bucket (views equal to 1000), we only examine the second custom ranking criteria for those two photos.… And, because the number of likes for those two photos is different, we never actually look at the created date at all.\nWhy does this matter?\nIf you just want like count and created date to be in the custom ranking as tie breakers, it doesn't. But it matters a lot if you want your results to display a good mix of well-viewed photos, well-liked photos and new photos.\nBecause of the precision of the number of views attribute, you're not much better off in this case than if you had only used this one attribute for your custom ranking.\nWhat do we do about it?\nQuite simply, we need to decrease the range of values by converting continuous values into discrete ones. We can do this in a few different ways, each with their benefits and drawbacks.\nTiering\nThe first way to do this is to create tiers of these values. What this means is that you take your values and separate them into deciles, quartiles, centiles or any other equal tier that you desire. From there, you send to Algolia the tier each record belongs to. So our record would then look this (with 10 being the highest tier):\n\nThis can be done in-memory or in common databases and is best done with values that don't change often.\nAnother easy way of creating tiers is to reduce the precision of the data itself in isolation of other values. For example, a date could be sent with values by day (20160119) or by hour (2016011922).\nCalculating the logarithm\nAnother option is to take the log of the values, rounding down to the nearest integer. Whether it's a natural log, log10 or anything else doesn't matter much, which makes the calculation much simpler.\nThis also creates larger buckets at the high-end, which is valuable because there's a much larger difference between 10 views and 1000 views than there is between 1,000,010 views and 1,001,010 views.\nCustom scoring\nA final option is to create a custom score at indexing time. This isn’t really a great option because you lose a lot of what makes Algolia so powerful. We… will go into the pros and cons of this approach in an upcoming blog post.\nWhat's right for you?\nSo what’s the right approach for your situation? It really depends on how often your data changes and how many pieces of data there are. With data that changes very often or with a large set of records, a logarithm might make more sense. For records where values are clumped closely together, perhaps a tiering system would work best. In general, we go with the logarithmic system, but give both a try and see what works best for you!"
  },
  {
    "id": "4934-0",
    "title": "Inside the Algolia Engine Part 2 — The Indexing Challenge of Instant Search",
    "author": "Julien Lemoine",
    "text": "Search engine usage has evolved a lot in the last few years. Today’s users expect instant feedback without having to click on the “enter” button and wait for the page to load to get their results. They also expect the engine to tolerate typing mistakes on the fly without having to validate proposals of query re-phrasing, like the (in)famous “did you mean?”\nOf course all these changes have pretty big consequences on the engine itself. In this post, we will concentrate on the way we handle indexing at Algolia, why we do it differently in order to maximize indexing speed and search performance and how we natively address these evolutions in search.\nIntroduction to indexing\nThe goal of the indexing process is to store your data in a specific data-structure that is optimized for search. Without indexing, we would need to scan all the documents in a database to detect which ones match a query. This would take forever and be highly inefficient, ultimately resulting in a terrible user experience. This is one of the reasons we continue to focus on optimizing indexing speed as part of our overall efforts to make search more efficient.\nThe main part of the indexing process is to create a data-structure that contains a mapping of each word to the associated list of documents containing this word. One mapping of the word to the list of documents is called an inverted list, with the name coming  from the inversion of the space: instead of having documents that contains words, we have now words with the list of documents containing each word. The concept is similar to the index you have at the end of a book, except all words are in the index.\nThere are a lot of algorithms to store those inverted lists in an efficient way while keeping good performance. Most of those techniques are pretty old and well described in the very good Managing Gigabytes book.\nThe creation of those inverted lists can be decomposed in two big parts:\n\nFor each document, we extract the list of… words and build a hash-table that associates words to documents\nWhen all documents are processed, we compute an on-disk binary data-structure containing the mapping of words to documents. This data-structure is the index we will use to process queries.\n\nHow search-as-you-type is usually implemented\nAs-you-type display of search results was introduced by Google in 2010 as \"Google Instant\" and later referred to as \"Instant Search.\" Users love this experience as it creates a seamless interaction with the data they are looking for! It removes the intermediate steps such as query validation, “did-you-mean” re-phrasing and decreases the time spent waiting for the results to show up! It also means indexing and search have to be redesigned because you don't simply search for words anymore, but also for partially written words (prefixes) and it needs to be very fast!\nThere are four main different approaches engineers use today to build this type of instant search:\n1) Autocomplete (also called Suggest)\nThis approach is the most commonly used and is based on a static data-structure on the side of the search engine. You can see it as a big dictionary in the form of a Trie (special type of Tree that associates string to an output, all the descendants of a node have a common prefix of the string).\nAdvantages:\n\nVery fast response time (usually few milliseconds)\nScales well, can easily handle thousands of queries per second on one machine\n\nDrawbacks:\n\nThis is not a search engine, you have to search for a sub-string on the pre-defined list of expressions in the dictionary. In terms of user-experience, it often leads to very frustrating \"no results\" or \"bad results\" because of this limitation\nThe relevance is not the same as the search engine itself, it is just a static dictionary. It works well if you propose \"frequent queries,\" but starts to have weird effects if you use it to search on the same records as the fully-featured search engine itself. The relevance will be… different, of lesser quality and will lead to a lot of confusion.\n\n2) Index prefixes\nThis approach is simply to build an inverted list for all prefixes of a word instead of just the word itself. For example instead of just having an inverted list for the word \"engine\", you will also have one for \"e\", \"en\", \"eng\", \"engi\" and \"engin\". Those lists will usually contain a specific indicator that allows at query time to make the difference between a prefix and an exact word (both exact and prefix inverted lists will be queried with a preference for exact).\nAdvantages:\n\nFast resolution of prefixes as they are pre-computed\nAbility to keep the relevance of the search-engine for all interaction with the engine, including as-you-type experience (user is never lost between two different ways to rank results)\n\nDrawbacks:\n\nIncrease the time to publish results because the indexing process is far more expensive than without inverted list and consumes far more memory (inverted list are kept in memory)\nIncrease a lot the size of the index, which reduces a lot the likelihood of fitting in memory and so can hurt performance (even on a SSD)\n\n3) ngrams computation\nThis approach is very similar to the indexing of all prefixes but it limits the computation of prefixes by allowing a minimum and a maximum number of letters in the prefix. The main advantage is to reduce the cost compared to the indexing of all prefixes, if you query a prefix that is bigger than the maximum number of letters, the query will be automatically be transformed into a query on several ngrams (queries with fewer letters than the minimum will not trigger the prefix search).\nAdvantages and drawbacks are the same as the indexing of all prefixes; it is just a small improvement to reduce a bit the cost of indexing by adding more computation in the search.\n4) Search prefixes at search time\nThis approach keeps the indexing process pretty light by keeping only one inverted list per word but it requires to do all the… processing at query-time. For example if you perform the query 'a', it will mean looking for all words that start by 'a' in a big dictionary and then perform a query with all those words in a big OR operand. This approach is very expensive and usually people that select this approach start the prefix search after a certain number of letters in the prefix to avoid having queries that are too expensive.\nAdvantages:\n\nNo indexing overhead for prefix searches\nNo compromises on the relevance of the search-engine\n\nDrawbacks:\n\nProduces very complex and slow queries\nOnly works if the number of documents and words is very small, it does not scale\n\nEngineers working on search have no other choice than spending time to test those different approaches and select the one that seems the best for their use cases, unfortunately all of them have serious drawbacks.\nA different approach\nAt the beginning of Algolia, we worked on a different product: an offline search engine SDK for mobile app developers. And we wanted this engine to provide a fast instant search experience on old devices like an iPhone 3G.\nNone of the standard approaches described above would have worked on such a low-end hardware, so we had to design a new way to do the indexing with very little RAM and CPU. All that while maintaining an instant search experience (one of the purpose of indexing being to speed up the search operations).\nHaving already worked on multiple data-structures for search engines prior to Algolia helped me a lot in designing a different way to solve the problem. The first inspiration came from a generational string B-tree I built several years ago. It was pretty similar to what the LevelDB fast key-value store developed by Google is doing (associate an arbitrary size value to an arbitrary size string).\nFirst, I knew that for Algolia I wanted to have a generational data-structure.\nGenerational data-structure\nHaving a generational data-structure makes updating the data a lot faster. Typically,… you’ll create two (or more) “generations” of your data-structure. A big one containing the “old” data, and a small one containing the “new” data.\nThen, if you need to add/update/delete an object, you’ll update a smaller structure containing only the “new” information, which is much faster.\nMerge of Generational data-structure\nAt some point (generally when your smallest generation reaches a certain size), you’ll need to merge several generations into one for a better efficiency. This operation is called a merge.\nIn order to have an efficient merge, each generation needs to be sorted, so that you can scan them in parallel while generating the result (merge of the parallel scan is done via a standard heap-merge algorithm).\nRadix tree\nFor Algolia, I decided to represent the words that will be searchable as a radix tree (space-optimized representation of a Trie, below we look at what a radix tree is in practice).\nFirst because a radix tree is sorted, which allowed us to have an efficient merge and to use multiple generations of the tree.\nSecond because it works very well to process the typo-tolerance in a very efficient way (more details on the typo-tolerance will come in another post).\nIt is easy to build a radix tree in memory before writing it on disk, but it consumes a lot of RAM. And I wanted to use the least amount of RAM possible, because:\n\nFor the initial offline search SDK, we simply didn’t have enough RAM on an iPhone 3G.\nFor our current SaaS engine, we want the RAM to be focused on making search fast (the indices are all stored on RAM for search), which means indexing should consume the minimum amount of RAM.\n\nWhat’s great is that you can actually write a radix tree on the fly without having to build the tree in memory if you’re indexing a list of sorted words.\nFor example, let’s consider that the dataset we want to search on contains 5 objects, each having a single word. Each object will be represented by a value from 1 to… 5.\nNormally, multiple objects could have the same word, so the leaf would have a list of values. But for simplicity’s sake, we’ll take records containing different words.\nDictionary example mapping a word to an integer\nEach node of our radix tree will contain a string.\nIf the node is a leaf, it has a value associated to the corresponding object having this word.\nIf the node is not a leaf, it can have an associated value, but it can also just be an internal node representing a shared prefix.\nThe radix-tree we’re going to build will look like this:\nRadix-tree of the previous dictionary\nNow, let’s build our radix-tree. Seems simple now enough, right? The naive approach is to build the complete data-structure in memory and then to dump it on disk. We do not want to use this approach because we want to keep as much memory as possible on our servers to hosts the indices of our users and provide a fast search! This is why we designed all the indexing to use as little memory as possible while optimizing speed of indexing.\nHere’s the trick: we can build this data-structure on-disk directly from the list of words and with only a few kilobytes of memory. To do that, we’ll flush from the RAM the nodes as soon as possible, by building the tree on the fly. \nThe first step is to order alphanumerically the words of the documents we want to index (which is already done in our example).\nThen, we’re going to build our tree using a depth-first search: since a radix tree is sorted in the alphanumeric order, it’ll be easy to flush nodes to the disk as soon as we don’t need them.\n\n1. Add the word Fast\n\n2. Add the word Faster\n\n3. Add the word Test\n\n4. Flush the branch “fast” to disk\n\n5. Add the word toaster\n\n6. Flush the branch “est”\n\n\n7. Add the word toasting\n\n8. Flush the branch \"er\", then \"ing\", \"oast\" and finally \"t\"\n\nThis way to build a tree is very efficient because it only keeps in memory a very small number of nodes (in the worst case scenario you will… have a number of nodes in memory that is equal to the longest key in the tree, in our case we always have fewer than 100 nodes in memory).Now that this is done, let’s focus on the next part: prefix search!\n\nPrefix search\nRemember that we wanted an instant search experience. This means that the engine must be able to retrieve results as-you-type, at each character.\nIf you type “f”, you want to retrieve the results “fast” and “faster”, so the engine must know that both these words contain the prefix “f”.\nAnd we don’t want to calculate the association prefix-object for all the prefixes, which would take a lot of time and memory, but only when it’s really necessary.\nThe good news is that our radix tree can tell us precisely where we need to store prefixes: all the nodes in the tree which have children.\nIn our example, it is only necessary for the nodes “fast”, “t” and “toast”.\nRadix-tree with prefix inverted list computed\nYou see what I just did right there? Yes! the prefixes can be computed on the fly too! We just need to remember what was in the last few branches that we flushed.\nFor example, if I just flushed the branch “est”, I’ll remember that the prefix “t” is associated to the object 3 (test). I’ll do the same with “toaster” (object 4) and “toasting” (object 5). And when we’re done processing all the words beginning with “t”, I’ll know that this node should contain the objects 3, 4 and 5.\nA few numbers\nThis way to build prefixes has been built into our engine since 2012 and we index billions of jobs every day with this algorithm. We have measured the impact on a lot of different indices, the impact on search is pretty obvious as we always have the prefix inverted list when we do a query (and we’ll have a lot of RAM handy), but the impact on indexing time and on the size are less obvious. Here is a small summary of the impacts that are very small compared to the advantages it gives at query… time:\n\n\n\n\nMinimum\nAverage\nMaximum\n\n\nImpact of prefixes on index size\n+9%\n+21%\n+43%\n\n\nImpact of prefixes on indexing speed\n+3%\n+12%\n+17%\n\n\n\nThis approach is significantly better than any of the other approaches presented before, it is actually optimal at query time and has a very reasonable impact on indexing performance!\nThis optimization of the prefixed indexing is just one of the numerous optimizations we have built in the Algolia engine. The reason we built it is because of our focus on performance and the fact that our engine was developed specifically to allow for the strong constraints of mobiles devices. We also had to think a bit out of the box, mainly by considering the word dictionary and inverted lists as one unique data-structure instead of two independent one.\nI look forward to reading your thoughts and comments on this post and continuing to explain how our engine is implemented internally, transparency is part of our DNA! 🙂\nWe recommend to read the other posts of this series:\n\nPart 1 – indexing vs. search\nPart 3 – query processing\nPart 4 – Textual Relevance\nPart 5 – Highlighting, a Cornerstone of Search UX\nPart 6 – Handling Synonyms the Right Way\nPart 7 – Better relevance via dedup at query time\nPart 8 – Handling Advanced Search Use Cases"
  },
  {
    "id": "4924-0",
    "title": "Listen up! Algolia co-founders on the podcast circuit",
    "author": "Laura Evans",
    "text": "April has been a busy month for the co-founders of Algolia. Nicolas had the opportunity to appear on the 20 Minute VC Podcast and the official SaaStr podcast, and Julien was featured on the Software Engineering Daily podcast.\nNicolas first sat down with host Harry Stebbings for an episode of the 20 Minute VC to chat about Algolia and what it takes to build a successful company while maintaining a strong core culture. He even shares his favorite blog, perhaps a bit predictable, and his favorite book, a little bit less so. \nHe then continued the conversation with Harry on the official SaaStr podcast to discuss what he and Algolia took away from the YC process and some advice for being a successful SaaS company, among several other things.\nMeanwhile, Julien and Jeff Meyerson from Software Engineering daily discussed the unsolved problems in search and building Algolia for the long run. Head on over to SE Daily to hear Julien. While you’re there, check out some of the other content we’ve sponsored, including Bootstrapping a SaaS for Developers with Itai Lahan and Developer Analytics with Calvin French-Owen.\nMany thanks to Harry and Jeff for inviting us to participate in their podcasts. We thoroughly enjoyed the experience!\nLet us know if you have any questions after listening to the podcasts, and definitely give us a shout to share some of your favorite podcasts in the SaaS/software spaces."
  },
  {
    "id": "4893-0",
    "title": "Welcoming Kendall Collins to the Algolia starting lineup",
    "author": "Nicolas Dessaigne",
    "text": "We’re excited today to share some news that is very dear to us—Kendall Collins has joined our board of directors!\nWe met Kendall at the end of last year and have been utterly impressed by his understanding of SaaS and his passion for search. We had the chance to build an advisory relationship with him over the last few months and are thrilled to announce that he’s joining Algolia as a full member of our board.\nKendall spent the last 12 years at Salesforce as their global CMO and more recently as the CEO of Salesforce Cloud. In his last role, he was responsible for a number of core product functions, including search and Salesforce’s internal open source implementation. The breadth of his experience at the leading cloud computing company is going to be very valuable to us as we continue to scale Algolia at a fast pace. He also recently joined AppDynamics as their CMO and is actually working only two blocks away from our US headquarters!\nI've had the opportunity to chat with Kendall several times over the past few weeks, and he wanted to share a few of his reasons for joining us on our journey:\n“Google continues to raise the bar for delightful search for end users, and most companies and apps are failing to keep pace with that user experience. When I first tried Algolia, it was clear to me that their Search Platform and API approach is the future for easily building incredibly fast and relevant search inside any application.”\n“Algolia has already proven to be trusted by enterprises and loved by developers. Anyone can try it now on Product Hunt, Hacker News, or leading sites and apps such as Periscope, Arcteryx, Medium and Vevo. They have just successfully deployed at one of the top 5 software companies in the world and also at one of the top 5 retailers. And this is just the beginning. It is an honor to work with a team that has such a strong technical vision and an equally caring and compassionate culture.”\nWith now more than 1,400 paying customers,… including many enterprise accounts, Algolia is changing the way people interact with content and leading the transformation of the search market to SaaS. Having Kendall join us on that mission is a big boost in our ability to make it happen even faster.\nWelcome to the team, Kendall!"
  },
  {
    "id": "4876-0",
    "title": "Inside the Algolia Engine Part 1 — Indexing vs. Search",
    "author": "Julien Lemoine",
    "text": "In previous blog posts, we have discussed the high-level architecture of our search engine and our worldwide distributed infrastructure. Now we would like to dive a little deeper into the Algolia search engine to explain why we implemented it from scratch instead of building upon an existing open-source engine. \nWe have many different reasons for doing so and want to provide ample context for each, so we have split “Inside the Algolia engine” into several posts. As you learn more about our search engine, please let us know if there’s anything you would like us to address in future posts.\nIf you have ever worked on a search engine with significant traffic and indexing, you are undoubtedly familiar with the problem of trying to fine-tune your indexing to avoid negatively affecting search performance. Part one of this series will focus on one of the quintessential problems with search engines—the impact of indexing on search queries—and our approach to solving it.\nWhy does indexing impact search performance?\nIndexing impacts search performance because indexing and search share two critical resources—CPU and Disk. More specifically:\n1. Both indexing and search are very CPU intensive and compete for available resources. Imagine if you experience a sudden spike in search queries while simultaneously needing to run a large number of indexing operations. You run a significant risk of not having enough CPU to handle both.\n2. Both indexing and search perform a lot of disk I/Os. Search often performs a large number of read operations on the disk because the data is not always stored in memory, and indexing performs a large number of both read and write operations to the disk. There is also a battle for disk resources, even on high-end SSD drives.\nThe obvious way to solve this problem is to try to reduce or remove the conflicts of access to the shared resources.\nClassical approaches to solving this issue \nThere are a lot of different approaches to dealing with this… issue, and the majority fall into one of the following three categories:\n1. Update the data in batches during a regularly scheduled, controlled period of low traffic. This approach works well when your users are located in a specific country but doesn’t really work if your users are distributed worldwide. More importantly, this approach prevents frequent updating of the index, which means that searches will often be performed on outdated data.\n2. Use different machines for indexing and search. In this approach, a specific set of machines is used for indexing, and the generated files are copied onto the search machines. This is pretty complex to set up but has the added benefit of removing the indexing CPU load from the search machine. Unfortunately, this does not solve the problem of shared I/O, and your indexing will be bound by the network, especially when a compaction is triggered (an operation performed on a regular basis by the search engine to aggregate all incremental updates, also called optimization). The main drawback to this approach is the substantial impact on indexing speed as a sizable delay is introduced between when an update is made and when that update is actually available to be searched.\n3. Throttle the indexing speed via a priority queue. The goal of this approach is to limit the number of indexing operations in order to minimize the impact indexing has on search performance. Throttling introduces a delay in indexing speed that is difficult to measure, especially when coupled with a priority. The compaction described in the previous paragraph can also worsen the delay by causing a cumulative slow down effect on the indexing. This approach slows down indexing while also making it very difficult to avoid impacting search performance, especially during the compaction phases.\nWhile complex to implement, the second approach of using different machines for indexing and search is a good solution if indexing performance is not crucial to you. The… other two approaches only partially solve the issue as search remains impacted. Realistically, none of these approaches appropriately solves the problem of indexing affecting search performance because either indexing performance, search performance or both end up suffering.\nHow we solved the race for CPU resources\nBy splitting the indexing and search into different application processes! \nAt Algolia, indexing and search are divided into two different application processes with different scheduling priorities. Indexing has a lower CPU priority than search based on a higher nice level (Nice is a tool for modifying CPU priority on Unix-like operating systems). If there is not enough CPU to serve both indexing and search, priority is given to search queries and indexing is slowed down. You can keep your hardware architecture designed to handle both by simply slowing down indexing in the case of a big spike in search queries.\nAs is the case with using different machines for indexing and search, separating them into different application processes introduces some complexity; for example, the publication of new data for search becomes a multi-process commit.\nThis problem is pretty common and can easily be solved with the following sequence:\n\n1. When the indexing process receives new jobs, build a new incremental update of the index.\n2. Commit the incremental update to the disk.\n3. Notify all search processes of the data changes to the disk.\n4. Redirect new queries to the new data-structure. Currently processing queries remain served by the old data-structure.\n5. When no more queries are processing on the old data-structure, erase it from the disk.\n\nThis approach solves the problem of needing to share and prioritize CPU resources between indexing and search but is unfortunately something that most search engines on the market today cannot implement because indexing and search are executed in the same process.\nHow we solved the race for disk resources\nThe race for disk… resources is a bit more complex to solve. First, we configured our kernel I/O scheduler to assign different priorities to read and write operations via the custom expiration timeout settings within the Linux deadline scheduler. (Read operations expire after 100ms, write operations expire after 10s). Those settings gave us a nudge in the right direction, but this is still far from perfect because the indexing process performs a lot of read operations.\nThe best way to address the contention for finite disk resources is to make sure the search process does not perform any disk operations, which means that all the data needs to remain in memory. This may seem obvious, but it is the only way to ensure the speed of your search engine is not impacted by indexing operations. It may also seem a bit crazy in terms of costs (having to buy additional memory), but the allocated memory can actually handle the vast majority of use cases without issue. We of course have some users that want to optimize costs for huge amounts of data, but this makes up a very small percentage of our users (less than 1%) and is addressed on an individual basis.\nDefault to fast and reliable\nEverything at Algolia is designed with speed and reliability in mind—your data is stored in memory and synced on a high-end SSD and at least three different servers for high availability. Our ultimate goal is to remove all of the pains associated with building a great search feature, and solving the dependency between indexing and search was a very important step in getting there!\nWe take a lot of pride in building the best possible product for our customers and hope this post gives you some insight into the inner workings of our engine and how we got where we are today. As always, we would love your feedback. Definitely leave us a comment if you have any questions or ideas for the next blog in the series.\nWe recommend to read the other posts of this series:\n\nPart 2 – the indexing challenge of instant… search\nPart 3 – query processing\nPart 4 – Textual Relevance\nPart 5 – Highlighting, a Cornerstone of Search UX\nPart 6 – Handling Synonyms the Right Way\nPart 7 – Better relevance via dedup at query time\nPart 8 – Handling Advanced Search Use Cases"
  },
  {
    "id": "4736-0",
    "title": "How Algolia tackled the relevance problem of search engines",
    "author": "Nicolas Baissas",
    "text": "When we started Algolia, we wanted to build an offline search SDK for mobile applications. Unlike other search engines on the market, we couldn’t rely on pre-existing search engine software such as Lucene because they weren’t available on mobile.\nWe had to build everything from the ground up within the limitations imposed by smartphones such as low memory and low processing power. Ultimately, we spent a considerable amount of time reinventing the search engine from scratch.\nWhen we eventually decided to pivot to a SaaS solution, we had built a completely new ranking algorithm. And in a typical “constraints foster creativity” situation, we had made two huge improvements: Speed and Relevance.\nAlgolia is known for its blazing-fast speed. It’s usually the first thing people notice when they start using it and also what makes it so exhilarating to use. But what I consider to be the most impressive and important achievement of Algolia is not actually speed but Relevance.\nAlgolia reinvented how search engines define relevance.\nBefore diving into the specifics behind Algolia’s ranking formula, let’s briefly go over how other search engines work.\nA little history of search engines\nIn the late 90s, the Text Retrieval Conference organized a series of annual workshops financed in part by the US Department of Defense. The goal of these workshops was to determine the best ranking algorithm for searching a dataset consisting of long unstructured documents. The conference resulted in several enhancements to search engine ranking algorithms, the most defining being improvements to TF-IDF, what was then the main component of the most powerful ranking formulas.\nTF-IDF is a statistical weighting scheme based on two elements: TF or Term Frequency (the number of occurrences of a word in a document, which is calculated on a per document basis) and IDF or Inverse Document Frequency (the importance of a query word within the whole corpus of documents).\nBasically, if a query… word is repeated multiple times within a document, that document will rank higher in the results than a document that only contains the query word once. IDF comes into play to reduce the importance of words that occur a lot in the corpus such as “the”, “of”, “a”, etc. The more times a query word recurs within the whole corpus, the lower a document containing that query word will be ranked. The length of the document is also taken into account; a short document with query word matches will rank higher than a long document with the same match.\nTF-IDF (along with similar algorithms like BM25) became a central tool to score/rank documents given a user query, and it was used to search any type of data. Unfortunately, it was not actually designed for that! TD-IDF was built as an answer to TREC’s workshops and was designed specifically to work within long unstructured documents such as PDFs or enterprise documents.\nThe issue with this kind of statistical approach is that it surprisingly only makes sense for a very small number of use-cases, like enterprise document search, where:\n\nYou don’t really know what type of data you’re searching into\nThe content is drastically different from one document to another\nThe documents don’t have any popularity metrics, and you need to rank them solely based on their “textual relevance”\n\nWhat Algolia was built for\nAlgolia wasn’t built to search long unstructured documents. It was built to power the search of websites and mobile applications. And in 99% of cases, this data is very different.\n\nIt has a defined schema with meaningful attributes that you know beforehand (title, name, list of tags, description…)\nYou know the underlying content, what matters and what doesn’t (e.g. is the title more important than the description?)\nThere are often one or more popularity metrics associated with each object (number of sales/likes/views)\nAs the owner, you sometimes have a strategy that impacts the ranking (give a bonus… to featured objects)\n\nLet’s look at an example\nSay that you’re on IMDB searching for an actor, and you type “Brad”. You don’t care about the Term Frequency (how many times the document contains the word Brad), or the Inverse Document Frequency (how many times the word Brad is present in the list of all actors).\nFortunately, there are other signals that matter:\n\nIs the word Brad in the “name” attribute or in the description?\nIs it the first word or the last word in this attribute?\nIs it spelled exactly B-r-a-d? Does it contain a typo? A suffix?\nHow many followers/likes/views does Brad have?\n\nAs you can guess, these questions give us a much better chance at finding Brad Pitt than a statistical approach like TF-IDF.\nThis is the first pillar of Algolia’s revolutionary improvements—the rules taken into account in the Ranking algorithm.\nThe rules in Algolia’s ranking formula\nAlgolia doesn’t rely on any variation of TF-IDF. Instead, it uses six default rules to evaluate the textual relevance of an object for a specific query:\n\n1. Typo: Will rank higher a word that doesn’t contain a typing mistake.\n2. Words: Will rank higher an object matching more of the words typed by the user if the query contains more than one word.\n3. Proximity: Will rank higher words close to each other (George Clooney is better than George word Clooney).\n4. Attribute: Will rank higher a match in a more important attribute (Title, Description).\n5. Words position: Will rank higher words that are at the beginning of an attribute.\n6. Exact: Will rank higher words that match exactly without any suffix (Algolia natively handles prefix search).\n\nMost search engines have somewhat similar implementations of the Words and Attribute rules (some also have an implementation of Proximity). The rest of these rules are unique to Algolia. You can of course disable or customize the rules to fit your specific use-case, and you can also use non-textual criteria such as geo-location.\nThese rules… are a great improvement over what existed before, but the biggest and most important change Algolia brought to the table is something else — how these rules are used to rank the objects.\nHow is the ranking done?\nThe biggest issue with the relevance of most search engines is not TF-IDF (which can actually be disabled in most search engines using it). It’s the way the ranking criteria are used. Other search engines compute a big floating-point score that mixes everything. They have a very complex mathematical formula with a lot of coefficients/boosts that merges every rule into one score for each document. Then, they use this unique score to rank the search results.\nThe first problem with this approach is that it mixes apples and oranges. For example, if your ranking formula takes into account TF-IDF and the number of likes of the documents:\n\nWhat happens if a document has a very high number of likes? It will override all the other criteria like textual relevance, and this document will always show up in the results, even when the textual relevance score is very bad.\nSimilarly, if an object is the most relevant to the query but doesn’t have any likes, it won’t show up in the results.\nThe second issue with this approach is its complexity. Nobody is able to understand what’s really happening. Even the most skilled search experts need to spend a lot of time crafting the best mathematical formula. Via a trial-and-error process, they’ll find a formula that works well for a list of the 100 most popular queries, but the formula cannot be optimized for the whole catalogue. And as soon as the catalogue (or any metrics used in the formula) changes, they need to go back to the drawing board.\n\nThis exception-driven approach is not maintainable.\nAlgolia’s ranking formula is designed to fix this. And it does so with a different ranking approach via the Tie-Breaking algorithm.\nThe tie-breaking algorithm\nInstead of calculating one big score and then sorting the… results once, a tie-breaking algorithm calculates N scores and applies N successive sorting. Here’s how it works:\n\nAll the matching records are sorted according to the first criterion.\nIf any records are tied, those records are then sorted according the second criterion.\nIf there are still records that are tied, those are then sorted according to the third criterion.\nAnd so on, until each record in the search results has a distinct position.\n\nThe order of the criteria determines their importance. It’s like in a children’s game—if you sort by color and then by shape, you don’t get the same results as when you sort by shape then color.\nConfiguring the relevance of Algolia for a specific use-case is as simple as figuring out the set of rules that need to be applied and ordering them by importance.\nOh, and everything is customizable—from the order of the attributes to which attributes are used. So there’s virtually no ranking strategy that you can’t build with this ranking formula.\nLet’s consider an example with the following dataset:\n\nAnd this index configuration:\n\nTo simplify our example, let’s ignore some of the rules and focus only on Typo, Featured and Number of Likes. For the query “John,” we’ll get the following results:\n\n1. John Thompson (0 typos; featured; 8 likes)\n2. John Jackson (0 typos; not-featured; 17 likes)\n3. John Paul (0 typos; not-featured; 3 likes)\n4. Jon Black (1 typo; featured; 4 likes)\n5. Jon White (1 typo; not-featured; 9 likes)\n\nIn this example, we:\n\nDecided that Typo was the most important rule. If an object has a typo, it should be ranked lower, no matter what the other rules are.\nGave a bonus to records marked “featured”.\nDecided to position our main popularity metric, Number of likes, at the bottom of the formula so it doesn’t override the textual relevance criteria.\n\nA few things to note:\n\nWe didn’t need to use coefficients or boosts of any kind. Nor did we need to craft some complex mathematical formula.… But we still managed to take into account business metrics and merge them with our textual relevance.\nThis formula works for every query. It is not designed to work for the top 100 most popular searches, it is designed to work for the whole catalogue.\nWhat happens if 50% of the catalogue changes tomorrow? Nothing. We don’t need to change the formula as long as the structure of the data remains the same.\nIt is very simple to understand why one result is ranked higher than another. And if you can understand the results, fine-tuning the formula will be much easier.\n\nWhere do we go from here?\nAs of writing this, Algolia is live on thousands of websites and mobile applications, and this ranking strategy has consistently proven successful. We have spent a long time fine-tuning every detail and are pleased with the level of performance. We continue to make relevance a core focus, in particular to introduce new criteria to better enable the personalization of results per user.\nWe’re always happy to help you figure out how you could use this algorithm to achieve the ranking strategy that best fits your specific use-case. And we’re eager to know what you think, so feel free to leave us a comment."
  },
  {
    "id": "4791-0",
    "title": "Come see Algolia SuperSearch at SaaStr Annual 2016",
    "author": "Laura Evans",
    "text": "How ‘bout them Broncos!? The big game might be over, but Super Bowl mania isn’t, at least at Algolia.\nSaaStr Annual 2016 kicks off in San Francisco on Tuesday, and we’re excited to join more than 5,000 other SaaS professionals for three action-packed days of high-quality networking, learning from those who’ve done it and, of course, great food, generously-poured drinks and plenty of fun. We’ll be demoing our new SuperSearch project—every Super Bowl ad ever aired, searchable with Algolia— for the first time at our booth all day Tuesday in the Mauna Kea Expo Hall at booth 61.\nOur CEO Nicolas Dessaigne will also be part of the panel “Managing and Scaling Globally (The Good, The Bad, and The Ugly)” in the Tactical Theater at 3:30pm on Wednesday. \nOther Speakers include: Marketo CEO Phil Fernandez; Netsuite CEO Zach Nelson; New Relic CEO Lew Cirne; Mattermark CEO Danielle Morrill; Salesloft CEO Kyle Porter; Slack’s April Underwood; Hubspot Founder Dharmesh Shah; Bloomberg’s Sarah Frier; Co-Founder of TalkDesk, Christina Fonseca, Tomasz Tunguz of Redpoint Ventures, and over 100 more!\nCheck out the rest of the conference agenda here.\nIn addition to the speakers, panels and expo, there are nightly social events, including our free SaaS to Dev Happy Hour: A conversation on selling to developers featuring a panel with AWS, Algolia and Keen.io. Registration for our event is free, but space is running out, so don’t forget to sign up.\nIf this is your first SaaStr Annual, definitely take a minute to read the guide they’ve prepared for first timers, which includes some great tips on how to make the most of your conference experience.\nHope to see you there, and check back in next week for our conference recap."
  },
  {
    "id": "4771-0",
    "title": "Find every commercial ever aired during the Big Game with Algolia SuperSearch",
    "author": "Guillaume Dumortier",
    "text": "Algolia HQ is in San Francisco, so it’s been pretty impossible to escape the Super Bowl mania sweeping the city. (If only they also actually swept the city.)\nOnce we got over our slight annoyance with all the street closures because of the Super Bowl Village they built two blocks from our office (40 miles away from the actual Super Bowl at Levi’s Stadium), we realized that we’re actually pretty excited for this weekend’s game for two reasons:\n1. Neither of the teams playing is the Patriots!\n2. THE COMMERCIALS!!!\nIf we’re being completely honest here, Super Bowl party snacks came in at a close third.\nWhen we started talking about commercials from years past and trying to share them with one another, we realized it wasn’t easy to find Super Bowl commercials. This is a huge bummer if you’re anything like me and want to watch every commercial those majestic Clydesdales have ever been in.\nSo we started thinking...and hacking. Why can’t we just make all the Super Bowl ads that ever aired searchable with Algolia? Everyone was onboard almost immediately once we explained that in order to search the data set, we would need to tag the data set...yes, we will pay you to watch YouTube.\nWe won’t admit how much time we’ve now wasted showing each other “the one with the flying baby” or “the one with Will Ferrell in that stupid outfit,” but we can tell you that actually building and implementing the search for this demo took us less than a day. This is how we did it:\n1. Scouted the interwebs to find the best archive of existing Super Bowl ads (found on YouTube).\n2. Ensured data searchability and homogeneity—all ads must include Title, Brand, Year and Super Bowl edition.\n3. Pulled custom ranking data for the ads such as number of likes, number of views, etc.\n4. Enriched the existing data set with video content tagging.\n5. Designed and built the UI with instantsearch.js widgets.\nAnd voilà—SuperSearch.\n\nWe’re going to add this year’s commercials… in as close to real time as possible and have already included a few that were pre-released. (Let’s not talk about puppymonkeybaby.)\nWe really hope you enjoy SuperSearch as much as we’ve enjoyed building it. Have a look for some of your favorite commercials, and don’t forget to leave a comment and tell us which ones you like (and don’t like).\nGo to SuperSearch"
  },
  {
    "id": "4731-0",
    "title": "Building a better iStyles shopping experience with Algolia search",
    "author": "Guillaume Dumortier",
    "text": "iStyles, a fashion accessories provider for consumer electronics, boasts more than 200K products and 1600 designs, as well as support for over 800 devices. With the already large and constantly growing number of products offered, iStyles needs their customers to be able to find what they want quickly and easily. That’s where we come in. Here’s the scoop on how iStyles uses technology, including Algolia, to provide the best shopping experience for their customers.\nThe iStyles story\niStyles was originally formed in 1997 as a web development firm that offered web services and built applications and e-commerce infrastructure for companies worldwide. As such, iStyles has always been a technology company at its core and continually evaluates and leverages the best tech out there to provide an optimal shopping experience.\nIn 2004, after acquiring some product distribution contacts, iStyles decided to make their e-commerce store the main focus of the company and has since grown the business into the fashion accessories provider it is today.\nThe e-commerce challenge\nThe market has become very competitive over the years, and it is increasingly difficult to attract new customers in a saturated marketplace. iStyles needs to stand out from the crowd and prove to customers why we are a much better place to shop than Amazon or other online or brick-and-mortar retailers. We have built a loyal base of customers over the years but still have to continually work hard to attract new ones through marketing and other innovations.\nWhen we first launched in 2004, we based the store on an existing e-commerce platform. The problem with using a popular e-commerce platform is that your competitors can (and will) deploy stores using the same platform. There’s no innovation—no differentiator. The other problem we faced was the rigidity of such e-commerce platforms. One size may fit many, but it will not fit all. If you need to present or link products more meaningfully or have better… control over your search results, you have no choice but to customize the solution.\nThe technical difficulties of customization\nA good e-commerce store allows customers to explore the product range available and get to what they need or want quickly and easily. We know the significant role a search engine plays in this process, but the custom search algorithms we developed to overcome the limitations of the default e-commerce platform search engine were inadequate. It was very difficult to build both speed and flexibility.\nPerformance is an important part of the shopping experience, and we are obsessed with making things faster, but it was a large technical hurdle as our product range and traffic grew. Searches on our previous search engine took as long as 2 to 3 seconds for basic searches. It honestly sometimes feels like we’re trying to overcome the limits of physics. And let’s be real, a good search engine is so much more complicated than simply presenting precisely matching results. We didn’t have the resources or time to build another Google from scratch.\nHow can we have great search with our limited resources?\nWe needed a search engine that would return relevant results quickly and present them clearly. We also needed better control over result ranking, a way to allow for typos and the ability to match on synonyms. So we started looking. We were looking for a solution that would allow us to deploy \"really awesome search functionality\" without having to re-invent the wheel or have more back-end infrastructure to worry about.\nExactly what we were looking for—Algolia\nWe signed on for and evaluated trial accounts on several search services, but Algolia quickly emerged as the obvious choice during our 14-day trial period. We were impressed most by the comprehensive documentation and feature set.\nWe designed and built our new Algolia powered search functionality from the ground up and were thrilled it took us less than a week to do so thanks to the… excellent Algolia documentation and easy to use Search API clients.\nOur front-end is now powered by Algolia's JavaScript Search API and custom scripts that tie everything together. We keep the search indexes updated via their PHP API at the back-end.\nIt works, it really works!\nSearch performance is critical for us, and we're glad to say that Algolia exceeds our expectations. There is something to be said when search performance improves from 3 seconds to 0.03 seconds. Search results are now virtually instant—they are so fast that we can update results as the user types.\nWith this virtually instant response, our user search experience was completely transformed. The search is a pleasure to use, and the engine returns more relevant results as we are able to index more comprehensive data with no loss in performance and have more control over how results are presented.\nAlgolia Isn't just a simple search bar either. Users can search for anything they want in the search bar and narrow down the selection easily via the filter options (facets) in the left column. These facets also update virtually instantly based on what the user selects and/or types.\nUsers can now get to what they want more quickly and in fewer steps. We have no doubt that the Algolia search implementation was part of the reason why our conversion rates this year are consistently higher than last year's.\nWe're planning on building new sections on iStyles, and it's likely that these new areas will be powered by Algolia at the backend.\nWe think you should give it a try\nAs developers, we are always trying to build better products. One way to do so is leveraging the great technology available to us. When we started the project to revamp the iStyles search functionality, our main aim was to allow customers to search more effectively. Algolia enabled us to not only build really good search functionality, but also essentially build a whole new way to experience iStyles.\nThere are options out there that may… better suit some projects depending on the resources you want to throw into it, but if you're looking for a fast, flexible and reliable backend that you can use to build your dream search engine, give Algolia a try."
  },
  {
    "id": "4684-0",
    "title": "How we updated our JS libraries for React Native",
    "author": "Alexandre Stanislawski",
    "text": "Our mission at Algolia is to make sure that every website or app has the best search experience possible. That's why we created several clients for iOS (in Swift and objective-C), Android, Windows/C# and JavaScript.\nBut we're always looking for new ways to help developers and are very interested in the possibilities introduced by React Native, Facebook’s latest initiative to change the way we build apps.\nBased on the awesome React library (that we already use), React Native lets you develop applications for Android and iOS using Javascript, hopefully making the development process easier and more consistent. What’s even cooler is that it can leverage what's already available on NPM such as algoliasearch-client-js or algoliasearch-helper.\nBut React Native is a new execution environment different from Node or the web. Some expected features are implemented, some are not. When we tried to use our libraries to build an app using React Native, we hit a few bumps. We want to share with you what those were and how we got over them.\nTL;DR : we made our JS libraries compatible with React Native, and it’s available right now! And we made a sample to help you get started.\nAlgolia and React Native in action.\nNot a Node polyfill\nThe first thing we noticed when working with React-Native is that even though it looks similar to Node, it’s not quite the same. React Native lets you use  but doesn't polyfill the Node standard libraries like Browserify or Webpack do. The reason it doesn’t do this is that it’s not using the same tool to do the packaging and they made the choice not to implement this compatibility layer.\nIn order to make our module work in this new environment, we need to provide the core Node modules used by our libraries. Hopefully, all those Node standard libraries are available on NPM and we just need to add them to our package.json file as we are installing them ( for the win).\nBut won't we now run into issues with the module running on Node because… we’ve modified the package.json? Nope! Because of the built-in rules of  in Node, we know that it will always use the built-in version if it is available.\nThe web but not exactly\nMost of the work in our JS-client library is based on HTTP calls. And hopefully, React Native implements a version of XHR for us. Unfortunately, its XHR implementation comes with some particularities. Some of these differences break stuff, whereas some simplify the work. Let’s see in more detail what differs from the web:\n\nReact-Native is for creating native apps, and they are not restricted to a single domain like web pages, so implementing CORS capabilities doesn't make sense.\nReact-Native doesn’t come in many versions, and the set of features is clearly defined, so there’s no need for web fallbacks/compat hacks such as JSONP.\nOur library also relies on XHR timeout, but it is not implemented there, so we rely on the good old setTimeout to implement this.\n\nThose changes were only needed in the JS Client that handles the request to our API. The JS-Helper only relies on the client and some libraries, so it does not need further modifications.\nNew JS territories, here we come!\nWe applied what we learned about React Native to make our JS libraries, the JS client and the JS Helper compatible with it, so now you have a new way to build your mobile applications on Android, iOS and more. Let us know if you have any questions.\nGet started with Algolia using React Native using our HN search sample."
  },
  {
    "id": "4611-0",
    "title": "Robots vs. humans: Who will emerge supreme in the battle of the image tags?",
    "author": "Alexandre Stanislawski",
    "text": "One of the most exciting challenges in tech today is undoubtedly trying to beat the human brain at its own game. Whether it’s with the super “helpful” automatic checkout at the grocery store or IBM’s supercomputer Watson, technology has come a long way in keeping pace with and occasionally surpassing human capabilities.\nInterpreting images, however, is still considered a human’s game as images are processed faster by the brain than a machine, and human minds consistently outperform technology when it comes to image analysis.\nAs a full-text search engine, Algolia searches a lot of images but cannot actually search the images directly. We rely instead on metadata associated with the image such as title, description, location and other tags to help us make sense of the image. We often run into situations where an image has no associated words or tags but customers still want to search within those images. So what happens then?\nLuckily, we have seen a lot of text-based algorithm tools emerge in the past decade. One such tool, Imagga, boasts an image recognition software that can—amongst other things—automatically generate these missing image tags. And it just so happens to be an API too! Imagga is essentially the missing link that enables searching raw images within Algolia. Algolia provides a powerful way to explore large amounts of data, and Imagga brings to the table the ability to create textual data from a set of images.\nWe’ve devised a game combining both tools to compare how humans and machines tackle image tagging and how this affects your ultimate search experience. Find out who comes out on top in Human vs. Robot: Battle of the image tags.\nPlay Clash of Tags\nIllustration by Martin David"
  },
  {
    "id": "4625-0",
    "title": "Bringing Algolia’s faceting experience to mobile–the evolution of AppApp",
    "author": "Guillaume Dumortier",
    "text": "We built Algolia to make search easier for everyone, and a lot of our customers implement search within already existing websites or mobile applications.\nBut then we also have customers like AppApp.\nAppApp, an independent search engine for the iOS App Store, was actually conceived after its founders realized the potential of Algolia while building solutions for existing clients. Based on faceting, grouping search results into categories, AppApp lets you search within the official App Store in ways the native search does not allow. AppApp immediately saw an opportunity to bring this faceting functionality to their mobile interface and reached out to us to share their process. Dan Singerman, its founder, explains.\n\n \nFaceting with Algolia\nWith a typical desktop faceted search design, the instant nature of Algolia's faceting allows you to quickly see the breadth of data, understand the results of your actions and know whether you have applied overly specific criteria. Toggling a filter takes fractions of a second, so there is very little perceived cost to experimenting with filter combinations to produce the desired results.\nAlgolia obviously offers a great faceting experience, but the wheels were in motion—how do you fit it into a mobile design?\nMaking it work for mobile\nFrom the beginning, we wanted the search results to visibly change as the faceting was taking place. This is, after all, the essence of instant search. Most mobile faceting interfaces take you to a separate page when refinements are applied, and you lose the instant feedback you get with a desktop design.\nA good example of this is Amazon.com. While they have a beautiful interface, a user would be unlikely to experiment with a large number of different faceting criteria. Who wants to keep navigating back and forth between the search categories and the results?\n\n\nBuilding AppApp.io for mobile\nOur first mobile implementation was a naive attempt to replicate the desktop approach with facets… vertically aligned alongside results.\n\nIt was immediately obvious that this was not going to be successful. The touch targets for the facets were far too small, and the design would not extend to support the large number of facet categories and values we were planning to have. We realized that showing the facet categories and their values simultaneously with the results was just not going to be possible on a mobile screen.\nOur next iteration moved the facet values into a modal while leaving the facet categories vertically aligned with the results.\nThis iteration allowed facet selection to instantly update the search results, even if it was occurring in the background. Given that we didn’t have the space of a desktop, we felt this was a good compromise. The more pressing issue with this approach was that on a mobile device, the result set was very narrow and did not make good use of space.\nWith apps, text descriptions alone do not give a good sense of whether an app is what you’re searching for. We like the official App Store’s approach of showing screen shots in line with results and wanted to replicate that in our design. Because of this, we felt that constraining the search results to only two thirds the width of a mobile screen was not optimal. We wanted to make use of the full width available whilst also having an effective faceting interface.\nSo our next iteration did just that.\n\nWe floated the faceting component over the results and moved it below the default title position for the first result so the title wasn’t hidden. Now we had full width results alongside facets. In retrospect, this was a bad idea. Nobody wants components to sit permanently on top of each other. What were we thinking? But trying this on a real device very quickly told us that.\nSo, our next evolution allowed the floating facet component to be hidden.\n\nThis was definitely an improvement in that the facet controls could be hidden so as not to obscure the result set, but it felt… unnatural. You had to actively decide to hide the facets when scrolling through the results if you wanted to be unencumbered by the floating facet component. Weren’t facets the whole point of this project? This wasn’t going to work, but it did take us another step toward our current design.\nWe figured it out—our current design\n\nWe wanted facets (filters) that would appear consistently when activated by the user but naturally disappear as the user focused on the results. We moved the facet control below the search box and added a tab so it can be opened in a drawer-like fashion.\nWe chose to label the tab instead of assigning it an icon because we felt an icon alone would not be a strong or clear enough call to action. We decided on the label ‘filters’ because we thought this would be much more meaningful to users than ‘facets’ or ‘refinements’. We also made the tab bright orange so it would stand out to users and could not be easily overlooked.\nThe facet drawer also naturally closes as you start to scroll, which gives the results more screen real estate.\nDoes it work?\nThe initial evidence is positive for this approach. Since launching a month ago, our analytics tell us that 61% of users on mobile interact with the facet controls via this interface. While we’d definitely like to increase that number, this is a good start. The analytics also show that users who interact with this control do indeed try many different combinations of facets. Ultimately, we’re pleased with the results, even though it took several iterations to get here, and continue to be impressed by the faceting capabilities of Algolia that inspired this project.\nIllustration by Martin David"
  },
  {
    "id": "4575-0",
    "title": "How Algolia keeps all your data safe but searchable on Solid",
    "author": "Guillaume Dumortier",
    "text": "In today’s world of easily accessible data, privacy is key. But protecting data is often a manual and expensive process requiring extensive planning. Solid, an app designed to help you run easy and effective meetings, faces a unique struggle when it comes to privacy. They have access to large amounts of personal data and need it to be searchable yet secure. \nAligned with their mission to make meetings easy and effective, they needed a way to keep privacy easy and effective while not minimizing any search functionality. So, they called Algolia. \nThibaut Davoult, Growth Manager at Solid, recently reached out to share the Solid/Algolia experience with us. Here’s what he had to say.\n\nWhat does Solid do?\nSolid is a meetings management app used by managers, employees and freelancers around the world. Our platform has hosted hundreds of thousands of meetings and counting, and clients such as LinkedIn, Dropbox, Deezer and Airbnb trust us with their private data. Solid, in return, places the utmost importance on privacy.\nProtecting your data\nIn fact, privacy is one of the stepping stones for Solid’s success. Meeting invitations can contain very sensitive data. Even meeting names alone can be revealing, so we work hard to prevent any of this info from leaking. And this has been the case from the very beginning. We largely manage to keep privacy in check thanks to deep provider integration. We currently integrate with Office 365 and Google through their OAuth system and rely on ACL to keep users out of others’ meetings.\nBut we needed search...\nThe need for a search feature quickly became evident while trying to manage all these meetings. We had a large amount of data and no clear way to organize and find information. Thus, the problem to solve was quite straightforward:\nHow do we implement a search feature that:\n1) Keeps our users’ privacy in check\n2) Doesn’t hurt our app’s performance\n3) Can be implemented as quickly as possible\nWe had a few options:\n1)… Develop a less-satisfying search via MySQL+Redis\n2) Implement open source solutions like ElasticSearch or Solr\n3) Go with a 3rd party API \nThe latter seemed like the way to go. Here’s why it seemed like our best option and how we chose Algolia.\nWhy Algolia?\nAlgolia is the leading hosted search API and the only one that delivers instant and relevant results from the first keystroke. Once we realized this, it didn’t take long for us to reach out to learn more. What followed, as you’ll see, cemented our decision. We knew it was a no brainer to implement them.\nAn easy but careless way to keep search performance up would have been to go without a backend proxy so the JavaScript code could directly send requests to the search engine from the end-user browser or device. But that’s not easy. Since JS is executed client side, it would expose all the code and access keys to users, allowing them to search through other meetings. Not very secure. We needed to find a way to secure this information without going through a backend—and secured—proxy.\nWe recognized that Algolia’s Secured API Keys were the appropriate solution. They allow you to securely apply filtering on the query, done via the back end for optimal security. That means the JS can directly and securely request the Algolia API without any hiccups or slowdowns.\nWe also needed to add a tagging system to ensure users could and would only access meetings that were meant for them. Each indexed meeting contains a tag array with a list of participants that looks like this: `[“user_xyz”, “user_abc”]`.\nWhen users start a search, their searches are automatically filtered with their associated tags. As a result, they will only be able to search meetings that contain both their keyword AND their user ID. This way, we’re guaranteed not to show anyone else’s results.\n\nTags are associated with the meeting when they are indexed.\n\nView the code on Gist.\n\nThe back end fixes the tag filters and prepares a… session-secured token for the search using Algolia’s php client.\n\nView the code on Gist.\n\nThe data for search is sent to the front end through a specific endpoint.\n\nView the code on Gist.\n\nThe JS client uses the token and the filters to directly request the Algolia search endpoint.\n\nView the code on Gist.\nSolid also allows users to ignore events on a case-by-case basis. We know you don’t want to see meetings you’ve ignored in your search results, so we filter them out per your settings.\nPerformance at scale\nCould we have done this using other methods such as Elasticsearch? Sure, but at what cost? To get the same performance level and still keep up with security, we would have had to pour quite a bit more resources into the project. Resources, ultimately, that we just don’t have.\nWhile we did all of the above in only five days using Algolia, it would have taken close to three more weeks to complete on Elasticsearch. That’s not even accounting for the maintenance work we would have needed to perform as we processed our initial 500k meetings. This would have been increasingly difficult to scale quickly as we onboard new users. We usually love it when our developers tackle difficult problems (and they do too), but this is just an unnecessary problem to have. But protecting your privacy? That’s a problem that's right up our alley! And giving you great search? That’s right up Algolia’s!"
  },
  {
    "id": "4526-0",
    "title": "Announcing instantsearch.js: Everything you need for great search in one library",
    "author": "Alexandre Stanislawski",
    "text": "We’re very excited to announce the launch of instantsearch.js, our new library of UI widgets to help you build the best instant-search experience with Algolia’s hosted search API.\nSo how does this library change search?\nOur mission at Algolia is to make sure any website or application has the best search experience. But we don’t want to make this only available to developers with extensive coding knowledge. Which leads us to the second part of our mission—making great search available to everyone. This is where instantsearch.js comes into play.\nWith this new library, we’re separating the logic and design of search, empowering developers and designers alike to create unique search experiences.\n\nThrough hundreds of search implementations, we’ve identified and developed best practices for delivering a top-notch search experience. instantsearch.js packages those best practices into a single JavaScript library of UI widgets that you can assemble and customize as you wish.\nWhen building or adding an instant-search experience to your website, you can now think in terms of end results and how pages will look and feel, rather than having to carefully create and refine an Algolia query.\ntl;dr; Using instantsearch.js, you can:\n\nBuild an instant-search UI faster than ever\nFocus entirely on building the UI, forget about the complex code\nPlug and play—everything is included in one library\n\nHow did we get here?\nThere are two ways to implement Algolia on the front end. The first way is with an autocompletion menu.\nThis kind of UI can be broken down into two elements: the text input and the search suggestions. As the user types, the search suggestion list refreshes and refines, providing the user with only the most relevant results.\nAlgolia's autocomplete.js in action\n \nThe second way, what we like to refer to as instant search, is more difficult to implement as it is made up of multiple elements such as search filters, results and pagination that interact with… one another. And you have to glue them together manually.\nE-commerce example using Algolia's instantsearch.js\n \nWe first presented this instant-search experience to our users in the form of a tutorial featuring a sample project that could be modified to fit their needs. This project was heavily based on our JavaScript helper. Unfortunately, there are several drawbacks to this kind of approach:\n\nYou need to fully understand the whole tutorial code.\nYou can’t easily configure individual components.\nOnce you’ve copied and pasted the code into your implementation, you won't automatically get important updates from our engineers.\n\nThe tutorial was a solid start, but it was clearly not good enough. The JavaScript helper provided a starting point—an API that simplifies the call to the search engine—but even with all the other moving parts (JavaScript client, jQuery and jQuery plugins), there were still things missing in order to be compatible from a UI perspective.\nWe really needed a solution that offered much more guidance in terms of building out a great search UI. So, to overcome these limitations and make implementing instant search easier for everyone, we started working toward a more universal solution—instantsearch.js.\nWith instantsearch.js, we propose a simple and powerful API that allows our users to build a complete search experience simply by mapping parts of the UI to components of the instant search via a series of widgets.\nWidgets are essentially the UI components that deal with either the input of the search (such as the searchBox or refinementList widgets) or the output of the search (such as the hits widget). The instantsearch.js library includes a set of core widgets, but you can also build and implement your own. Here are some custom widgets we created to inspire the community.\nAs with most of our projects, the instantsearch.js code is entirely open source and available on GitHub. If you’d like to contribute, please jump on… in!\nReady to start building your own instant-search experience?\nGet started with our instantsearch.js documentation"
  },
  {
    "id": "4458-0",
    "title": "Collaborative 3D Printing Made Possible by Pinshape, Search Powered by Algolia",
    "author": "Guillaume Dumortier",
    "text": "Pinshape, a collaborative 3D printing design community based in Vancouver, has made creating the top marketplace for 3D artists to sell and share their designs their focus because they know designers are fueling and inspiring the 3D revolution. But it’s not always easy to create a superior product and user experience with the limited resources of a startup. With the help of search powered by Algolia, Pinshape is now able to provide the truly collaborative website experience necessary for powering the 3D printing design community.\nWho is Pinshape?\nPinshape is a global marketplace built to enable designers to collaborate on and share 3D printed designs, enabling downloads straight to a customer’s printer. We offer more than 10,000 unique 3D printing designs, all available to download or stream, and each design comes with step-by-step printing instructions so the person using the file prints the best possible product.\nIn 2014, Pinshape was accepted into 500 Startups, a startup accelerator in Silicon Valley, and the business really took off. Since then we've worked hard to build our website into a creative community with thousands of designs and collaborators from 130 countries around the world. Pinshape encourages collaboration to make 3D printing easier for people everywhere, and community and shared learning is part of our culture. We've got big plans, and we're just getting started.\nWhy 3D printing?\nWhen we founded Pinshape in 2013, 3D printing technology was still in its early stages. We kept hearing more and more about it and saw huge potential for the technology to have a global impact. We knew we wanted to be in the 3D printing industry, but we really had to narrow down what specific area to enter.\nWe didn't have the technical skills to build 3D printers or the materials science background to innovate in the field, so we had to think of something else. After spending time in the 3D online community, we noticed a lack of quality printable content; for those… who could not create their own designs, it was extremely difficult to find good 3D print files to download. Our solution—create a community and marketplace to connect 3D printer owners with 3D designers so that everyone can access great 3D print files.\n3D Printing Industry Barriers\n3D printing technology is still foreign to the general public. One of the most common questions we get is “sure it’s cool, but what do people actually print?”.  For 3D printing to reach mass adoption, the applications of the technology need to be clearer to consumers. Let’s be real, before Apple and Microsoft came along, no one really knew what to do with personal computers. Similarly, we need to show consumers the killer applications of 3D printing for them to realize its true potential.  \nAnother challenge is that the user experience of 3D printing is relatively complex. There are numerous steps and software programs required to get from idea to physical product, and there is a steep learning curve when it comes to actually using the printer because of the technical difficulties associated with running and maintaining a complicated piece of hardware.\nWe want to bring this technology mainstream by creating a place for makers and designers to share and explore quality content. We also help 3D printer owners get the best prints possible by providing printing instructions from the community. \nThe Importance of Search \nFor a printer or maker, the fun is in the printing, not searching for a specific design. Technical challenges aside, we want to make it easy and fast for users to find amazing designs to print. Since our site has so many different 3D print files to sort through, we need to provide a search experience that’s fast, easy to use, and powerful.\nOne of our challenges was creating a product and user experience that is superior to that of our competitors, but with the limited resources of a startup. Before Algolia, we were using a free tool called Elasticsearch.… Elasticsearch had some great qualities, but it took a lot of time to manage and code. We ultimately needed a tool that managed tuning performance and the search ranking algorithms so that we could focus on UI and UX. We heard other startups were using Algolia to fine tune their search rankings, so we had a good idea that Algolia had the technology we needed. \nAlgolia’s Impact\nWith Algolia, our search load time is approximately 160% faster than with our previous search tool. Our users can now quickly find what they are looking for with filters that update without a page refresh. It makes our site more aesthetically appealing, and the autocomplete makes the search time even faster. We also love the Algolia analytics page that allows us to see popular searches so we can figure out exactly what our users are looking for. This gives us valuable insight into our design community. Since launching Algolia, our searches have increased by 91.23%, and our transactions have grown with it.\nSince we were spending a lot of time on our search functions, we know there are others out there who are experiencing the same problem. We estimate that using Algolia instead of developing the search code internally has saved us at least two months of work.\nWith Algolia, we deliver improved search functionalities at a lightning quick speed. Searching has gone from a bit of a chore to something that’s delightful to use, and it keeps people coming back to find their next print, which is what Pinshape is all about.\nTry Algolia free\nIllustration by Justas Galaburda"
  },
  {
    "id": "4328-0",
    "title": "Algolia's Checklist for Selecting a Critical SaaS Service",
    "author": "Julien Lemoine",
    "text": "Here at Algolia, we love SaaS companies (we're not at all biased) and use a lot of the different services they offer. Before selecting a particular service, we go through the usual process of comparing the different features and prices, but we also dig a little deeper and think about other aspects such as the infrastructure of the service.\nThis is especially important to us when the proposed service will play a critical role in our own infrastructure, like enabling our cloud DNS or helping provide support to our users. Here are five criteria we seriously consider that may be overlooked when selecting an infrastructure critical SaaS provider. \n1. Support\nHaving top notch support that you can rely on is mandatory for all critical services you use. I recommend taking the time to specifically engage support and test their capabilities and capacity during the trial period. If support is not excellent when they're trying to sell you, there's a high probability that it won't blow you away later on. I also like to to ask if they provide 24/7 support with SLA requirements on the response time. If an issue comes up in the middle of the night, will they wake someone up to fix it, or will you be the only one awake, pulling your hair out?\nA good way to ensure a high level of support is to verify the tools used for incident management and the escalation policy they have in place. It's always great to hear that someone is using a tool like PagerDuty or VictorOps and has someone on call around the clock. If they don't seem to understand the concepts of incident management and escalation, run away! Fast!\n2. High availability\nThe availability of a service's infrastructure should also be part of your consideration process, but a lot of SaaS providers may not even bring this up as part of the conversation. Fortunately, there are a few questions you can ask to determine whether they maintain high availability.\n\nIs the service distributed across multiple… datacenters? The minimum you should accept is a service distributed across multiple availability zones (especially the data storage component). If the service is fully hosted in one datacenter, I would not recommend using it for critical purposes.\nIs the service distributed across multiple providers? If high availability is important, you should check that everything is redundant, including the hosting providers.\nIs the service distributed across multiple regions? Few services have this level of distribution. Multi-region distribution ensures not only high availability, but positively impacts performance as well.\nAre there any SPoFs (single points of failure)? This is a tough question, but it's definitely worth asking so you have a better idea of the service's architecture.\n\nWhen it comes to availability, I don't put much stock in what's promised in the SLA. I have seen too many companies promise 100% who really don't have the architecture to back it up.\n3. Trust\nOne tricky question you should probably ask yourself before choosing a service is—Can I trust this company, or are they completely nuts? The age of a company and a company's certifications are a decent indicator of stability, but an older company doesn't necessarily make for a more trustworthy company.\nOne of the first elements I always check that can give a lot of information regarding the transparency of a company is their service status page and the level of detail this page goes into. I also search for at least one Post Mosterm Analysis (also called Root Cause Analysis) of a previous outage. I don't know of any serious services who haven't experienced at least one outage, and looking at the way they communicate on the outage and the subsequent followup to ensure it won't happen again can give you a lot of information about the technical skills of the company and the transparency you will get as a customer.\n4. Security\nIf you are sending important data to the service, you probably… care a lot about the security of the solution. I personally don't rely on security certifications anymore because I have seen in practice that they do not mean much. I prefer to ask the following basic questions to get a better idea of the security of a solution:\n\nWho has access to your servers? Are they employees? I like to see that only a small number of people who are part of the company have access to the server. I personally consider it a show stopper if servers are managed by an external company.\nWhat is the process for accessing a production server? I like to hear an answer like \"SSH via passphrase but requires a VPN connection that is established via a two factor authentication,\" or at least something that shows there are several actions required in order to gain access to the server. You would be surprised to hear the number of stories where a stolen laptop resulted in someone gaining access to the entire infrastructure of a company.\nHow do you test the security of your software? There is no magical answer to this question, but I like to hear that there is at least a penetration test done by an external company or, even better, a public bug bounty program that involves the community such as HackerOne or BugCrowd.\n\n5. API Quality (Optional)\nIf you plan on using a service's API, you need to make sure that it's stable and scales well. We've had a few surprises with products that worked really well, but when it came down to it, the APIs were almost unusable because they either went down too often or had a high rate limit and just couldn't scale. I recommend testing the API in real conditions before making the decision to move forward with a service with an API, especially if you plan on making a one-year-commitment to the service.\nAt the end, infrastructure is key!\nWhen an external service is critical, you need to understand the infrastructure of this service. I hope this checklist will help you better make your next SaaS… decision. If you are still not convinced of the solution after this list, it isn't necessarily a terrible idea to use the solution, but you probably need to implement a fallback solution in case of failure. The price of the fallback service itself is usually not very high, but it can cost a lot of money to maintain usage of several services in your software. As per usual, this is a tradeoff 🙂\nI would love to hear how you evaluate your mission-critical SaaS providers, especially if you think we're forgetting something important! Feel free to leave us a comment on this post to add your top considerations.\nIllustration by Justas Galaburda"
  },
  {
    "id": "4482-0",
    "title": "Reveal the Hacker Within: Build a Better Search Experience for Free with Algolia’s new Hacker Plan",
    "author": "Nicolas Dessaigne",
    "text": "Yes, Algolia has a free version, and now it’s even better.\nLike many SaaS companies, we offer a version of our product that’s free to use. We want people to be able to try Algolia without feeling like they have to make a financial commitment, and we also want to make our technology available to companies just starting out who might not be able to afford any monthly subscription fee.\nBut our free plan, otherwise known as our Hacker plan, just wasn’t cutting it. We have historically limited access to this plan to select open source projects and nonprofits, but we want to really allow people to test the full functionality of Algolia. Ultimately, our Hacker plan couldn’t have the restrictions we initially put in place.\nSo, we decided to change it.\nOur new Hacker plan lets any organization sign up for Algolia and run up to 100,000 operations on 10,000 records without limiting any of the functionality such as instant search and custom ranking that you’ve come to expect from our product (we only ask for unobtrusive branding in return). Whether you’re a CTO who wants to fully evaluate a search solution before allocating budget or a developer whose project could benefit from instant search, we hope our Hacker plan gives you the access you need to best-in-class search technology.\nAnd if you get to a point where the Hacker plan doesn’t meet your needs anymore, it’s really easy to upgrade to one of Algolia’s paid plans. All of your data moves with you, and the upgrade is seamless.\nOne of the reasons for the Hacker plan is to support organizations furthering technology and programs doing good to the best of our abilities. We hope nonprofits and community projects such as Laravel can continue to take advantage of our free offering. Laravel has been using Algolia to enable search within the documentation for their PHP framework, allowing them to focus their money and efforts on enabling others to build more agile applications.\nThat’s our Hacker plan in a… nutshell. Feel free to sign up and let us know what you think. Don’t hesitate to reach out if you have questions or concerns.\nGet started implementing Algolia"
  },
  {
    "id": "4337-0",
    "title": "How we unit test React components using expect-jsx",
    "author": "Vincent Voyer",
    "text": "React was designed to solve one problem—how to build applications with data that changes over time. Considering the as-you-type search experience offered by Algolia wherein whole results pages, hits and refinement links alike, are updated at each keystroke, React was the obvious choice when we needed a tool to build out our new library of UI widgets: instantsearch.js.\nWe quickly struggled, however, with the unit test best practices for React components. We jumped from Github issues in facebook/react to blog posts and discuss.reactjs.org but couldn't find a clear unit testing strategy.\nNow that we're happy with our testing stack, we wanted to share it with the community to hopefully make your testing process a bit easier. Feel free to also check out the unit testing tips we shared a few months ago.\nHow to test a React component\nBefore digging into the whole testing stack, how do we test a React component?\n\nIf you search for \"React unit testing\", you'll quickly find the popular React discussion—What’s the preferred way to test React.js components? In the thread, React developer Ben Alpert advises testing React components using a technique known as shallow rendering.\nHere's an example of a Button and a Label component tested with mocha:\nView the code on Gist.\nAnd here is the output when you run it:\nWhat about failing test cases? Let's make it fail by changing our test:\nView the code on Gist.\nNot too bad. Shallow rendering provides a way to render a React element only one level deep. This means that if you have children elements (like Label in our Button), it will not trigger their render methods. Very nice for unit testing!\nThe shallow rendering technique is simple to use and well suited for unit testing as it takes a React Element as INPUT and produces a React Element as OUTPUT.\nThere are other techniques available, but this is the currently recommended one and we expect (?) the React team to develop more tools around it in… the future.\nBut wait, it's not over! Did you see that diff? You passed some nice expected JSX (), and all you got was a weird object output diff.\nThis is because JSX is transpiled to a React.createElement call that then returns a React element (a JavaScript object). So when doing expect(something).toEqual(somethingElse), you are just comparing two JavaScript objects.\nIt would be better to get something like this:\nLet's make it happen! OK, we need a bit of tooling to get there.\nWhat is the rackt team using?\nAfter we found out about shallow rendering, we needed a good stack to make assertions and run the tests. We decided to have a look at what rackt, the Github org responsible for react-router and redux (two very popular react libraries) was using. After all, the closer you are to the tools of your community, the easier it is to get contributions.\nThe rackt team is using mochajs/mocha to run the tests and mjackson/expect to write assertions. They also follow the facebook/jest convention of putting tests next to the files they are testing:\n\nIntroducing expect-jsx\nAfter choosing mjackson/expect as our assertion library and mochajs/mocha as our test runner, we wanted better JSX diffs instead of long object diffs when our tests were failing.\nWe asked the React community how to do this and got some great feedback. In fact, another assertion library already did it—bruderstein/unexpected-react-shallow. alexlande/react-to-jsx was also a candidate for implementing the JSX string diff in the expect assertion library.\nUnfortunately, those libraries were not meeting our expectations. So...we built algolia/expect-jsx.\nLet's enhance our previous test with this new module:\nView the code on Gist.\nexpect-jsx transforms the passed React elements to JSX strings to nicely compare them. This seems a rather naive approach, but this is completely OK for unit testing.\nUsing expect-jsx will save you a good amount of time and spare you from having to… write things like  expect(result.props.children[0].props.children.className).toEqual('U mad?');\nexpect-jsx is based on algolia/react-element-to-jsx-string. This means that if you're using tape, chaijs or jasmine, you can easily build the same assertions goodness within a day and publish it to npm.\nSince we released react-element-to-jsx-string, developers have implemented JSX diff in multiple assertion libraries:\n\ntape-jsx-equals\njsx-chai\njasmine-expect-jsx\n\nWarning: If you use Karma runner to launch mocha tests within browsers, you will not have any nice object or string diff. This is currently an issue in the html-reporter.\nSimulating events like onClick\nAs of today, the shallow rendering technique does not support simulating clicks using React test utilities Simulate.\nThis means you have to resort to directly calling the underlying function of your handlers props. Doing so doesn't introduce any test-specific nonsense, as it's the same thing Simulate would do in the end.\nLet's change our Button component to illustrate this:\nView the code on Gist.\nOur test now looks like:\nView the code on Gist.\nAs you can see on line 24, we're just calling the props.onClick handler directly to check that our custom handler has been called properly. Easy, right?\nWell, that's the way we unit test our component handlers. If you have a better solution, leave it in the comments!\nTesting using references\nShallow rendering does not have access to any references you may have defined in your Component, so you cannot use them when testing with the shallow rendering technique. This is being investigated by the React team.\nIn the meantime test your refs dependent Components using the standard Test Utilities.\nnpm run test:watch\nAt Algolia, we try to minimize the entry cost to a project for any developer. All projects have an npm run test:watch task that we can use to TDD with a rapid feedback loop.\nYou can easily get one by adding the following to your package.json:\nView… the code on Gist.\nYour development workflow will then look like this:\n\nWhere to go from here?\nThe whole code used in this article can be found at algolia/react-test-boilerplate.\nIf you need some DOM features in your tests, check out tmpvar/jsdom and rstacruz/mocha-jsdom.\nIf you need to run your tests in real browsers, you can use karma-runner or defunctzombie/zuul coupled with Sauce Labs or Browser Stack.\nIf you liked this post and are interested in a follow-up article about those subjects, leave a comment or tweet @algolia.\nHappy coding!\nLearn more\nUpdate November 02 2015\nWe did a talk about this at ReactJS Paris, here are the slides:\n\nSources\n\nWhat’s the prefered way to test React.js components?\nUnit testing React components without a DOM\nTesting React on Jsdom\nTest Utilities\nApproaches to testing React components - an overview\nTesting React Components\nTesting Components, The Easy Way\nbruderstein/unexpected-react-shallow\nfacebook/react/#4835\nmjackson/expect/#37\njquense/teaspoon"
  },
  {
    "id": "4316-0",
    "title": "OneSignal Builds Hacker News Notifications With a Little Help From Algolia",
    "author": "Guillaume Dumortier",
    "text": "The Algolia community is one of our strongest assets, and we love when customers tackle projects with our technology in unexpected ways. OneSignal recently reached out to us to share how they used Algolia's Hacker News Search API to create a demo of their notification platform, and it's pretty awesome. OneSignal CEO George Deglin joins us for a guest post to explain how they used Algolia to spread the word on the capabilities of Web Push and OneSignal.\n\nAt OneSignal, we help mobile and web developer send smarter and more effective push notifications to their users. Our tools make it easy for developers to schedule, localize, A/B test, and monitor the notifications they send.\nIn late 2014, the World Wide Web Consortium published a Recommendation of HTML5. Among many impressive features, the one that stood out the most to us was Web Push notifications. Finally, web developers were being given a much better alternative to e-mail for re-engaging their visitors.\nEarlier this year, Google Chrome was the first browser to publicly launch Web Push support. We were right there with them, announcing our support for this new technology just days later.\n\nWe knew we couldn’t simply launch and wait for clients to sign up. Nearly every developer we talked to had no idea that websites could even send push notifications now. We had to find an effective way to demonstrate our technology to thousands of web developers at once, and there’s no better place to do that than Hacker News.\nTo get upvotes on Hacker News, we’d have to create a demonstration that appealed to readers and wasn’t self-promotional fluff. With the help of Algolia, we found a perfect solution.\n\nInstead of simply talking about our launch of Web Push support, we built a demo that provides real value to Hacker News readers.\nWe stumbled upon Algolia’s easy-to-use Hacker News Search API after seeking out an easy way to identify trending Hacker News Stories. Next, we created an Algolia query to identify Hacker… News stories that received over 250 upvotes. Finally, we would use Web Push to deliver notifications of these stories to users who visited the site and opted-in. Simply by combining the two with a script that would run every 5 minutes, we had built a push notification system for Hacker News that required nothing but the Chrome web browser!\nFor the developers out there, here’s the code we used.\nOur efforts were a success! After posting the story on Hacker News, it quickly received enough upvotes to briefly trend on the front page. Thousands of developers learned about the potential of Web Push and how OneSignal can help them get started with it.\nFind the right tutorial for your needs"
  },
  {
    "id": "4127-0",
    "title": "Geo-Search on steroid!",
    "author": "Julien Lemoine",
    "text": "Geo-search is playing an important role in user experience, especially on mobile applications. Therefore we've never stopped challenging it and we are excited today to announce a big improvement!\nThis new geo-search release simplifies most of the use cases, adds several features, and maintains backward-compatibility! And because we are hardcore when it comes to performance, we've made sure this update won't impact anything. Better yet, all queries will actually be faster!\n \n\nThe end of the radius pain!\nThe most used geo-search feature was the retrieval of records around a specific geolocation. It is widely used by mobile applications via the device's GPS position. On browsers, you can use our IP based geolocation feature. This feature requires a maximum distance called \"radius\" to perform the search, which was set by default to 10km. Because radius has an impact on performance, it is important to keep it low. In reality, it may become extremely cumbersome for our users to set it as the radius would depend on the area's information density!\nFortunately, we have removed this pain in our new geo-search! While you can still manually define the radius, you can now let the system adjust it according to the density of the area for you ... without doing anything! Just leave the  query parameter unset! The radius will adapt itself to collect a few thousand records. Its value will also be returned in the result set (in the  attribute). This information can help you display your map at the best scale. If you want to have a minimum radius size, we also introduced a query parameter called  that configures the minimum value that can be accepted as a radius.\nWe hope this new feature will help you and will make it super easy to have a good geo-search!\n \n\nSeveral geolocations per record\nOur previous geo-search was limited to only one geolocation per record. No more! Our new version now supports an arbitrary number of geolocations per record. You have just… to use an array syntax.\nExample of a record with one geolocation:\n\nExample of a record with several geolocations:\n\nIf your records contain several geolocations, you will be able to retrieve the matched geolocation and the distance in meters by requesting the ranking information ( query parameter).\nHere is an example of a hit with several geolocations:\n\n \n\nNew ways to filter\nIn the past months, we have received several requests to improve the number of operators supported to filter by geolocation. We were only supporting filtering by a bounding box but we are now excited to offer three new types of filtering!\n\nFilter by a polygon (defined by a list of point), you can look at the documentation of the insidePolygon query parameter\nFilter by a OR between several bounding boxes, you have just to pass several times the insideBoundingBox query parameter\nFilter by a OR between several polygons, you have just to pass several times the insidePolygon query parameter\n\nWe need your feedback!\nWe see an increasing number of users having geo-search needs, especially on mobile. We hope you will like this big evolution of our geo-search feature and that it will simplify your life! We would love to have your feedback and ideas on how to improve it, feel free to contact us anytime!\nGet started with our tutorial"
  },
  {
    "id": "4248-0",
    "title": "How Beyond Retro Improved Its Ecommerce Site with Algolia's Magento Plugin",
    "author": "Guillaume Dumortier",
    "text": "Editor's Note: Jascha McDonald is Software Developer and Application Architect for BankVogue, North America's trusted partner in buying, selling and shipping used goods around the world. One of their companies includes Beyond Retro, a vintage clothing retailer in Europe.\nFounded in London in 2002, Beyond Retro's stores offer an exhilarating pick ‘n’ mix of vibrant 20th century fashion. Led by an inspiring team of true originals, the staff are the beating heart of a creative community that spans from England to Sweden with verve and style. Beyond Retro's retail network recycles upwards of 500,000 items each year. Purchased directly or indirectly from charities, they're part of a global movement that brings new purpose to excess clothing and accessories.\nBeyond Retro's Magento-powered global site offers more than 35,000 objects with a twist: they're all unique pieces! We asked them how they managed to deal with such a difficult constraint.\nAlgolia: What technical challenges have you faced building Beyond Retro?\nJascha: Since almost all Beyond Retro product is one-of-a-kind vintage there are many difficulties we faced when breaking into the e-commerce world. All items need to be photographed and classified individually. When a garment sells it immediately needs to be removed from the site.\nProducts being constantly added and removed from the catalogue caused us incredible headaches in terms of balancing website performance and an up to date catalogue. Our products are so varied that most search architectures were not sufficient or returned results too slowly.\nTo that extent, Magento's built-in search was not effective and Magento Catalogue had performance issues with our limited computing budget. Filtering through products was slow, hindering our customers ability to find the right product for them. There had to be a better solution than the built-in Magento tools or the Manadev navigation we were using.\nA: What did you do about it?\nJ: Our technical… agenda was very clear: we needed to find a replacement for our search to better help customers find product faster. We also needed to speed up the site as the queries took too much time to process.\nAs were originally looking only for a search replacement, we quickly found out that many were very expensive. We discovered Algolia be a recommendation from an SEO consultant. This was originally suggested to us as just a search replacement but after experimenting with the product we decided that it would be an effective catalogue replacement as well.\nAlgolia's price point is great and its speed beat all competitors we benchmarked. The free trial was also a great incentive as it gave us the ability to test the integration before deciding. Integration was straightforward and well documented and developers on Github helped us resolve small technical glitches.\nA: What changes have you noticed ?\nJ:  Speed was already a decisive factor for us to implement Algolia. It directly impacted the customer experience on our site. Algolia is insanely fast, it's almost like it reaches into the future to get your search results. Before, we would have periods in the day when some pages would take up to 10 seconds to generate the HTML.\nNow, we are averaging about 60 milliseconds with a more stable performance across the board. We also lowered the average server response time from 0.9 to 0.55 seconds. Server performance has been drastically increased and the load spikes we used to see have disappeared. Better yet, the whole implementation eventually saved us about 30% on our infrastructure costs.\nThere was also an unexpected benefit to implement Algolia as our search. It allowed us to turn off Magento Full Page Cache invalidation for category pages, a major improvement for Beyond Retro's site performance.\nA: How did you proceed ?\nJ:  It took about 1 month to roll out the full Algolia catalog replacement. First, we installed Algolia's Magento plugin and iterated with the… help of the documentation and the Github community. We then applied our customized theme, tested it and rolled it out. It was a very straightforward process. We are now working on increasing the usability of our search with Synonyms and the other search features available to us.\nExplore our Magento Extension\nLearn More about Ecommerce Site Search"
  },
  {
    "id": "4139-0",
    "title": "Building a Video Discovery Platform with Algolia: Unreel.co",
    "author": "Guillaume Dumortier",
    "text": "When it comes to Algolia users, we're dedicated to providing the best quality, value and service to meet their needs. The more we discuss with them, the more amazed we are by the quality and diversity of their projects. That's why we're introducing today a new series of articles to highlight our favorite projects from the Algolia community.\nThis first project caught our eye because it tackles the hottest content trend of 2015: online video. Krish and his team at Unreel.co are working on creating the best video discovery platform on the web. We've invited them to tell their story on our blog in the form of a guest post.\n \nThe Concept\nWe founded Unreel with the goal of unveiling the best video moments on the web.  We came up with the unique concept of automatically crawling the internet’s videos to figure out which parts were actually worth watching.  Those videos, with their worthwhile moments tagged and categorized, would then be viewable on our site, Unreel.co.  \nBy presenting videos broken down by their ‘moments that matter,’ we had just invented a new way to discover, view, and share trending videos. Today we have collected over 3.5 million moments/scenes available for search with over 90,000 videos to watch from various channels. \nThe Proof\n\nEvery new Beyonce video generates tons of attention and thousands of conversations. Her 7/11 video clip was no exception. What was different this time is that people were focusing more on the small details throughout the video than the song itself. Altogether, we identified more than 600 moments/scenes within the video that people most talked about, from her baby Blue Ivy making a split-second appearance, to people running in the background.\nThe Execution\nTo accomplish this, we built an AI - we call it Bumblebee internally (think Transformers)  - to find the most highlighted moments in digital video and bring it to the surface in a new and interesting way. Our tech looks at things like trending data,… meta-data, UGC, and engagement data around videos​ to identify moments within viral videos that strike a cord with people.  \nBe it moments that make you laugh, cry, cringe, or experience any other number of emotional reactions our favorite videos can cause us. As we go out and sift through all this incredible video data, we programmatically create video channels on our platform to categorize and organize user’s video discovery experience. In upcoming versions of Unreel, we'll let anyone create those channels to tune their viewing experience so you can watch, interact with, and share only the moments you care about.\nThe Secret Sauce\nThe key to our success is going to be our ability to empower users to customize their discovery experience.  No tool is going to be more important in doing that than search.  We had a classic build vs. buy challenge when it came to incorporating search into our platform. Our initial thought process was to build an in-house solution, setup a full infrastructure/framework and house our own search. \nWe quickly realized this would have been a huge undertaking, burning through resources we could ill afford to loose.  We needed to find an alternative; an easy to integrate, customizable, instant search experience with a pay as you go model.  Algolia is just that.  \nThe Algolia Factor\nOur goal is to make moments search easy – so we set the following attributes to index: moment tag, title, description and video channel.  And ranked our search results based on class (moments vs videos) and rank (video score).\nWe discovered Algolia on Product Hunt, and have been fans ever since.  The detailed documentation and tutorials made it seamless to integrate it into our platform. The whole process to go live took approx. 3 weeks – which, by the way, is super awesome by the way compared to building our own in-house solution.  \nWe first spent a lot of time on determining how our data would look like in Algolia as a record to be optimal for… search with all the data points we needed for building a unique search experience. This really helped us to make sure that we don’t waste type re-syncing/re-indexing data unnecessarily. \nWe then used the mongo-algolia adapter to sync all of Unreel data into Algolia. We had to make several tweaks to the adapter to make it work for us. We also built our UI in parallel to build out the interfaces of displaying search results appropriately on unreel.co\nThe Benefits\nCalculating the cost of development time, resources and monthly maintenance of a search-cluster, our search integration has already paid off in a big way and is propelling our progress. \nWe’ve presented search to some big name partners in the video space and already received applause both for our ability to search scenes within videos but also the speed at which the videos show up. Unreel’s future is bright thanks to Algolia search.\nWhat's Next\nUnreel is launching its first iOS app called JustLOL - which highlights the best scenes of the funniest videos on the internet. \nWant to showcase your project? Drop us a line at hey@algolia.com"
  },
  {
    "id": "3973-0",
    "title": "How to Build a Helpful Search for Technical Documentation : The Laravel Example",
    "author": "Julien Lemoine",
    "text": "You can offer the best technology in the world, but if you don’t document it properly, the only engineers that will use it are the ones that are paid to do it, or that enjoy adventure. Sure enough, your goal is to attract every type of developer.\nBuilding a great documentation search seems very easy at first but there are several pitfalls that you need to be aware of and avoid. This blog post explains those pitfalls and describes the recipe we have used to develop the Laravel documentation search.\nUPDATE: we have launched DocSearch, the easiest and fastest way to add search to your documentation. Take a look, it's free!\nPitfall Nº1: the web page as the default entry\nDeveloper documentations often mean lengthy pages filled with a lot of content. Most people try to index the complete page as one entry in their search engine. But, they discover later on that there were a lot of edge cases and they try to fix them through relevance tuning but it quickly becomes an endless story as the issue comes from the actual indexing itself:\n\n\n1. Relevant content only\n\n\nFor example the query  will match the QuickStart page because the menu contains  and the first paragraph contains the  word. This is not the kind of match that provides a good user experience.\n\n2. Pages contain too long pieces of text\nDevelopers don't like to change web pages too often and they like to have long pages containing a lot of information. If such a page is indexed as one document, it will almost systematically trigger relevance issues. This is why we do not recommend to use a standard web crawler, but rather a scrapper to have access to the original content (most of the time available in Markdown).\nFor example, querying  will match the Query Builder page because it contains a paragraph with the word  and another paragraph with the words  and . This is a false positive because it is not relevant: the more text you have on a page, the more irrelevant results you will get.\n \n3.… The right anchored section\nIn order to deliver the best user experience, it is key to open the page  at the exact position of the match. This is made very difficult if you only index one document per page. That's why there are so many documentation searches that just open the page at the top and the user needs to scroll or use the search of his browser to jump to the right section. This not always easy and is a waste of time.\n\nPitfall Nº2: indexing titles only\nIndexing the titles of your documentation page will probably answer common queries but this is not enough. The underlying paragraphs contain most of the words your users will search for. To obtain a great level of relevance, it's important to index the whole content, body text included.\nIn this example, the text is required to correctly answer to the  or  queries.\n\nPitfall Nº3: poor relevance\nWith most search engines, relevance is the trickiest part of the configuration because it is often defined by a unique and complex formula that mixes a lot of information almost impossible to manage. Engineers often adjust the formula or add some bonus/malus scoring to improve the results on one specific query. Since they don't have any non-regression tools, they cannot measure the real impact for all queries. The consequences can be significant.\nIn order to keep the ranking under control, it is key to split the ranking formula in several pieces that you understand and will tune independently. In practice we are able to split the ranking formula with a Tie-Breaking algorithm.\nLet's imagine your ranking formula is split in 2 parts:\n\nthe first one defines the textual relevance of a matching hit,\nthe second one defines the importance of a matching hit (from a use-case/business POV).\n\nYou can then first apply the textual relevance and only if two hits have the same value move to the use-case/business relevance (importance). This is the best way to ensure your end-users will always have relevant hits… first (from a text POV, matching exactly their query words) and then - in terms of relevance equality - tie the results using the business relevance.\nSince you're not mixing together the text & the business relevance (but applied them one after another), you can modify the business relevance without impacting how the text relevance is working.\nGetting Started With Realtime Search\nOur recipe\n1. Create small hierarchical records\nIn order to solve all those pitfalls, we split the page in a lot of smaller chunks indexed as separate records by using the HTML structure of the page (H1, H2, H3, H4, P).\nSee the Validation page of Laravel's documentation:\n\nThe first record generated will be the Validation page title. It will be transformed into the following JSON object. The \"link\" attribute only contains the last part of the URL, the first part being easily rebuilt with the tag:\n\nThen, the first section of the page (The Introduction) will be turned into the following record. The link now contains an anchor text and that keeps the title of the page:\n\nA paragraph of this page under a H3 section would be translated into the following record:\n\nThis approach fixes pitfalls #1 and #2. We have solved the problem by indexing each chunk of text as an independent record while keeping the titles hierarchy in each record.\n2. Use a tie-breaking ranking algorithm\nAlgolia is designed natively to use a Tie-Breaking algorithm to make sure everyone understands & is able to tune the ranking. Now,Pitfall #3 can be easily resolved by applying the settings we recommend for a documentation search implementation:\n\nMatching hits will now be sorted against those six ranking criteria: the first 5 are related to text relevance and the last one is the custom business relevance.\nRanking criterion Nº1: number of matched words (words)\nFirst, we sort the number of query words found in the records. We have decided to process the query with all words as mandatory (AND between query terms).… If there are not enough matching words, we run the query again with all words as optional (OR between query terms). This process is configured with a single index setting and allows your to get the best of both worlds: AND guarantees to reduce the number of false positives while OR allows to return results even if the query is too narrow.\nRanking criterion Nº2: number of typos (typo)\nIf two records match with the same number of search terms, we use the number of typos as the differentiator (so we have exact matches first, then matches with 1 typo, then matches with 2 typos, ...).\nFor example if the query is \"validator\", the record that contains \"validate\" will match with some typos but will be retrieved after the record containing \"validator\".\nRanking criterion Nº3: proximity between query terms (proximity)\nWhen two records are identical for the words and typos ranking criteria, we then move to the next criteria which compares the proximity of the query terms in the record. It will basically count the number of words in between them until a limit is reached (after a certain point they are considered as \"too far\").\nFor example, the  query will have a proximity of 1 when it matches the sentence:  and will have a proximity of 2 when it matches the sentence . We sort this value by increasing order as we prefer records that contains the query terms close together first.\nRanking criterion Nº4: the matched attribute (attribute)\nIf two records are identical for the 3 first ranking criteria, we use the name of the matched attribute to determine which hit needs to be retrieved first. In the index settings, just order the attributes you want to search by order of importance:\n\nThat means that if the match is identified inside h1, it will be better than in h2, better than in h3, etc. You can also notice there is an \"unordered\" flag on each attribute. It means that the position of the match inside the attribute is not considered in the ranking. That's why… the query will match with the same attribute score for a record that contains  or   for the same attribute.\nRanking criterion Nº5: the number of terms matching exactly (exact)\nIf two records are identical for the first 4 criteria, then we use the number of query terms  that match exactly in the record to determine which hit needs to be retrieved first. Because we're returning results after each keystroke, the last query term will mostly match as a prefix (it will match beginning of words). This criterion is used to rank an exact match before a prefix match.\nFor example the query \"valid\" will retrieve the records containing \"valid\" before the ones containing \"validation\".\nRanking criterion Nº6: business ranking (custom)\nThere is still one important thing missing: your use-case/business criterion. If all previous criteria are identical for two records, we use the custom ranking which is defined by the user.\nFor example, searching for  will match the two following records using the most important \"h1\" attribute. That results in a tie on all previous criteria but we want to retrieve the page title first because the other record is a paragraph. This is how the  attribute plays out when added to the records.\n\n\nThe \"importance\" value is a integer that goes from 0 (page title) to 7 (text section under h4) and that we use in the custom ranking in an ascending order (the smaller, the better):\n\nThe complete scale of importance is the following:\n\n0 for h1,\n1 for h2,\n2 for h3,\n3 for h4,\n4 for text under h1,\n5 for text under h2,\n6 for text under h3,\nand 7 for text under h4.\n\nThis is a generic recipe\nWe have successfully applied this recipe on several technical documentation,namely the Laravel documentation and Bootstrap documentation search. The way results are displayed differ but we use exactly the same approach and the same API.\nGet DocSearch for your website\nOne of our missions is to help all developers to better access and navigate technical… documentations. If you are working on an open source project we'd be happy to help you! We will provide you with a free Algolia account and with any support to make your implementation a best-in-class reference. Drop us a note!"
  },
  {
    "id": "3784-0",
    "title": "Search Your Marketplace Like a Boss",
    "author": "Sylvain Utard",
    "text": "We use marketplaces every day. Whether looking up new apps for our phones and computers, or while doing a little shopping on Etsy or Amazon, we're always looking for something.\nBy design, marketplaces are built to deal with millions of objects, thus making search the critical element of the shopper's experience. Navigation through that massive amount of products should be made as easy and intuitive as possible.\nBut, search is complex. Relevant search is way more complex. On top of that, you also need to leverage the few dozens of words that describe your objects in order to always return first the most relevant results your demanding users are looking for.\nRelevancy is more than just relevant VS irrelevant: it's all that gray zone in between. Without an adequate search engine technology, search results are not always as relevant as you wish they would be.\n \nHow to implement the best marketplace search ever? We're giving you the whole recipe and an open-source project to implement it.\nCheck out the GitHub repository\n \nThe Psychology of Product Search\nWhat your shoppers expect\nYour end-users are used to the Google bar:  they want to find what they are searching for whatever the way they write their queries and regardless the number of typing mistakes. They want to find what they are looking for on the first result page, in the top 3 results.\nIt turns out that searching that kind of non-structured (free text) content is not taken for granted. Most of the time, the objects your end-users and searching for are described by a 3-4 words title and a short description.\nWhat your publishers/sellers dream of\nOn the other side of the fence, publishers and sellers will do everything they can to appear on the 1st search results page. Sometimes, they will even come close to the same kinds of SEO techniques spammers use on Google so as to crack their ranking algorithm.\nYour search engine must be able to work around these hacks and must keep returning… relevant results, regardless how the publishers name their products.\nWhat You Need to Do\n1. Deal with user-generated content\nEven if most marketplaces have strict rules and guidelines on object titles & descriptions, you will need to deal with edge-case submissions that might respect the rules but that will trick your search algorithm with SEO hacks.\nOne trick in the book consists of injecting trendy keywords in the object title and/or description. For instance, adding \"Facebook\" to an object title would make the search engine retrieve that object every time the \"facebook\" word is queried, even if the underlying object has nothing to do with it.\nThe number of matches of a query word is also often taken into account. But what if a query word matched multiple times in the object description? Is that better than a single match? Well that's typically something you want to have an answer to. Imagine all Apple accessories resellers who will rush on adding long lists of compatible devices \"iphone, iphone 3G, iphone 4, iphone 4S, iphone 5, iphone 5S, iphone 5C, iphone 6, iphone 6\" just to trigger a \"very relevant\" match on any \"iphone\" query... Probably not something you want, at least not before the actual iphones.\nAlgolia doesn't give more importance to objects that are matching several times compared to records matching once. Instead:\n\nAlgolia ranks the hits depending on the matching attribute weight (more important first),\nAlgolia (optionally) ranks hits based on the position of the matching word in the attribute (considering the \"iPhone 6\" more important than \"Leather case for iPhone 6\" for the \"iphone\" query),\nAlgolia considers all query words by default and fallback considering all words optional if there are not results. Hits matching more words are then ranked before others.\n\n\n2. Embrace typos\nYour users will do typing mistakes. A lot. And even more as the volume of mobile searches keeps on growing. But it makes the task far more challenging and… complex to execute because it also involves find-as-you-type search that retrieves objects before the query is even completed.\nAlgolia natively supports typing mistakes and as-you-type searches. Hits having the less typos are ranked before others. The highlighting feature still works in order to help your users understand where the match occurred.\nThat said, when I search for \"rihana\" (misspelled, with a single 'n') I still want to see the popular \"rihanna\" objects first. That's tricky, because it means that even if there are some objects matching the misspelled \"rihana\" you want to see the real \"rihanna\" first.\nTo implement such ranking strategy, your ranking formula must be able to consider the popular objects separately, and apply the actual sorting twice:\n\ndisplay first the popular objects and compare them against each others to show the most relevant popular hit first,\nand then display the other objects and compare them against each others to show the most relevant non-popular hit first.\n\nAn eCommerce website could apply the same type of formula for discounted and featured products.\nAlgolia's Ranking Algorithm Unveiled is the ideal solution for such ranking strategy: comparing one ranking criterion after another, moving to the next criterion if the results are tied.\nTo deal with such \"uber popular\" objects, you would tag your objects with a \"popular\" flag and inject it into Algolia's ranking formula. Putting that flag as the most important ranking criterion will always retrieve \"uber popular\" hits first (whatever their number of typos) and the non-popular after.\n\n3. Redefine Popularity\nThere may be several business metrics you want to use to refine your ranking algorithm. Those metrics probably include:\n\nthe number of rating stars (out of 5),\nthe number of reviews, downloads or installations\nthe best selling products ($ales)\netc.\n\nThe more, the better. But, you eventually need to deal with the mathematical formula ruling them all \\o/ Not mentioning… that you need to also consider the text relevance as well.\nSo what if the average rating of a object is 5/5 based on 3 reviews; compared to another object which has an average of 4.3/5 based on 1000 reviews?\nAlgolia doesn't combine your ranking criteria automatically because that's really too business-specific: there isn't any generic way to mix those business metrics.\nIn Algolia's default ranking formula, the business metrics are only used to compare hits that are matching equally from a text-relevance point of view. The goal is first to display results that matches the user's query words and then, if several matches are found: to sort them based on that business data.\n\nIf you're dealing with ratings and number of reviews, you should give a look at the Bayesian Average.\nDemo\nHere is an implementation we've done that combines all those best practices. It's a search of Wordpress plugins:\n\n38K plugins indexed,\nDefault typo-tolerance settings,\nPopular flag: set to \"1\" if downloaded at least 10K times,\nBusiness metric used to customize the ranking: the number of downloads of each plugin.\n\nBuilding such experience literally took a few minutes!\nTry it live: WordPress Plugins Search"
  },
  {
    "id": "4039-0",
    "title": "At Springbok Speed: Our First Data Center in Africa",
    "author": "Nicolas Dessaigne",
    "text": "\"Ons het ons deure oopgemaak - die eerste datasentrum in Afrika. Jy kan nou Johannnesburg as jou voorkeurplek vir data kies\" to our Afrikaans-savvy readers!\nIn case you missed it, Algolia is on a quest to make search faster than 50ms from anywhere in the world. To achieve this ambitious goal, we constantly need to expand our synchronized infrastructure. Yet, there was a missing piece to call our network coverage truly worldwide: Africa.\nToday, we’re very happy to add Johannesburg, South Africa as our newest location and first data centre in Africa, where your data could be automatically replicated. We’re now covering 6 continents!\n\n\nNORTH AMERICA:  United States (East, West, Central), Canada\nSOUTH AMERICA: Brazil \nEUROPE:  France, Germany, Russia\nASIA:  Hong Kong, India, Japan, Singapore\nOCEANIA: Australia\nAFRICA:  South Africa\n\n\nThis new addition is critical for our worldwide infrastructure network:\nPrior to now nearly all Algolia traffic delivered to Africa was served from our French and Indian data centers. It now allows us to better serve our African customers by reducing network latency in this region thus ensuring an optimal find-as-you-type quality of service for their end users.  This new location will also reinforces the high availability of Algolia’s search infrastructure by offering an extra backup location for the neighboring regions.\nMore importantly, Africa is quickly changing. Following a long period of strong growth and rapid urbanization, the continent is going digital at full speed. In 2013, Africa only had 16% Internet penetration and is now expected to reach 50% in the next ten years. Moreover, there are expected to be 360 million smartphones on the continent that will generate $75 billion in e-commerce revenue by 2025 (source: McKinsey). \nThat's a tremendous opportunity for us to contribute to bridge the digital gap in Africa by bringing the Instant Search revolution to the new companies, products and services that… will flourish there in the near future.\nIf you're already using Algolia and want to migrate to the Africa region, simply drop us a line at support@algolia.com or on the live chat.\nIf you're not using Algolia yet, give us a try  (14-day trial, no credit card required) and select Johannesburg as your location."
  },
  {
    "id": "3794-0",
    "title": "Algolia's Fury road to a worldwide API",
    "author": "Julien Lemoine",
    "text": "The most frequent questions we answer for developers and devops are about our architecture and how we achieve such high availability.\nSome of them are very skeptical about high availability with bare-metal servers, while others are skeptical about how we distribute data worldwide. However, the question I actually prefer is “how it is possible to build such an infrastructure for a startup”. It is true that our current architecture is impressive for a young company:\n\nwe host our high-end dedicated machines in 14 regions worldwide, 26 data-centers\nour master-master setup replicate our search engine on at least 3 different machines\nwe process more than 6 billion queries per month\nwe receive more than 20 billion write operations per month\n\nBut this infrastructure was not built in one day. I started this series of posts to share with you the 15 steps that were instrumental for us when building this infrastructure. I’ll be fully transparent and include the outages, the bugs, and how we have used them to improve our architecture.\n \n  Algolia's Fury Road To A Worldwide API Part 1 - on Highscability.com\nIn the first post, we focused on the three first steps of the service in the early days, when it was still in beta, from March 2013 to August 2013.\n \n Algolia's Fury Road To A Worldwide API Part 2 - on Highscability.com\nThe second post focused on the first 18 months of service from September 2013 to December 2014, including our first outages!\n \nAlgolia's Fury Road To A Worldwide API Part 3 - on Highscability.com\nFinally the third post, described how we transformed our \"startup\" architecture into something new while meeting the expectation of big public companies.\n "
  },
  {
    "id": "3865-0",
    "title": "Algolia’s find-as-you-type now available on Magento Connect",
    "author": "jeremy",
    "text": "With the new Algolia’s Magento extension, available on Magento connect, anyone can now implement, in only a few clicks, the most seamless search experience on a Magento store. Yep: Instant-Search on a Magento store, only a few clicks away 🙂 We wanted to share with you the story behind this release and what drove us to adapt this experience for the Magento community. Happy reading!\nUPDATE: For users looking for a Magento 2 extension, you can read more about our latest release.\n200,000 Magento stores who share 1 goal: connect visitors with the right products, as quickly as possible.\nOur commitment to our users is to allow them to offer the ultimate search experience inside their stores. In eCommerce, ultimate search is all about finding the right product in no time. Shopping online should be as seamless as think, find and buy. This natural “find-as-you-type” flow that our API allows has been quite well received by the eCommerce community since the early days of Algolia. This caught our attention at the time and so a little over a year ago we started to investigate this community.\nWe quickly found out that the vast majority of online stores actually use platforms such as Magento, Shopify or Prestashop. Magento was the best candidate for our research, because of the size of its community but also of the diversity and the scale of the stores.\nPlatforms are awesome at putting together all the basic services you need to run your store but when you start looking at specific features you always find some room for improvement. Some of these features, such as search, have a dramatic impact on the bottom line of the store, and for these, good enough can not be enough.\nIt happens that the Community Edition of Magento offers as default search simple SQL queries that seem to be fine at first sight but actually do not handle basic features such as partial matches, autocomplete or fuzzy search. Our initial gut feeling was that people probably weren’t happy with this.\nSo… we went on to meet with hundreds of members of the Magento community, from owners to developers to agencies and system integrators.\nWhat struck us is that literally all the people we talked to complained about the relevance and the speed of the default search of the Community Edition. They also wanted to have much more control over the way they could tweak the relevance of their search engine.\nWhat drove store owners crazy is that a user who searches is 6 times more likely to buy than a visitor who simply browses and they were very much aware that this poor search capability directly impacted their bottom line.\nWe believe that great search should be a commodity in eCommerce, because of its impact on the bottom line. But the reality is very different for hundreds of thousands of stores. So we decided to change this!\nTo all of you who helped us in this journey, thank you! We hope what we put together will match your expectations!\nAlgolia Instant Search: an open-source extension for Magento stores that connect visitors with what they’re looking for, in a couple of keystrokes!\nWith the Algolia extension, your visitors can now navigate across your store and find what they’re looking for in a much more intuitive way, as they are typing their query. That means two things:\n\n\nAn Instant-search results page\n\n\nFor each new keystroke that the visitor types, the entire results page and filters are updated. This allows to narrow down the results extremely quickly and focus the attention of the visitors only on what matters to them. Algolia also allows to combine filtering AND text search, allowing for instance to search on products only inside a specific category.\n\nA multi-categories auto-complete\nAt each keystroke, the autocomplete menu refines the products, pages, categories, brands and popular searches results. This allows an accelerated and much more targeted access to specific sections of your catalogue.\n\nAnd because one size does not fit all, the extension’s… dashboard gives you all the flexibility you need to tweak the search engine and make it fit perfectly with your business and your users.\nAll this can be integrated right now on your Magento instance, in just a few clicks. If you want to see how it works before jumping in, just watch this step-by-step tutorial video on how to install and configure the extension.\n\nAnd for the developers who are still reading, don’t forget that the extension is open-source; so feel free to fork the project on GitHub!\nJoin the Algolia + Magento Community!\nThe story is just starting and we need your support to take the online shoppers experience to where it should be. What brought us here is you guys, your feedback and your insights. And that is exactly what will also bring all of us to the next frontier of the shopper experience! If you are a Magento store owner, a developer, a system integrator, or just someone interested in our Magento extension, we want to hear from you!\nPing us at magento@algolia.com to become member of the Algolia + Magento community: you will get early access to trainings, as well as newsletters focused on our latest Magento related news and updates.\nAnd one last thing…\nThere are tons of great solutions on Magento connect that can help you run your store and deliver the best shopping experience to your visitors. We’ve put together a small hack to help you find your favorite Magento extensions, powered by Algolia!\nFor that one I‘d just say: check it out… and feel free to share it.\n\nEnjoy!"
  },
  {
    "id": "3830-0",
    "title": "Add instant search to your blog or documentation using our Jekyll plugin",
    "author": "Tim Carry",
    "text": "Our goal at Algolia is to provide the fastest and the most relevant search experience everywhere around the world. In order to reach this scale, making the life of developers easier had to be one of our founding values. That's why you can find clients for our API for many languages and frameworks.\nToday we are proud to announce the new born in our family of integrations: the Jekyll plugin.\nWhat is Jekyll\nJekyll is a static website generator. You write your content as markdown files, create a few layout templates and it generates the whole HTML website for you. You can then easily deploy it on any web server as it does not need any backend language or database to operate.\nJekyll itself is written in Ruby and is mostly used for blogs or documentation websites. GitHub actually provides a feature called GitHub pages that will automatically build and host a Jekyll website for you if you push it to a branch named .\nHow to use the plugin\nAs a Jekyll website is only static HTML files, it also means that you cannot have any search capabilities. Or at least you couldn't until now. With Algolia's plugin, you now have access to the  command. It will scan your Jekyll website and push every paragraph of text as a new record to an Algolia index so that you can search it from your website.\nYou can see it in action here.\n \nThis fully tested plugin is available from rubygems and you can easily install it by adding just one line to your . Once installed, you only need to edit your  to add your credentials. All information regarding installation and configuration are available in the readme.\n\nHow it works\nThe plugin looks at the HTML files generated by the  command and extracts every  paragraphs of text from them. It then adds a bit of metadata context to each of them and pushes them to your index.\nBy reading the final HTML pages that are generated, we do not rely on any specific markdown parser and you can even use any custom plugin you'd like.\nFor each extracted paragraph, we index… its raw HTML value as well as its sanitized text. We also include the page url, the full hierarchy of headings ( to ) where the paragraph was found and a few other informations (like a unique CSS selector for the  as well as its closest parent heading).\nAll this information will let you display nice search results and even point the user to the exact matched  in the results page. By default, results are grouped by urls, meaning that only the most relevant paragraph of each page will be returned, but this can be changed from the settings (as we'll see in the next section).\nNote that by itself, the plugin only imports your data to your index. For rendering the results, you can follow our tutorials or use our forked version of the popular Hyde theme.\nAdvanced usage\nThe default settings are perfectly tuned for a blog, but if you have a different kind of content, you might want to tweak the plugin configuration to fit your needs.\nFortunately, the plugin is highly configurable. First of all, if you want to index more than just paragraphs (maybe you would like headings and block quotes as well), you can define your own CSS selectors.\nNext, all the Algolia index settings are overridable directly from the  file, allowing you to group results based on another attribute, add snippeting to your fields or define your own custom ranking rules.\nFinally, we also provide custom hooks that let you write custom code to add your own attributes to your records or add/remove records right before pushing them.\nOur goal is to give you as much freedom as possible when using this plugin. We can't wait to see what you're going to build with it."
  },
  {
    "id": "3791-0",
    "title": "Welcome Texas!",
    "author": "gaetan",
    "text": "You probably already know it: any millisecond that end-users have to wait to get their results drives us nuts. But what on Earth does this have to do with Texas? Actually a lot!\nYou want your search to be instant? Let’s talk network...\nWhen looking at the speed of search on a website or a mobile application, the performance of the search engine is just one part of the equation. When you’re using an extremely fast engine, network latency and saturated links quickly become your biggest enemies: it simply takes time for the user query to reach the engine and for the results to get back to the user’s browser. In some cases, the round trip can easily take more than a second. In the US, it can take up to 300ms to simply establish an SSL connection between the two coasts. All this also applies to the communications between your backend and the servers that host your search engine. The network can simply ruin the real time experience you hoped to offer with your search engine.\nA new US Central point of presence to reach a 25ms total delivery time across the US\nA great search experience is to drive end-users towards what they’re looking as quickly and seamlessly as possible. For us at Algolia it means to be able to dynamically update the content displayed as the end-user is typing a query. Being able to offer this find as-you-type experience obviously requires a very performant search engine but it also requires to host the search engine itself as close as possible to the end-user in order to tackle the network latency.\nThis is why we are adding this new US Central region to our existing twelve regions. With the addition of the Dallas PoP, Algolia’s API is now accessible from thirteen different regions including US (East, West and Central), Australia, Brazil, Canada, France, Germany, Hong Kong, India, Japan, Russia, and Singapore.\nIf your audience is spread out across multiple regions, you can use Algolia from a combination of these regions to ensure minimal… results delivery time and optimal speed for all your users (Algolia’s Distributed Search Network automatically routes user queries to your closest region). This new US Central PoP, combined with Algolia’s US East and US West PoPs, now allows to deliver search results across the US with less than 25 milliseconds of latency. This guarantees a seamless find-as-you-type experience on websites and mobile applications all across the US.\n \n\nGetting closer to additional infrastructure providers\nWhen you choose SaaS providers, especially when their service becomes a core component of your product, you probably prefer the ones hosted close to where you operate your backend, for latency and availability reasons. This is actually why we initially started in the US by opening PoPs in Ashburn (VA) and San Jose (CA), close to the AWS PoPs, which most of our customers rely on today.\nOur new presence in Texas allows services which rely for their backend on local infrastructure providers such as Rackspace and Softlayer to also benefit from the full power of Algolia. This new PoP offers them an extremely low network latency between their backend and our API.\nIf you’re not already an Algolia user and you want to give it a try, simply sign up for a 14 day trial and select the US Central region in the process.\nIf you are already using Algolia and want to migrate to the US Central region, simply drop us a line at support@algolia.com or on the live chat.\nIf you’re none of the two above, we still think you’re awesome!\nCheers!"
  },
  {
    "id": "3735-0",
    "title": "When Solid State Drives are not that solid",
    "author": "Adam Surak",
    "text": "It looked just like another page in the middle of the night. One of the servers of our search API stopped processing the indexing jobs for an unknown reason. Since we build systems in Algolia for high availability and resiliency, nothing bad was happening. The new API calls were correctly redirected to the rest of the healthy machines in the cluster and the only impact on the service was one woken-up engineer. It was time to find out what was going on.\nSUMMARY:\n1) the issue raised by Algolia is due to a Linux kernel error\n2) Linux kernel error can affect any SSD under the same operating conditions\n3) Samsung has also posted a Linux kernel patch that should fix the issue\nUPDATE June 16:\nA lot of discussions started pointing out that the issue is related to the newly introduced queued TRIM. This is not correct. The TRIM on our drives is un-queued and the issue we have found is not related to the latest changes in the Linux Kernel to disable this feature.\n# smartctl -l gplog,0x13 /dev/sda\nsmartctl 6.2 2013-07-26 r3841 [x86_64-linux-3.16.0-31-generic] (local build)\nCopyright (C) 2002-13, Bruce Allen, Christian Franke, www.smartmontools.org\nGeneral Purpose Log 0x13 does not exist (override with '-T permissive' option)\nUPDATE June 17:\nWe got contacted by Samsung and we provided them all the system specifications and all the information about the issue we had. We will continue to provide Samsung all the necessary information in order to resolve the issue.\nUPDATE June 18:\nWe just had a conference call with the European branch and the Korean HQ of Samsung. Their engineers are going to visit one of the datacenters we have servers in and in cooperation with our server provider they will inspect the mentioned SSDs in our SW and HW setup.\nUPDATE June 19:\nOn Monday June 22, the engineering team from Samsung is going analyze one of our servers in Singapore and if nothing is found on-site, the server will travel to Samsung HQ in Korea for further analysis.\nUPDATE July 13:\nSince… the last update of this blog-post, we have been in a cooperation with Samsung trying to help them find the issue, during this investigation we agreed with Samsung not to communicate until their approval.\nAs the issue was not reproduced on our server in Singapore, the reproduction is now running under Samsung supervision in Korea, out of our environment. Although Samsung requested multiple times an access to our software and corrupted data, we could not provide it to them in order to protect the privacy and data of our customers.\nSamsung asked us to inform you about this:\n\nSamsung tried to duplicate the failure with the latest script provided to them, but no single failure has been reproduced so far.\n\n\nSamsung will do further tests, most likely from week 29 onwards, with a much more intensive script provided by Algolia.\n\nAfter unsuccessful tries to reproduce the issue with Bash scripts we have decided to help them by creating a small C++ program that simulates the writing style and pattern of our application (no files are open with O_DIRECT). We believe that if the issue is coming from a specific way we are using the standard kernel calls, it might take a couple of days and terabytes of data to be written to the drive. We have been informed by Samsung that no issue of this kind have been reported to them. Our server provider has modified their Ubuntu 14.04 images to disable the fstrim cron in order to avoid this issue. For the last couple of months after not using trim anymore we have not seen the issue again.\nUPDATE July 17:\nWe have just finished a conference call with Samsung considering the failure analysis of this issue. Samsung engineering team has been able to successfully reproduce the issue with our latest provided binary.\nSamsung had a concrete conclusion that the issue is not related to Samsung SSD or Algolia software but is related to the Linux kernel.\nSamsung has developed a kernel patch to resolve this issue and the official statement with details will… be released tomorrow, July 18 on Linux community with the Linux patch guide. Our testing code is available on GitHub.\nThis has been an amazing ride, thank you everyone for joining, we have arrived at the destination.\nFor all followers of this blogpost and all the new readers:\nThe discovered issue has much bigger impact than we originally expected and is not caused by Samsung SSDs, as we originally assumed.\nMy personal apologies to Samsung!\n\n \nThe NGINX daemon serving all the HTTP(S) communication of our API was up and ready to serve the search queries but the indexing process crashed. Since the indexing process is guarded by supervise, crashing in a loop would have been understandable but a complete crash was not. As it turned out the filesystem was in a read-only mode. All right, let's assume it was a cosmic ray 🙂 the filesystem got fixed, files were restored from another healthy server and everything looked fine again.\nThe next day another server ended with filesystem in read-only, two hours after another one and then next hour another one. Something was going on. After restoring the filesystem and the files, it was time for serious analysis since this was not a one time thing. At this point, we did a breakdown of the software involved in our storage stack and went through the recent changes.\nInvestigation & debugging time!\nWe first asked ourselves if it could be related to our software. Are we using non-safe system calls or processing the data in an unsafe way? Did we incorrectly read and write the files in the memory before flushing it to disk?\n\nFilesystem - Is there a bug in ext4? Can we access the memory space of allocation tables by accident?\nMdraid - Is there a bug in mdadm? Did we use an improper configuration?\nDriver - Does the driver have a bug?\nSSD - Is the SSD dying? Or even worse, is there a problem with the firmware of the drive?\n\nWe even started to bet where the problem was and exactly proposed, in this order, the possible solutions going from… easy to super-hard.\nGoing through storage procedures of our software stack allowed us to set up traps and in case the problem happens again, we would be able to better isolate the corrupted parts. Looking at every single storage call of our engine gave us enough confidence that the problem was not coming from the way in which we manipulate the data. Unfortunately.\nOne hour later, another server was corrupted. This time we took it out of the cluster and started to inspect it bit by bit. Before we fixed the filesystem, we noticed that some pieces of our files were missing (zeroed) - file modification date was unchanged, size was unchanged, just some parts were filled with zeros. Small files were completely erased. This was weird, so we started to think if it was possible that our application could access certain portions of the memory where the OS/filesystem had something mapped because otherwise our application cannot modify a file without the filesystem noticing. Having our software written in C++ brought a lot of crazy ideas of what happened. This turned out to be a dead-end as all of these memory blocks were out of our reach.\nSo is there an issue in the ext4? Going through the kernel changelog looking for ext4 related issues was a terrifying experience. In almost every version we found a fixed bug that could theoretically impact us. I have to admit, I slept better before reading the changelog.\nWe had kernels 3.2, 3.10, 3.13 and 3.16 distributed between the most often corrupted machines and waited to see which of the mines blows up. All of them did. Another dead-end. Maybe there was an issue in ext4 that no one else has seen before? The chance that we were this “lucky” was quite low and we did not want to end up in a situation like that. The possibility of a bug in ext4 was still open but highly improbable.\nWhat if there was an issue in mdadm? Looking at the changelog gave us confidence that we should not go down this path.\nThe level of despair was reaching a… critical level and the pages in the middle of the night were unstoppable. We spent a big portion of two weeks just isolating machines as quickly as possible and restoring them as quickly as possible. The one thing we did was to implement a check in our software that looked for empty blocks in the index files, even when they were not used, and alerted us in advance.\nNot a single day without corruptions\nWhile more and more machines were dying, we had managed to automate the restore procedure to a level we were comfortable with. At every failure, we tried to look at different patterns of the corruption in hopes that we would find the smallest common denominator. They all had the same characteristics. But one thing started to be more and more clear - we saw the issue only on a portion of our servers. The software stack was identical but the hardware was slightly different. Mainly the SSDs were different but they were all from the same manufacturer. This was very alarming and led us to contact our server provider to ask if they have ever seen something like this before. It’s hard to convince a technical support person about a problem that you see only once in a while, with the latest firmware and that you cannot reproduce on demand. We were not very successful but at least we had one small victory on our side.\nKnowing that the issue existed somewhere in the combination of the software and drive itself, we reproduced the identical software stack from our servers with different drives. And? Nothing, the corruption appeared again. So it was quite safe to assume the problem was not in the software stack and was more drive related. But what causes a block to change the content without the rest of the system noticing? That would be a lot of rotten bits in a sequence...\nThe days started to become a routine - long shower, breakfast, restoring corrupted servers, lunch, restoring corrupted servers, dinner, restoring corrupted servers. Until one long morning shower full of… thinking, “how big was the sequence?” As it turned out, the lost data was always 512 bytes, which is one block on the drive. One step further, a block ends up to be full of zeroes. A hardware bug? Or is the block zeroed? What can zero the block? TRIM! Trim instructs the SSD drive to zero the empty blocks. But these block were not empty and other types of SSDs were not impacted. We gave it a try and disabled TRIM across all of our servers. It would explain everything!\nThe next day not a single server was corrupted, two days silence, then a week. The nightmare was over! At least we thought so… a month after we isolated the problem, a server restarted and came up with corrupted data but only from the small files - including certificates. Even improper shutdown cannot cause this.\nPoking around in the source code of the kernel looking for the trim related code, we came to the trim blacklist. This blacklist configures a specific behavior for certain SSD drives and identifies the drives based on the regexp of the model name. Our working SSDs were explicitly allowed full operation of the TRIM but some of the SSDs of our affected manufacturer were limited. Our affected drives did not match any pattern so they were implicitly allowed full operation.\nThe complete picture\nAt this moment we finally got a complete picture of what was going on. The system was issuing a TRIM to erase empty blocks, the command got misinterpreted by the drive and the controller erased blocks it was not supposed to. Therefore our files ended-up with 512 bytes of zeroes, files smaller than 512 bytes were completely zeroed. When we were lucky enough, the misbehaving TRIM hit the super-block of the filesystem and caused a corruption. After disabling the TRIM, the live big files were no longer corrupted but the small files that were once mapped to the memory and never changed since then had two states - correct content in the memory and corrupted one on the drive. Running a check on the files… found nothing because they were never fetched again from the drive and just silently read from the memory. Massive reboot of servers came into play to restore the data consistency but after many weeks of hunting a ghost we came to the end.\nAs a result, we informed our server provider about the affected SSDs and they informed the manufacturer. Our new deployments were switched to different SSD drives and we don't recommend anyone to use any SSD that is anyhow mentioned in a bad way by the Linux kernel. Also be careful, even when you don't enable the TRIM explicitly, at least since Ubuntu 14.04 the explicit FSTRIM runs in a cron once per week on all partitions - the freeze of your storage for a couple of seconds will be your smallest problem.\nTL;DR\nBroken SSDs: (Drives on which we have detected the issue)\n\nSAMSUNG MZ7WD480HCGM-00003\nSAMSUNG MZ7GE480HMHP-00003\nSAMSUNG MZ7GE240HMGR-00003\nSamsung SSD 840 PRO Series\nrecently blacklisted for 8-series blacklist\nSamsung SSD 850 PRO 512GB\nrecently blacklisted as 850 Pro and later in 8-series blacklist\n\nWorking SSDs: (Drives on which we have NOT detected the issue)\n\nIntel S3500\nIntel S3700\nIntel S3710"
  },
  {
    "id": "3717-0",
    "title": "We just raised our Series A. What's next?",
    "author": "Nicolas Dessaigne",
    "text": "You may have heard last week in the press, Algolia has just raised an $18.3M Series A round of financing led by Accel Partners! Philippe Botteri from Accel is joining our board and we can't wait to benefit from his experience! We are also excited to welcome Lead Edge Capital and to have received the trust of industry pioneers such as Ilya Sukhar of Parse, Solomon Hykes of Docker, Erik Swan of Splunk, and Kevin Rose of Digg.\nThis funding represents a major milestone for Algolia. Thanks to the commitment of our customers our growth last year enabled us to demonstrate a strong product market fit. We are proud to count many of you as our customers who have seen in our offer a way to deliver a better search experience, improving their end-users' engagement.\nWe want to change the way people interact with information. We don't want people to \"search\" in the traditional type-keyword/hit-enter/wait-for-results/repeat-until-found-or-abandon way; we want them to intuitively access data. We strongly believe that search should become a frontend and UX priority. That's why we focus so much on the two must-haves for building a seamless and interactive experience: speed which enables updating results as-you-type, and relevance which ensures that results are good even after only a couple of keystrokes.\nIt's time for us to accelerate on that vision. With the help of this new funding, we are going to continue investing in our core product, and in making it available to an ever-expanding community with many new integrations. Search is everywhere and you can count on us to come up with new creative ways to delight your users with an outstanding experience. Stay tuned!\nWe will also double down on customer success, which has been so important to our growth. Please make us accountable and let us know if there is anything we can improve.\nWe have embarked on a journey to change the face of user-facing search, everywhere. Join us, it's going to be fun!\nPS: We're hiring!"
  },
  {
    "id": "3699-0",
    "title": "DNS fallback for better resilience",
    "author": "Adam Surak",
    "text": "At Algolia, we are obsessed with finding a way to have a 99.9999% available architecture. On our way to achieve that, we have to make sure every piece of the architecture can safely fail without affecting the service.\nThe first point of the architecture where a customer's request starts to interact with our service is not the router in the datacenter, but a DNS resolving a domain name to the IP address \"long time\" before that. This piece of architecture is very often overlooked and that is no surprise as you mostly get best-effort DNS service automatically with your server.\nLatency\nFor couple months we are a happy user of NSONE that provides us with the first level of logic. We use NSONE for its superb performance and data-driven DNS that gives us control in steering the traffic of our Distributed Search Network to the proper server - whether it means closest or simply available one. But as any other network dependent service, there are factors outside of NSONE's control that can influence availability of its DNS resolves and consequently Algolia. BGP routing is still a huge magic and \"optimizations\" of some ISPs are beyond understanding. Well, they do not always make the optimizations in the direction we would like to. For some services the change of DNS resolution time from 10 to 500ms does not mean a lot but for us it is a deal breaker.\nResolution of latency-1 via NSONE\nDDoS\nWhen we started to think about our DNS dependency, we remembered the 2014 DDoS attack on UltraDNS and the situation when there was not enough #hugops for all the services impacted. During the previous attack on UltraDNS in 2009 even big names like Amazon and SalesForce got impacted.\nSolution\nIn most of the cases it would mean adding another DNS name server from a different provider and replicate the records. But not in ours. NSONE has some unique features that we would have to give up and find a common feature subset with a different provider. In the end we would have to serve a portion of… DNS resolutions via slower provider for no good reason.\nSince we provide custom made API clients we have one more place where to put additional logic. Now came a time to choose a resilient provider for our secondary DNS and since we like AWS, Route53 was a clear choice. Route53 has ok performance, many POPs around the world and API we already had integration for.\nIn the last moment, one more paranoid idea came to us - let's not rely on a single TLD. No good reason for that, it was just \"what if...?\" moment.\nResolution of latency-1 via Route53\nRight now, all the latest versions of our API clients (detailed list below) use multiple domain names. \"algolia.net\" is served by NSONE and provides all the speed and intelligence, \"algolianet.com\" is served by Route53 in case that for any reason contacting server via \"algolia.net\" fails. It brings more work to our side, brings more cost on our side but it also brings better sleep for our customers, their customers and us.\nAnd now we can think what else can fail...\nMinimal versions of API clients with support of multiple DNS:\n\nJavascript v2: 2.9.6\nJavascript v3: 3.1.0\nNode.js: 1.8.0\nRuby: 1.4.1\nRuby on rails: Ruby dependency 1.4.1\nPython: 1.5.2\nPHP: 1.5.5\nJava: 1.3.5\nAndroid: 1.6.3\nObjective-C: 3.4.1\nC-Sharp: 3.1.0\nGo: 1.2.0"
  },
  {
    "id": "3643-0",
    "title": "Modern JavaScript libraries: the isomorphic way",
    "author": "Vincent Voyer",
    "text": "Algolia's DNA is really about performance. We want our search engine to answer relevant results as fast as possible.\nTo achieve the best end-to-end performance we've decided to go with JavaScript since the total beginning of Algolia. Our end-users search using our REST API directly from their browser - with JavaScript - without going through the websites' backends.\nOur JavaScript & Node.js API clients were implemented 2 years ago and were now lacking of all modern best practices:\n\nnot following the error-first or callback-last conventions;\ninconsistent API between the Node.js and the browser implementations;\nno Promise support;\nNode.js module named algolia-search, browser module named algoliasearch;\ncannot use the same module in Node.js or the browser (obviously);\nbrowser module could not be used with browserify or webpack. It was exporting multiple properties directly in the window object.\n\nThis blog post is a summary of the three main challenges we faced while modernizing our JavaScript client.\ntl;dr;\nNow the good news: we have a new isomorphic JavaScript API client.\nIsomorphic JavaScript apps are JavaScript applications that can run both client-side and server-side.\nThe backend and frontend share the same code.\nisomorphic.net\nHere are the main features of this new API client:\n\nworks in Node.js 0.10, 0.12, iojs and from Internet Explorer 8 up to modern browsers;\nhas a Promise + callback API;\nis available at npmjs.com/algoliasearch and on cdn.jsdelivr.net;\nhas builds for jQuery, AngularJS and Parse.com;\nis compatible with all module loaders like browserify or webpack;\nis fully tested in all supported environments.\n\nIf you were using our previous libraries, we have migration guides for both Node.js and the browser.\n\nChallenge #1: testing\nBefore being able to merge the Node.js and browser module, we had to remember how the current code is working. An easy way to understand what a code is doing is to read the tests. Unfortunately, in the previous version of the… library, we had only one test. One test was not enough to rewrite our library. Let's go testing!\nUnit? Integration?\nWhen no tests are written on a library of ~1500+ LOC, what are the tests you should write first?\nUnit testing would be too close to the implementation. As we are going to rewrite a lot of code later on, we better not go too far on this road right now.\nHere's the flow of our JavaScript library when doing a search:\n\ninitialize the library with \ncall \nbrowser issue an HTTP request\n\n\nFrom a testing point of view, this can be summarized as:\n\ninput: method call\noutput: HTTP request\n\nIntegration testing for a JavaScript library doing HTTP calls is interesting but does not scale well.\nIndeed, having to reach Algolia servers in each test would introduce a shared testing state amongst developers and continuous integration. It would also have a slow TDD feedback because of heavy network usage.\nOur strategy for testing our JavaScript API client was to mock (do not run away right now) the XMLHttpRequest object. This allowed us to test our module as a black box, providing a good base for a complete rewrite later on.\nThis is not unit testing nor integration testing, but in between. We also planned in the coming weeks on doing a separate full integration testing suite that will go from the browser to our servers.\nfaux-jax to the rescue\nTwo serious candidates showed up to help in testing HTTP request based libraries\n\npgte/nock \nand Sinon.js fake XMLHttpRequest.\n\nUnfortunately, none of them met all our requirements. Not to mention, the AlgoliaSearch JavaScript client had a really smart failover request strategy:\n\nuse XMLHttpRequest for browsers supporting CORS,\nor use XDomainRequest for IE < 10,\nor use JSONP in situations where none of the preceding is available.\n\nThis seems complex but we really want to be available and compatible with every browser environment.\n\nNock works by mocking calls to the Node.js http module, but we directly use the XMLHttpRequest… object.\nSinon.js was doing a good job but was lacking some XDomainRequest feature detections. Also it was really tied to Sinon.js.\n\nAs a result, we created algolia/faux-jax. It is now pretty stable and can mock XMLHttpRequest, XDomainRequest and even http module from Node.js. It means faux-jax is an isomorphic HTTP mock testing tool. It was not designed to be isomorphic. It was easy to add the Node.js support thanks to moll/node-mitm.\nTesting stack\nThe testing stack is composed of:\n\nsubstack/tape, isomorphic testing and assertion framework\ndefunctzombie/zuul, local and continuous integration test runner\nalgolia/faux-jax, isomorphic HTTP mocking library\n\nThe fun part is done, now onto the tedious one: writing tests.\nSpliting tests cases\nWe divided our tests in two categories:\n\nsimple test cases: check that an API command will generate the corresponding HTTP call\nadvanced tests: timeouts, keep-alive, JSONP, request strategy, DNS fallback, ..\n\nSimple test cases\nSimple test cases were written as table driven tests:\nIt's a simple JavaScript file, exporting test cases as an array\nCreating a testing stack that understands theses test-cases was some work. But the reward was worth it: the TDD feedback loop is great. Adding a new feature is easy: fire editor, add test, implement annnnnd done.\nAdvanced tests\nComplex test cases like JSONP fallback, timeouts and errors, were handled in separate, more advanced tests:\nHere we test that we are using JSONP when XHR fails\nTesting workflow\nTo be able to run our tests we chose defunctzombie/zuul.\nLocal development\nFor local development, we have an npm test task that will:\n\nlaunch the browser tests using phantomjs,\nrun the Node.js tests,\nlint using eslint.\n\nYou can see the task in the package.json. Once run it looks like this:\n640 passing assertions and counting!\nBut phantomjs is no real browser so it should not be the only answer to \"Is my module working in browsers?\". To solve this, we have an npm run dev task that will expose our… tests in a simple web server accessible by any browser:\nAll of theses features are provided by defunctzombie/zuul\nFinally, if you have virtual machines, you can test in any browser you want, all locally:\nHere's a VirtualBox setup created with xdissent/ievms\nWhat comes next after setting up a good local development workflow? Continuous integration setup!\nContinuous integration\ndefunctzombie/zuul supports running tests using Saucelabs browsers. Saucelabs provides browsers as a service (manual testing or Selenium automation). It also has a nice OSS plan called Opensauce. We patched our .zuul.yml configuration file to specify what browsers we want to test. You can find all the details in zuul's wiki.\nNow there's only one missing piece: Travis CI. Travis runs our tests in all browsers defined in our .zuul.yml file. Our travis.yml looks like this:\nAll platforms are tested using a travis build matrix\nRight now tests are taking a bit too long so we will soon split them between desktop and mobile.\nWe also want to to tests on pull requests using only latest stable versions of all browsers. So that it does not takes too long. As a reward, we get a nice badge to display in our Github readme:\nGray color means the test is currently running\nChallenge #2: redesign and rewrite\nOnce we had a usable testing stack, we started our rewrite, the V3 milestone on Github.\nInitialization\nWe dropped the new AlgoliaSearch() usage in favor of just algoliasearch(). It allows us to hide implementation details to our API users.\nBefore:\nnew AlgoliaSearch(applicationID, apiKey, opts);\nAfter:\nalgoliasearch(applicationID, apiKey, opts);\nCallback convention\nOur JavaScript client now follows the error-first and callback-last conventions. We had to break some methods to do so.\nBefore:\nclient.method(param, callback, param, param);\nAfter:\nclient.method(params, param, param, params, callback);\nThis allows our callback lovers to use libraries like caolan/async very easily.\nPromises and callbacks… support\nPromises are a great way to handle the asynchronous flow of your application.\nPromise partisan? Callback connoisseur? My API now lets you switch between the two! http://t.co/uPhej2yAwF (thanks @NickColley!)\n— pouchdb (@pouchdb) March 10, 2015\n\nWe implemented both promises and callbacks, it was nearly a no-brainer. In every command, if you do not provide a callback, you get a Promise. We use native promises in compatible environments and jakearchibald/es6-promise as a polyfill.\nAlgoliaSearchHelper removal\nThe main library was also previously exporting window.AlgoliaSearchHelper to ease the development of awesome search UIs. We externalized this project and it now has now has a new home at algolia/algoliasearch-helper-js.\nUMD\nUMD: JavaScript modules that run anywhere\nThe previous version was directly exporting multiple properties in the window object. As we wanted our new library to be easily compatible with a broad range of module loaders, we made it UMD compatible. It means our library can be used:\n\nwith a simple , it will export algoliasearch in the window object\nusing browserify, webpack, requirejs: any module loader\nin Node.js\n\nThis was achieved by writing our code in a CommonJS style and then use the standalone build feature of browserify.\nsee browserify usage\nMultiple builds\nOur JavaScript client isn't only one build, we have multiple builds:\n\nvanilla JavaScript using native xhrs and Promises\njQuery build using jQuery.ajax and returns jQuery promises\nAngularJS build is using $http service and returns AngularJS promises\nParse.com build is using parse cloud http and promises\nNode.js, iojs are using the http module and native Promises\n\nPreviously this was all handled in the main JavaScript file, leading to unsafe code like this:\n\nHow do we solve this? Using inheritance! JavaScript prototypal inheritance is the new code smell in 2015. For us it was a good way to share most of the code between our builds. As a result every entry point of our builds are… inheriting from the src/AlgoliaSearch.js.\nEvery build then need to define how to:\n\ndo http request through AlgoliaSearch.prototype._request\nreturn promises with AlgoliaSearch.prototype._promise\nuse a request fallback where needed with AlgoliaSearch.prototype._request.fallback\n\nUsing a simple inheritance pattern we were able to solve a great challenge.\nExample of the vanilla JavaScript build\nFinally, we have a build script that will generate all the needed files for each environment.\nChallenge #3: backward compatibility\nWe could not completely modernize our JavaScript clients while keeping a full backward compatibility between versions. We had to break some of the previous usages to level up our JavaScript stack.\nBut we also wanted to provide a good experience for our previous users when they wanted to upgrade:\n\nwe re-exported previous constructors like window.AlgoliaSearch*. But we now throw if it's used\nwe wrote a clear migration guide for our existing Node.js and JavaScript users\nwe used npm deprecate on our previous Node.js module to inform our current user base that we moved to a new client\nwe created legacy branches so that we can continue to push critical updates to previous versions when needed\n\nMake it isomorphic!\nOur final step was to make our JavaScript client work in both Node.js and the browser.\nHaving separated the builds implementation helped us a lot, because the Node.js build is a regular build only using the http module from Node.js.\nThen we only had to tell module loaders to load index.js on the server and src/browser/.. in browsers.\nThis last step was done by configuring browserify in our package.json:\nthe browser field from browserify also works in webpack\nIf you are using the algoliasearch module with browserify or webpack, you will get our browser implementation automatically.\nThe faux-jax library is released under MIT like all our open source projects. Any feedback or improvement idea are welcome, we are dedicated to make our JS client your… best friend 🙂"
  },
  {
    "id": "3622-0",
    "title": "Quadrant.io solves the frustration of economic data search with Algolia",
    "author": "Marie-Auxille Denis",
    "text": "Browsing the cumbersome interfaces of government websites in the lookout for reliable data can be a very frustrating experience. It's full of specific terminology and there's not a government website that looks the same. It's like each time you want to use a car, you have to learn to drive all over again.\nConnect ideas with economic insight in a matter of seconds\nThat's what Quadrant.io is for. Solving the frustration anyone who makes their points with facts encounters when routinely performing data search. It offers them with the fastest and easiest way to find and chart economic data from trusted sources. Acknowledging that it can quickly become a nightmare to find reliable information scattered all over the web, Quadrant is on a mission to shorten any data search to seconds. So that data users can spend less time finding data and more time analysing it.\nTo keep this promise, Quadrant provides data users with an intuitive platform that aggregates more than 400,000 indicators from over 1,000 public sources, and keep them updated in real time. A powerful search allowing any user to find exactly what they are looking for even if they do not use economists’ jargon is a must-have functionality in such a service.\nAnd Algolia stood out as the perfect search solution for Quadrant.\nProvide a rewarding search experience to End-Users\nFirst because of the rewarding search experience it allows to deliver to its users.\nAlgolia surfaces data relevant to people's search in milliseconds, showing the most appropriate results from the very first keystroke.\n\nIt enables to search across different entry points corresponding to the different attributes describing data series (release date, source).\n\nThat wasn’t possible with other search solutions they tested before. After implementing Algolia, Quadrant.io received nice feedback from their customers, saying that \"search was much more comfortable, much more intuitive\".\nAlgolia empowers anyone to be a search expert\nSecond,… because of the simple experience it is to deploy Algolia on their web app. Back-end documentation and customer support was a major help: it took them less than a week to implement instant search, including relevance tweaking and front-end development. As Dane Vrabrac, co-founder of Quadrant.io concluded “with Algolia, it's awesome all the stuff I can do as a non developer !”\nImages courtesy of Quadrant.io. Learn more on their website."
  },
  {
    "id": "3041-0",
    "title": "Don’t let network latency ruin the search experience of your international users",
    "author": "gaetan",
    "text": "At Algolia, we allow developers to provide a unique interactive search experience with as-you-type search, instant faceting, mobile geo-search and on the fly spell check.\nOur Distributed Search Network aims at removing the impact of network latency on the speed of search, allowing our customers to offer this instant experience to all their end-users, wherever they may be.\nEvery millisecond matters\nWe are obsessed with speed and we’re not the only ones: Amazon found out that 100ms in added latency cost them 1% in sales. The lack of responsiveness for a search engine can really be damaging for one’s business. As individuals, we are all spoiled when it comes to our search expectations: Google has conditioned the whole planet to expect instant results from anywhere around the world.\n\nWe have the exact same expectations with any online service we use. The thing is that for anyone who is not Google, it is just impossible to meet these expectations because of the network latency due to the physical distance between the service backend that hosts the search engine and the location of the end-user.\nEven with the fastest search engine in the world, it can still take hundreds of milliseconds for a search result to reach Sydney from San Francisco. And this is without counting the bandwidth limitations of a saturated oversea fiber!\nHow we beat the speed of light\nAlgolia’s Distributed Search Network (DSN) removes the latency from the speed equation by replicating your indices to different regions around the world, where your users are.\nYour local search engines are clones synchronized across the world. DSN allows you to distribute your search among 12 locations including the US, Australia, Brazil, Canada, France, Germany, Hong Kong, India, Japan, Russia, and Singapore. Thanks to our 12 data centers, your search engine can now deliver search results under 50ms in the world's top markets, ensuring an optimal experience for all your users.\n\nHow you activate DSN\nToday, DSN is… only accessible to our Starter, Growth, Pro and Enterprise plan customers. To activate it, you simply need to go in the “Region” tab at the top of your Algolia dashboard and select “Setup DSN”.\n\nYou will then be displayed with a map and a selection of your top countries in terms of search traffic. Just select our DSN data centers on the map and see how performance in those countries is optimized.\nAlgolia will then automatically take care of the distribution and the synchronization of your indices around the world. End-users’ queries will be automatically routed to the closest data center among those you’ve selected, ensuring the best possible experience. Algolia DSN delivers an ultra low response time and automatic fail-over on another region if a region is down.\n\nIt is that simple!\nToday, several services including HackerNews, TeeSpring, Product Hunt and Zendesk are leveraging Algolia DSN to provide faster search to their global users.\nWant to find out more about the Algolia experience ?\nDiscover and try it here!"
  },
  {
    "id": "3003-0",
    "title": "New experimental version of Hacker News Search built with Algolia",
    "author": "kevin",
    "text": "Exactly a year ago, we began to power the Hacker News search engine (see our blog post). Since then, our HN search project has grown a lot, expanding from 20M to 25M indexed items, and serving from 900K to 30M searches a month.\nIn addition to hn.algolia.com we're also providing the HN Search API: a lot of you have used it to build various readers or monitor tools and we love the applications you're building on top of us. The community was also pretty active on GitHub, requesting improvements and catching bugs... keep on contributing!\nEating our own dog food on HN search\nWe are power users of Hacker News and there isn't a single day we don't use it. Being able to use our own engine on a tool that is so important to us has been a unique opportunity to eat our own dog food. We've added a lot of API features during the year but unfortunately didn't have the time to refresh the UI so far.\nOne of our 2015 resolutions was to push the envelope of the HN search UI/UX:\n\nmake it more readable,\nmore usable,\nand use modern frontend frameworks.\n\nThat's what motivated us to release a new experimental version of HN Search. Try it out and tell us what you think!\nApplying more UI best practices\nWe've learned a lot of things from the comments of the users of the previous version. We also took a look at all the cool apps built on top of our API. We wanted to apply more UI best practices and here is what we ended with:\n\n\nFocus on instantaneity\nThe whole layout has been designed to provide an instant experience, reducing the wait time before the actual content is displayed. It's also a way to reduce the number of mouse clicks needed to access and navigate through the content. The danger with that kind of structure can be to end up with a flickering UI where each user action redraw the page, activating unwanted behaviors and consuming a huge amount of memory.We focused on a smooth experience. Some of the techniques used are based… on basic performance optimizations but in the end what really matters for us is the user's perception of latency between each interactions, more than objective performance. Here are some of the tricks we applied:\n\nToggle comments: we wanted the user to be able to read all the comments of a story on the same page, our API on top of Firebase allowed us to load and display them with a single call.\nSticky posts: in some cases we are loading up to 500 comments, we wanted the user to be able to keep the information of what he is reading and easily collapse it, so we decided to keep the initial post on top of the list.\nLazy-loading of non-cached images: when you are refreshing the UI for each request you don't want every thumbnail to flick on the UI when loading. So we applied a simple fade to avoid that. But there is actually no way to know if an image is already loaded or not from a previous query. We manage to detect that with a small timeout.\nLoading feedback: the most important part of a reactive UI is to always give the user a feedback on the state of the UI. We choose to add this information with a thin loading bar on top of the page.\nDeferring the load of some unnecessary elements: this one is about performance. When you are displaying about 20 repeatable items on each keypress you want them as light as possible. In our case we are using Angular.js with some directives which were too slow to render. So we ended up rendering them only if the user interact with them.\nCache every requests: It's mainly about the backspace key. When a user want to modify his query by removing some characters you don't want to make him wait for the result: that's cached by the Algolia JS API client.\n\nFocus on readability\nWe've learned a lot from your comments while releasing our first HN Search version last year. Readability of the search results must be outstanding to allow you to quickly understand why the results are retrieved and what they are about. We ended up with 2… gray colors and 2 font weights to ease the readability without being too distracting.\n\nStay as minimal as possible\nIf you see unnecessary stuffs, please tell us. We are not looking for the most 'minimal' UI but for the right balance between usability and minimalism.\nSorting & Filtering improvements\nMost HN Search users are advanced users. They know exactly what they are searching for and want to have the ability to sort and filter their results precisely. We are now exposing a simple way to either sort results by date or popularity in addition to the period filtering capabilities we already had.\nInlined comments\nWe thought it could make a lot of sense to be able to read the comments of a story directly from the search result page. Keeping in mind it should be super readable, we went for indentations & author colored avatars making it really clear to understand who is replying.\n\nSearch settings\nBecause HN Search users are advanced users, they want to be able to customize the way the default ranking is working. So be it, we've just exposed a subset of the underlying settings we're using for the search to let you customize it.\n\nFront page\nSince Firebase is providing the official API of Hacker News, fetching the items currently displayed on the front page is really easy. We decided to pair it with our search, allowing users to search for hot stories & comments through a discreet menu item.\n\nStarred\nLet's go further; what about being able to star some stories to be able to search in them later? You're now able to star any stories directly from the results page. The stars are stored locally in your browser for now. Let us know if you find the feature valuable!\n\nContribution\nAs you may know, the whole source code of the HN Search website is open-source and hosted on GitHub. This new version is still based on a Rails 4 project and uses Angular.js as the frontend framework. We've improved the README to help you being able to contribute in minutes. Not to mention: we love… pull-requests.\nNow is starting again the most important part of this project, user testing. We count on you to bring us the necessary information to make this search your favorite one.\nWanna test?\nTo try it, go to our experimental version of HN Search, go to \"Settings\", and enable the new style:\n\n \n\n \nWant to contribute?\nIt’s open-source and we’ll be happy to get your feedback! Just use GitHub’s issues to report any idea you have in mind. We also love pull-requests 🙂\nSource code: https://github.com/algolia/hn-search\nTry it now!"
  },
  {
    "id": "2973-0",
    "title": "Github Awesome Autocomplete browser extension for Chrome and Firefox",
    "author": "Sylvain Utard",
    "text": "By working every day on building the best search engine, we've become obsessed with our own search experience on the websites and mobile applications we use.\nWe're git addicts and love using GitHub to store every single idea or project we work on. We use it both for our private and public repositories (12 API clients, HN Search or various d e m o s). We use every day its search function and we decided to re-build it the way we thought it should be.  We're proud to share it with the community via this Chrome extension. Our Github Awesome Autocomplete enables a seamless and fast access to GitHub resources via an as-you-type search functionality.\n\nInstall your Christmas Gift now!\n\nFeatures\nThe Chrome extension replaces GitHub's search bar and add autocomplete capabilities on:\n\n\ntop public repositories\n\n\n\n\nlast active users\n\n\n\n\nyour own private repositories (this one is done locally in JavaScript without Algolia: the list of private repositories remains locally in your browser)\n\n\n\n\nHow does it work?\nWe continuously retrieve the most watched repositories and the last active users using GitHub Archive dataset. Users and repositories are stored in 2 Algolia indices: users and repositories. The queries are performed using our JavaScript API client and the autocomplete menu is based on Twitter's typeahead.js library.\n\nThe underlying Algolia account is replicated in 6 regions using our DSN feature, answering every query in 50-100ms wherever you are (network latency included!). Regions include US West, US East, Europe, Singapore, Australia & India.\n\nExporting the records from GitHub Archive\nWe used GitHub's Archive dataset to export top repositories and last active users using Google's BigQuery:\n\n \nConfiguring Algolia indices\nHere are the 2 index configurations we used to build the search:\nRepositories\n\n\n\nUsers\n\n\n\n\nWant to contribute?\nIt's open-source and we'll be happy to get your feedback! Just use GitHub's issues to report any… idea you have in mind. We also love pull-requests 🙂\n\nSource code: https://github.com/algolia/github-awesome-autocomplete\n\nInstall it now: Github Awesome Autocomplete on Google Chrome Store [FREE]\n\n\nOr just want to add an instant search in your website / application?\nFeel free to create a 14-days FREE trial at http://www.algolia.com and follow one of our step by step tutorials at https://www.algolia.com/doc/tutorials"
  },
  {
    "id": "2789-0",
    "title": "FanFootage: Solving the Search problem with Algolia",
    "author": "Marie-Auxille Denis",
    "text": "The following post is a guest post by Eoin O'Driscoll (web developer), and Vinny Glennon (co-founder) of FanFootage.com\n\n\nWhen we founded FanFootage we knew there was something lacking in the concert and event experience. Fans were taking videos on their mobile phones during performances, loading them on YouTube, and the audio was horrible. So we created FanFootage where fans can upload their own video and we work with the bands to get the high def audio and put them together. Now fans can come to our site and see their favorite concerts or sports from any angle. They can also search for upcoming events and performances.\nFor a recent Linkin Park concert the band and the fan group contacted us. Their fans uploaded more than 1,500 videos from almost every angle. On FanFootage that single concert has had over 350,000 page views.\n\nOur user experience heavily relies on search\nEarlier in our design we knew that search would be key to making FanFootage the ultimate fan experience. When a user comes to our site the first thing they do is search for the event or artist. And we need to make sure that they either find the artist they are looking for or something similar, and it has to be fast.\nAs developers, our team isn’t new to search, particularly within the entertainment space. Our previous startup in the music space was bought by RealNetworks and a second startup was a competitor to Google. That is where we learned that search is hard. And when we thought to build our own search on FanFootage we quickly said it wasn’t going to happen.\nWe also know what fans need. User demands have changed now that they can access anything from their phones. Today we expect our applications and services to predict what we are going to do next. And because of Google, people don’t search with a single phrase. Users expect search to understand how phrases fit together and are related and of course it needs to spell check and it must be instant.\nWe also had different search… requirements than other sites. Normally search on a site is for one unit or concept; a site for flowers for example. For us we needed to allow fans to search for artists, bands, friends or upcoming events in their area and never get a zero result.\nWhy we chose Algolia\nAfter looking at a few search applications we agreed on Algolia.Many search applications look nice but don’t have the flexibility we needed to configure them they way our business needed. And most weren’t fast.\nWhy did we chose Algolia? First it has a developer-centric approach. It took us 2 hours to configure and a day to test and that was it. We basically had search up and running in a day. The dashboard lets me know that the API calls are returned within milliseconds and we have all the flexibility we need to configure as our content grows.\nToday, more than 250 artists have used FanFootage in 20 countries. We are growing quickly. As a company we are still learning what our fans are searching for and Algolia is helping us with that. As content grows we will continue to configure search to meet the needs of our fans. We will also be rolling out Algolia for mobile because of its multi-search capabilities.\nAlgolia is a simple solution to a complex problem. And it blew our mind away. It just works. And now we can focus on our own fanbase.\nImages courtesy of FanFootage. Learn more on their website"
  },
  {
    "id": "2926-0",
    "title": "Search inside websites and mobile apps is strategic to engage visitors - Part 1",
    "author": "Marie-Auxille Denis",
    "text": "In an economic environment where the competition for end-users’ attention and interest is fierce, overlooking search inside your website and mobile application may damage your business.\nPowerful and reliable Web search engines such as Google have created deeply rooted expectations for a responsive and intuitive access to online content and your users expect the same responsive experience once they access your service. Yet most websites and mobile applications still provide a frustrating and cumbersome navigation and exploration experience, supported by a poor internal search engine. Besides, people cannot stand wasting time and the Google guys got it: they’ve made moving between websites effortless. What people don’t find easily with you, Google will find it for them, and it may be with your competitors. Great site search reinforces retention but also brand awareness and customer loyalty.\nEnd-users have high expectations when it comes to search\nGoogle’s mission is to organize the world’s information and make it universally accessible. This is how people use the Web, they hunt for information and content. In 2004 already, according to Nielsen Norman Group (2004), people would start their web sessions with a search engine 88% of the time. This hunt for content does not stop once users access your service. By using extremely fast and intuitive Web search engines such as Google or Yahoo, users have developed well-established unconscious expectations about what great search should be: the invisible link that understands an intent and translates it into in the right answer. Users have been conditioned to rely on such responsive and supportive search interfaces.\nWith the ever growing amount of content online services offer their users, internal search is now more central than ever to keep up with this need for an immediate access to relevant answers. Search has become the most important UX component for information retrieval and exploration inside online… services. But the gap keeps increasing between this need for a powerful internal access to content and the poor navigability of some online services. It has become so important that unconsciously, people would rather trust Google to find content inside your service than your own internal search and navigation engine.\nReturn On Time Invested is the search’s KPI\nPeople see the Web as an “integrated whole” where the fundamental units are pieces of information, not websites, so it is critical for websites and mobile applications to be able to quickly surface relevant information. In such a system, expecting users to navigate complicated information architectures through endless links and tabs is simply not a viable solution.\nUsers optimize their time and efforts in their hunt for information (see the information foraging theory by Pirolli ). They just behave like our ancestors who looked for patches of foods, looking to get the largest benefit with the smallest effort. They exhibit a short attention span, are time-constrained and highly impatient. Thus, they will exercise judgement and pragmatic decision-making strategies in deciding whether to persevere with a given information resource or to look for a different one. The amount of time a user spends on a given website is directly proportional to the travel time between sites and what happens is a phenomenon Jacob Nielsen (2003) describes as information snacking: since information resources are often disappointing and the between-patch time decreases thanks to Google and fast Internet connections, users simply spend less time on a given website and instead multiply their options. All ecommerce websites know that usability guideline: “If users can’t find a product, they won’t buy it”. But with Google and the shrinking travel time between websites, things have changed: “If users can't find it fast, they won’t buy it” would indeed be closer to reality.\nSearch is a key element of your users’… loyalty\nAccording to a Kelton Research study conducted among one thousand American adults  (2007) on \"the state of search\", 78% of those who experience search engine fatigue “wished” that search engines could actually somehow “read their minds”. Visitors need to feel understood and treated fairly when interacting with a service. If you think about it, the search bar of a website or a mobile application is a unique field where the users express their intent the most clearly. This is by far the most valuable touch point between an end-user and an online service as well as a unique opportunity to engage a user in a “digital” conversation. Not surprisingly, returning poor results when a user takes the pain to articulate his intent translates into poor retention: would you engage in a relationship with someone who constantly answers off topic? Probably not and that’s nevertheless what’s happening on the Web today. The disappointment caused by a lack of relevance unfortunately damages your credibility and your brand.\nRelevance is mandatory for retention but personalization is the key to loyalty. And whereas it’s not really possible to offer a browsing interface personalized per user, an efficient search function can provide an experience tailored to the particular needs of end-users. Results of a particular query can be pushed up the search results page according to personal data gathered during a session. Search rankings can also be tweaked on a per profile basis, take into account in real-time the preferences of each user, her friends, etc.\nLet’s wrap up!\nToday we are in a paradoxical situation where most efforts are put on external findability, websites wanting to be immediately accessible from Web search engines. But without a strong focus on the search feature of the website to achieve a great internal findability, all those branding and search engine optimization tactics are in vain. Internal search is about organizing your own information and… making it universally accessible to your own users: what Google did for the Web, you now need to it for yourself!"
  },
  {
    "id": "2920-0",
    "title": "Black Thursday DNS issue",
    "author": "Julien Lemoine",
    "text": "Today we had a severe DNS issue that impacted some of our users during a total of 5 hours. Although most of our customers were not impacted, for some of them search simply went down. This event and its details deserved to be fully disclosed.\nThe context\nUp until recently, we were using Amazon Route 53 for our DNS routing needs. When we started to design our Distributed Search Network (algolia.com/dsn) a few months ago, we quickly realized that our needs were out of Route 53's scope: we needed a custom routing per user and the two options of Route 53 simply didn't work:\n\nLatency-based routing is limited to the 9 regions of AWS and we have 12;\nWith geography-based routing you need to indicate country per country how you want to resolve the IP.\n\nThis is a tedious process for a not even good solution as route 53 does not support EDNS right now.\nSo we started to look for new DNS options. Choosing the best DNS provider is not something you do overnight. It took us months to benchmark several vendors and find the right one: NSOne. The filter chain feature of NSOne was a perfect fit for our use case and the NSOne team was great in understanding our needs and even went the extra mile for us by building a specific module, allowing better performance.\nSomething we also discovered during this benchmark was that the algolia.io domain was not good for performance compared to algolia.net, as there are far more DNS servers in the .net anycast network than in the .io one. The NSOne team offered us a smart solution based on linked domain, so we wouldn't have to maintain two zones ourselves.\nThe migration\nThe goal of the migration was to move from Route 53 to NSOne. For several weeks we have been working on importing the records in NSOne and making sure Route 53 and NSOne were synchronized. Our initial tests revealed some issues but after a few days of continuous updates without any difference between Route 53 and NSOne, we started to be confident about our synchronization and started… the migration of the demos of our website to make them target the new algolia.net domain. We tested the performance and resolution from all NSOne POP (https://nsone.net/technology/network/) to be sure there were no glitches.\nThese first production tests were successful, synchronization was ok, performance and routing were good, so we decided to move the .io domain from Route 53 to NSOne as well.\nThe D-day\nThe big issue when changing the DNS is that it is global and involves caching logics, making rollbacking complex. With users in 45 countries it is almost impossible to find a suitable time for everyone: DNS changes cannot be done gradually. We decided to push the update during the night for the US, at 4am EST.\nWe witnessed a quickly rising number of queries targeting NSOne and it's once we reached about 1,000 DNS queries per second that we started to receive our first complain about failed DNS resolution. This routing issue was not impacting all DNS resolutions but some of them were replying with a NXDOMAIN answer, the equivalent of a DNS \"404 not found\".\n\n;  DiG 9.9.5-4.3-Ubuntu  APPID-1.algolia.io\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER< ;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 4096\n;; QUESTION SECTION:\n;APPID-1.algolia.io. IN A\n;; AUTHORITY SECTION:\nalgolia.io. 45 IN SOA dns1.p03.nsone.net. hostmaster.nsone.net.\n1414873854 43200 7200 1209600 3600\n;; Query time: 24 msec\n;; SERVER: 213.133.100.100#53(213.133.100.100)\n;; WHEN: Thu Nov 27 10:49:33 CET 2014\n;; MSG SIZE rcvd: 115\nAfter double-checking our DNS zone for those specific records, we understood it was a NSOne bug related to our custom routing code. We immediately rollbacked to Route 53.\nThe NSOne support was really quick to react and they found the issue pretty quickly: the issue only concerned some DNS with EDNS support on the algolia.io domain. The algolia.net domain was not impacted, explaining why all the… tests we've done weren't able to detect the issue before.\nUnfortunately, it did not stop here and something very unexpected happened: some customers (even not priorly impacted) started to face issues right after the rollback to Route 53.\nIn order to improve performance, the custom Algolia module developed by NSOne was doing some translation on our records: APPID-1.algolia.io is translated into 1.APPID.algolia.io and then resolved to CNAME for the actual server in the cluster serving that customer. The translation of APPID-1.algolia.io to 1.APPID.algolia.io was done with a TTL of 86400 seconds (1 day). Since these zones did not exist in Route 53 before, it was not possible to resolve there records anymore. What made the situation even worse was the TTL far exceeding the TTL of NS records. Most of the DNS servers flushed their cache for the domain, once the nameservers changed. But the remaining ones kept the record cached.\nTL;DR: Do not forget about IPv6. As if it was not enough, we eventually discovered something else: our custom DNS module was resolving APPID-X.algolia.io to X.APPID.algolia.io only in a case that there were no direct resolutions to an IP address. This translation worked pretty well as we had all the A records set. But some customers started to report weird resolutions. Normally we resolve APPID-1.algolia.io -> 1.APPID.algolia.io -> servername-1.algolia.io -> IP. Which was completely fine until the moment an IPv6 AAAA request came. Since we did not have AAAA records, the custom filter started to resolve: APPID-1.algolia.io -> 1. APPID.algolia.io -> servername-1.algolia.io -> 1.servername.algolia.io -> nothing.\nWe were in a bad situation feared by all engineers, this lonely moment when you really miss a \"purge cache\" feature.\nEventually, as soon as we got confirmation of the fix by NSOne, we changed again the DNS of algolia.io to NSOne and helped our customers to workaround the issue before the cache expiration:\nfor our customers impacted by the… NXDOMAIN issue, a simple migration to the algolia.net domain instead of the algolia.io problem fixed the issue;\nfor those impacted by the Route 53 rollback issue, we created new DNS records for them to avoid work-around DNS caches.\nConclusion: what we learned\nThis is by far the biggest problem we have encountered since the launch of Algolia. Although the first issue was almost impossible to anticipate, we have made mistakes and should have handled a few things differently:\nDNS is a highly critical component and being the first to use an external custom module was not a good idea, even if it improved performance;\nPutting more thought into the rollback part of our deployment would have helped us anticipate the second issue. For a component as critical as a DNS, having a robust rollback process is mandatory, no matter how much work it represents and even though such an event is extremely unlikely to happen.\nWe're very sorry for this disruption. We wanted to share these technical details to shed some light on what happened and what we’ve done in response. Thanks for your patience and support.\nIf you think we missed anything or if you'd like to share your advice on your own best practices, your comments are really welcome."
  },
  {
    "id": "2760-0",
    "title": "Jadopado delivers instasearch for mobile and web powered by Algolia",
    "author": "Marie-Auxille Denis",
    "text": "Algolia Increases Online Search Sessions By 60% and Unique Mobile Searches by 270%\nThe following post is a guest post by Omar Kassim, co-founder of JadoPado.\n Founded in 2010, JadoPado is one of the largest e-commerce sites servicing the GCC, Middle East, North Africa and South Asia.  Its CEO Omar Kassim wanted to bring an Amazon-like experience to the region.  In just 3 years of operations the company now boasts thousands of customers, hundreds of vendors and over $7 million in annual revenues.\nRealizing that search is a key component of their user experience and engagement, Omar and his small team of 15 set off to build new search capabilities that would help users find the products they wanted, lightning fast. In addition, the team was developing a revamped mobile experience and saw that search needed to be spot on for both smartphones and tablets. “I saw search as a competitive tool and as a strategy to get a leg up on our competition.  After seeing Algolia on Hacker News I was absolutely blown away.  After looking at the demos, we threw out what we were doing internally in terms of a small search revamp and I had one of our team get cracking with Algolia right away. As a little startup, it really helped that Algolia’s price points were within reach in terms of not breaking the bank to get things rolling.”\nThe Power of Instant Search\nAfter configuring and testing Algolia for two weeks, JadoPado had the results they were looking for. Branded internally as InstaSearch, JadoPado knew that it would dramatically improve how search functioned on both mobile and the web at JadoPado. “The idea from the outset was to build InstaSearch. I kept ending up at the Algolia demo and thought it would be incredible if we could forget all user interaction aside from typing and just display results right away. Remove what you’ve typed and the results disappear taking you back to where you were. We then spent a bit of time figuring out how to get each result… “page” to have a URL that could be used with external search or shared elsewhere,” explained Omar.\n\nMaking Search Seamless\n“We looked at a number of solutions. One of our biggest intentions was to try to get search to be extremely fast and as slick as possible. Customers should feel like search “just works” and that it is a super easy way to get straight to to whatever they may be looking for. Algolia has allowed us to accomplish that,” Omar explained.  “Moving search from a not really working internal model to a search as a service platform has allowed us to focus on other areas while knowing that search works and that we’ve got an edge over our competition.”\nSupport For Arabic\nWith more than 20 countries to support, the JadoPado team knew that the key to success in the region was to ensure that search be delivered in Arabic as well. Omar explained, “The final bits were figuring out a separate set of indexes for Arabic (as we were about to roll out a standalone Arabic version of JadoPado) and getting the faceting right. This was easy to do with the deep Algolia documentation.” Algolia works with all languages, including Chinese, Japanese, Korean, and Arabic. No specific configuration required, speed and ranking perform exactly the same way.\nBetter Business Through Search\nIn May the team rolled out InstaSearch, Arabic support and a newly revamped mobile experience with search at the center. JadoPado immediately experienced a doubling in conversions and activity that was triple a typical day. Compared to the same 30 day period in 2013, JadoPado saw an increase in site visits through search from 8.2% to 11.3%.\nAdditionally:\n\nSessions with search has jumped 59.96%.\nUnique searches has jumped 46.87%\nAverage search depth has increased by 58.87%.\n\nMastering Mobile Through Search\nThe greater impact of Algolia’s hosted search was JadoPado’s revamped mobile experience. Search is often the first action customers take on a mobile device.  With… instant search, autocorrect and full language support,  improving search and the quality of results can have a significant impact on revenues.  With Algolia implemented as part of JadoPado’s mobile site, the company saw strong results with visits from search increasing from 4.3% to 15% over the same time period and session exits decreasing by 16.57%. A big change. And search increased engagement on all levels:\n\nMobile sessions with search jumped by 233.92%\nTotal unique mobile searches jumped 268.37%\nAverage search depth on mobile devices jumped by 41.05%.\n\nImages courtesy of JadoPado. Learn more on their website."
  },
  {
    "id": "2734-0",
    "title": "AfterShip Leverages Algolia’s Search as a Service to Track 10 Million Packages Around The World",
    "author": "Marie-Auxille Denis",
    "text": "Algolia Speeds Up Search Result Delivery Times From 10 Seconds To 250 Milliseconds.The following post is a guest post by Teddy Chan, Founder and CEO at AfterShip.\n\nAfterShip is an online tracking platform which helps online merchants track their shipment across multiple carriers and notify their customers via email or mobile. Being an online merchant myself, I shipped more than 30,000 packages a month around the world. When customers contacted me to get an update on shipments I realized that I couldn’t track shipments from different carriers and get updates on their status in a single place. So I built Aftership to allow both consumers and online merchants view all their packages on a single platform.\nAfter winning the 2011 Global Startup Battle and 2011 Startup Weekend Hong Kong Aftership opened into beta and quickly helped thousands of online merchants to send out over 1,000,000 notifications to customers.\nOne of the key parts of our service is providing customers around the world with up-to-date information about their packages.\nRight now we have more than 10 million tracking numbers in our database. This causes a few different challenges when it comes to search and we needed technology that would help us continuously index constantly changing information.\nOur first challenge is that we are a small team with only 1 engineer.\nWe are not in the search business, so we needed a solution that would be easy to implement and work well with our existing infrastructure. Algolia’s extensive documentation made it easy to see that our set up and implementation time would be extremely fast and would work with any language and database, so we could get back to our core business.\nAlgolia was super easy, we had it tested, up and running in a week.\nOur second challenge was quickly delivering search results.\nOn Redis, searching for packages was simply impossible. For each query, it would simply lock up until the result was found, so it could run only one search at a time.… Each search with Redis was taking up to 10 seconds. With Algolia we reduced search result delivery times to 250 milliseconds for any customer anywhere in the world. When you think about thousands of merchants who send more than 1 million packages per month, you can see how speed is critical.\nDowntime also is not an option when tracking packages around the globe.\nWe are very strict when adopting new technologies and SaaS technologies can’t slow down our system.\nAlgolia had the highest uptime of the other solutions we looked at. There was no physical downtime.\nOur final challenge was search complexity.\nSometimes you need to know how many shipments are coming from Hong Kong and exactly where they are in transit to and from the U.S.. Shipments going around the globe can change status several times within a single day. With Algolia’s indexing we are able to instantly deliver up-to-date notifications on all 10 million packages, so that customers can not only track their package on its journey, but they can also go to their online merchant’s shop and see a real-time status of their package.\nIn the end, it was Algolia’s customer service that won us over.\nSimilar services and platforms were not responsive. With Algolia we either had the documentation we needed, immediately were able to get advice from an engineer or had our problem solved in less than a day. With such a small team this means a lot. And with the Enterprise package we know that Algolia will grow with us as quickly as our business does.\nWant to find out more about the Algolia experience ?\n Discover and try it here"
  },
  {
    "id": "2709-0",
    "title": "Algolia opened its 4th datacenter in California!",
    "author": "Nicolas Dessaigne",
    "text": "Do you know the 3 most important things in search? Speed, speed, and speed!\nAt Algolia, we work at making access to content and information completely seamless. And that can only be done if search results are returned so fast that they seem instant.\nThat means two things for us: getting server response time under 10ms (checked), and getting the servers close to end-users to lower latency.\nWe are on a quest to make search faster than 100ms from anywhere in the world, and today is an important step. We are thrilled to announce the opening of our 4th datacenter, located in California!\nYou can now choose to be hosted on this datacenter when signing up (multi-datacenter distribution is also available for enterprise users)."
  },
  {
    "id": "2700-0",
    "title": "Concertwith.me's Competitive Edge: A Revamped Search UX with Algolia",
    "author": "kevin",
    "text": "There are a lot of music discovery apps on the market, yet sifting through concert listings is anything but seamless. That’s why Cyprus-based startup Concertwith.me aims to make finding local concerts and festivals as intuitive as possible. Automatically showing upcoming events in your area, the site offers personalized recommendations based on your preferences and your Facebook friends’ favorited music. Covering over 220,000 events globally, the site uses Algolia to offer meaningful results for visitors who are also looking for something different.\nFounder Vit Myshlaev admits that concert sites often share the same pool of information. The differentiator is how that information is presented. “The biggest advantage one can have is user experience,” he explains. “There’s information out there, but do users find it? The reason that people don’t go to cool concerts is that they still don’t know about them!”\nAs an example, he showed me one of the largest live music discovery sites on the web. Searching for an artist required navigating a convoluted maze of links before pulling up irrelevant results. “Users have to type in queries without autocomplete, typo-tolerance, or internationalization. They have to scroll through a long list of answers and click on paginated links. That’s not what people want in 2014,” said Myshlaev.\nTo simplify search and make the results more relevant, Concertwith.me used our API. “We got a lot of user feedback for natural search,” Myshlaev wrote. Now visitors can search for artists and concerts instantly. With large user bases in the United States, Germany, France, Spain, Italy, Russia and Poland, Concertwith.me also benefits from Algolia’s multi-lingual search feature. “We’ve localized our app to many countries. For example, you can search in Russian or for artists that are Russian, and results will still come up,” says Myshlaev.\n\nFor users with a less targeted idea of what they’re looking for,… Concertwith.me implemented structured search via faceting. “We also realized that some visitors don’t know what they want. Algolia search helps them find answers to questions like, Where will my favorite artist perform? How much do tickets cost? Are there any upcoming shows?”\n\nConcertwith.me’s goal is to reduce informational noise so that users can find and discover music as soon as possible. The start up experimented with a number of other search technologies before reading an article about us on Intercom.io, which inspired Myshlaev. “When I saw what Algolia could do, I knew that this was the competitive edge I was looking for.”\nWant to build a search bar with multi-category auto-completion like Concertwith.me? Learn how through our tutorial."
  },
  {
    "id": "2684-0",
    "title": "How Abacus Leverages Algolia for Realtime Expense Reporting",
    "author": "kevin",
    "text": "When one thinks of expense reporting, speed is far from the first descriptor that comes to mind. Companies spend a substantial amount of time tracking expenses, while employees linger in paperwork purgatory, wondering when they will be reimbursed for their work-related charges. That’s why Abacus has made it their mission to simplify expense management so that it occurs in real time. Their creative implementation of Algolia helps make it happen.\nAbacus is a mobile and desktop application that allows small businesses to track and verify expenses on the go. Employees can upload a photo of their receipt on the mobile app, and Abacus takes care of the rest. “For each expense, we have a lot of data. We have the person who expensed it, the amount of the expense, the time, and where it took place. We also have a lot of metadata. For example, if you went out to breakfast, we pull in the name of the restaurant, the address, the URL of the merchant. There’s tags and categories and so on,” explains Ted Power, Co-Founder of Abacus. “And we wanted to make all of that searchable.”\n\nTo make all of that data accessible and interpretable for a financial manager, Abacus turned to our API. “Algolia made it super easy for us to get faceted, advanced search options. If you are the finance person at your company, you can basically say ‘Show me all of the expenses over $50,’ or ‘Show me all the expenses that don’t have a receipt.’ You can look at expenses for one person or one category, like travel. You can even pivot off of 8 of these different things. Algolia makes it super easy to do,” says Power. This accelerates the process of expense verification and approval. “It’s good search. We have tags like ‘car rental’ on auto-complete, for example. That’s all Algolia.”\nPower adds that a “great implementation experience” was especially beneficial for the start up. “It’s the kind of thing that would have taken ages to build from scratch.”… Co-Founder Joshua Halickman chimed in: “Being able to get up and off the ground really quickly was great. In general, I love the speed. Crazy fast. Really nice.”\n\nImages courtesy of Abacus. Learn more on their website."
  },
  {
    "id": "2615-0",
    "title": "Deploying Algolia to Search on more than 2 Million Products",
    "author": "Maxime",
    "text": "The following post is an interview of Vincent Paulin, R&D Manager at A Little Market (recently acquired by Etsy).\nAs a fast growing ecommerce site for handmade goods in France, A Little Market has seen its marketplace grow from a few thousand to over 2 million products in just 5 years. With 90,000 designers and artisans using A Little Market marketplace to buy, sell and collaborate, search quickly became a major part of their ecommerce strategy and user experience.\n\nWhat did you have in place as a search solution?\n“We implemented a Solr based search 5 years ago and had been trying to tweak it to fit our growing needs.  We had selected this system for its flexibility, however, over time, that flexibility translated into constant maintenance, modifications and lower relevance in our search results.\nThen we investigated Elasticsearch. It is complex, yet powerful. As I was diving deeper into Elasticsearch I realized that I could quickly gain an “ok” search experience; however, a powerful search experience would mean investing more time than we had to configure it. Then I did a little math:  learning the platform would take a few weeks, configuring servers - a few days, and configuring and tuning semantic search perfectly - several months.\nThen we found Algolia.  We only had 3 months and knew Algolia would be much easier to implement, so we A/B tested everything to see how it would impact the search experience.\nCan you tell us more about your integration process?\nThe first thing we wanted to get done was to reference all the shops and our best searches to make an autosuggest widget. Building this autosuggest with a basic configuration took us 2 days.\nThen we built an automatic task to aggregate shops and best searches every day and configure Algolia indices. We also took on the task to create the front javascript plugin. With the Algolia documentation and the examples on Github it took us less than 1 hour.\nThe results of this first test were very encouraging.…  With around 500k requests per day, the response time was about 4 milliseconds on average and we saw the conversion rate multiplied by 3 compared to the previous conversion rate with a search bar with “no suggest”. For A Little Mercerie, another marketplace we manage, the improvement was about 4 times greater.\nAfter this first test, we were ready to fully commit to Algolia for our whole search experience. The first step was to create a script to index our entire product database in Algolia. This was easy to do with batch insert in Algolia indices. We selected some attributes of our products such as the title, categories, materials and colors to be indexed. That was a first try. We wanted it to be quick and simple.\nWith the help of the open source demo code we developed a full JS sandbox which can display paginated results with faceting to show the progress to the team.  In less than a week, we had a fully working sandbox and the results were promising.  Our query time averaged less than 20 milliseconds on 2 millions records.  With confidence we started to upgrade the algorithm on Algolia, test it, again and again, adding some attributes to index such as specific events (christmas, valentine’s day), custom tags, etc.\nIn addition, we implemented sorted results. They are really relevant with the new numeric ranking option in settings. At that step we were able to sort results by price, date, etc. You must create a specific index for each specific ranking you need.  We also created a different index for each language (French and Italian) and took this opportunity to do the same across our  other websites, alittlemercerie.com and alittleepicerie.com.\nTo do this we created a custom API which abstracts the use of any kind of search engine for all API clients. We end up losing the real-time search but we need that for now in order to abstract everything and to collect data before sending the results.\nThe next step was to erase the “no results” pages. For… that, we were progressively adding the last words of the query as optional words until we had somes results.We never set as optional all the user queries.  We set at least the first word or the first two words.\nWhen search was ready, we still had plenty of time left to implement it on our clients’ applications. We took more time than was needed to implement Algolia. The speed of iteration with the Algolia API enables us to test everything in a much shorter timeframe.\nHow has Algolia's API helped search on A Little Market?\nWe are now able to answer more than 500/1000 requests per minute and we add 6000 new products every day to the search engine while over 3000 are removed, in real time.\nAfter this integration of the Algolia API, we saw an increase in our conversion rate on search by 10%. This represents tens thousands of euros in turnover per month for us. In a few weeks of work with one engineer, we had replaced our main search engine for a better solution thanks to Algolia.”"
  },
  {
    "id": "2437-0",
    "title": "Keeping Data in your Search Engine Up-to-Date",
    "author": "Julien Lemoine",
    "text": "When we developed the first version of Algolia Search, we put a lot of effort into developing a data update API. It worked like this: You could send us a modified version of your data as soon as the change appeared, even if it concerned only a specific part of a record. For example, this batch of information could be the updated price or number of reviews, and we would only update this specific attribute in your index.\nHowever, this initial plan did not take into account that most of our big customers would not benefit from this API due to their existing infrastructure. If you had not planned to catch all updates in your architecture, or if you were not using a framework like Ruby on Rails, it could be very difficult to even have a notification for any of these updates. The solution in this case was to use a batch update on a regular basis. It was a good method to use if you didn't want to change a single line of code in your existing infrastructure, but the batch update was far from a cure-all.\nThe problem of batch update\nThere are two main ways to perform a batch update on a regular basis:\n\nScan your database and update all objects. This method is good if you have no delete operation, but if some data are removed from your database, you will need to perform an extra check to handle delete, which can be very slow.\nClear the content of the index and import all your objects. With this method, you ensure that your index is well synchronized with your database. However, if you receive queries during the import, you will return partial results.  If interrupted, the whole rescan could break your relevance or your service.\n\nSo the two approaches are somewhat buggy and dangerous.\nAnother approach: build a new index with another name\nSince our API allows the creation of a new index with a different name, you could have made your batch import in a new index. Afterward, you would just need to update your front end to send queries to this new… index.\nSince all indexing jobs are done asynchronously, we first need to check that an indexing job is finished. In order to do that, we return an integer (called TaskID) that allows you to check if an update job is applied. Thus, you just have to use the API to check that the job is indexed.\nBut then a problem arises with mobile applications: You cannot change the index name of an application as easily, since most of the time, it is a constant in the application code. And even for a website, it means that the batch will need to inform your frontend that the index name is different. This can be complex.\nThe elegant solution: move operation\nTo solve these problems, we implemented a command that is well known on file systems: move. You can move your new index on the old one, and this will atomically update the content of the old index with the content of the new one. With this new approach, you can solve all the previous update problems with one simple procedure. Here's how you would update an index called \"MyIndex\":\n\nInitialize an index \"MyIndex.tmp\"\nScan your database and import all your data in \"MyIndex.tmp\"\nMove \"MyIndex.tmp in \"MyIndex\"\n\nYou don't have to do any modification on your backend to catch modifications, nor do you need to change the index name on the frontend. Even better, you don't need to check the indexing status with our TaskID system since the \"move\" operation will simply be queued after all \"adds\". All queries will go to the new index when it is ready.\nThe beauty of the move command\nThis command is so elegant that even customers who had been sending us realtime updates via our updates API have decided to use this batch update on a regular basis. The move command is a good way to ensure that there are no bugs in your update code, nor divergence between your database and Algolia.\nThis operation is supported in our twelve API Clients. We go even further in our Ruby on Rails integration: You need only use the 'reindex' command (introduced… in 1.10.5) to automatically build a new temporary index and move it on top of the existing one.\nThe move command is an example of how we try to simplify the life of developers. If you see any other way we can help you, let us know and we'll do our best to remove your pain!"
  },
  {
    "id": "2366-0",
    "title": "Common Misperceptions about Search as a Service",
    "author": "Julien Lemoine",
    "text": "Since the first SaaS IPO by salesforce.com, the SaaS (Software as a Service) model has boomed in the last decade to become a global market that is worth billions today. It has taken a long way and a lot of evangelisation to get there.\nBefore salesforce.com and the other SaaS pioneers succeeded at making SaaS a standard model, the IT departments were clear: the infrastructure as well as the whole stack had to be behind their walls. Since then, mindsets have shifted with the cloud revolution, and you can now find several softwares such as Box, Jive or Workday used by a lot of Fortune 500 companies and millions of SMBs and startups.\nEverything is now going SaaS, even core product components such as internal search. This new generation of SaaS products is facing the same misperceptions their peers faced years ago. So today, we wanted to dig into the misperceptions about search as a service in general.\nHosting your search is way more complex and expensive than you may think\nSome people prefer to go on-premises as they only pay for the raw resource, especially if they choose to run open source software on it. By doing this, they believe they can skip the margin layer in the price of the SaaS solutions. The problem is that this view highly under-estimates the Total Cost of Ownership (TCO) of the final solution.\nHere are some reasons why hosting your own search engine can get extremely complex & expensive:\nHardware selection\nA search engine has the particularity of being very IO (indexing), RAM (search) and CPU (indexing + search) intensive. If you want to host it yourself, you need to make sure your hardware is well sized for the kind of search you will be handling. We often see companies that run on under-sized EC2 instances to host their search engine are simply unable to add more resource-consuming features (faceting, spellchecking, auto-completion). Selecting the right instance is more difficult than it seems, and you'll need to review your copy if your dataset,… feature list or queries per second (QPS) change. Elasticity is not only about adding more servers, but is also about being able to add end-users features. Each Algolia cluster is backed by 3 high-end bare metal servers with at least the following hardware configuration:\n\nCPU: Intel Xeon (E5-1650v2) 6c/12t 3,5 GHz+/3,9 GHz+\nRAM: 128GB DDR3 ECC 1600MHz\nDisk:  1.2TB  SSD (via 3 or 4 high-durability SSD disks in RAID-0)\n\nThis configuration is key to provide instant and realtime search, answering queries in"
  },
  {
    "id": "2319-0",
    "title": "A New Way to Handle Synonyms in a Search Engine",
    "author": "Julien Lemoine",
    "text": "We recently added the support for Synonyms in Algolia! It has been the most requested feature in Algolia since our launch in September. While it may seem simple, it actually took us some time to implement because we wanted to do it in a different way than classic search engines.\nWhat's wrong with synonyms\nThere are two main problems with how existing search engines handle synonyms. These issues disturb the user experience and could make them think \"this search engine is buggy\".\nTypeahead\nIn most search engines, synonyms are not compatible with typeahead search. For example, if you want tablet  to equal  ipad in a query, the prefix search for t , ta , tab , tabl  & table  will not trigger the expansion on iPad ; Only the tablet query will. Thus, a single new letter in the search bar could totally change the result set, catching users off-guard.\nHighlighting\nHighlighting matched text is a key element of the user experience, especially when the search engine tolerates typos. This is the difference between making users think \"I don't understand this result\" and \"This engine was able to understand my errors\". Synonym expansions are rarely highlighted, which breaks the trust of the users in the search results and can feel like a bug.\nOur implementation\nWe have identified two different use cases for synonyms: equalities and placeholders. The first and most common use case is when you tell the search engine that several words must be considered equal, for example st and street in an address. The second use case, which we call a placeholder, is when you indicate that a specific token can be replaced by a set of possible words and that the token itself is not searchable. For example, the content  street could be matched by the queries 1st street or 2nd street but not the query number street.\nFor the first use case, we have added a support of synonyms that is compatible with prefix search and have implemented two different ways to do… highlighting (controlled by thereplaceSynonymsInHighlight  query parameter):\n\nA mode where the original word that matched via a synonym is highlighted. For example if you have a record that contains black ipad 64GB  and a synonym black equals dark, then the following queries will fully highlight the black word : ipad d , ipad da , ipad dar & ipad dark. The typeahead search is working and the synonym expansion is fully highlighted:  .\nA mode where the original word is replaced by the synonym, and the matched prefix is highlighted. For example ipad d  query will replace black by dark and will highlight the first letter of dark: . This method allows to fully explain the results when the original word can be safely replaced by the matched synonym.\n\nFor the second use case, we have added support for placeholders. You can add a specific token in your records that will be safely replaced by a set of words defined in your configuration. The highlighting mode that replaces the original word by the expansion totally makes sense here. For example if you have  mission street  record with a placeholder  = [ \"1st\", \"2nd\", ....] , then the query 1st missionstreet will replace  by 1st  and will highlight all words: .\nWe believe this is a better way to handle synonyms and we hope you will like it 🙂 We would love to get your feedback and ideas for improvement on this feature! Feel free to contact us at hey(at)algolia.com."
  },
  {
    "id": "2294-0",
    "title": "Why JSONP is still Mandatory",
    "author": "Julien Lemoine",
    "text": "At Algolia, we are convinced that search queries need to be sent directly from the browser (or mobile app) to the search-engine in order to have a realtime search experience. This is why we have developed a search backend that replies within a few milliseconds through an API that handles security when called from the browser.\nCross domain requests\nFor security reasons, the default behavior of a web browser is to block all queries that are going to a domain that is different from the website they are sent from. So when using an external HTTP-based search API, all your queries should be blocked because they are sent to an external domain. There are two methods to call an external API from the browser:\n JSONP\nThe JSONP approach is a workaround that consists of calling an external API  with a DOM &lt;script&gt;  tag. The &lt;script&gt; tag is allowed to load content from any domains without security restrictions. The targeted API needs to expose a HTTP GET endpoint and return Javascript code instead of the regular JSON data. You can use this jQuery code to dynamically call a JSONP URL:\n\nIn order to retrieve the API answer from the newly included JavaScript code, jQuery automatically appends a callback argument to your URL (for example &amp;callback=method12 ) which must be called by the JavaScript code that your API generates.\nThis is what a regular JSON reply would look like: \n\nInstead, the JSONP-compliant API generates:\n\nCross Origin Resource Sharing\nCORS (Cross Origin Resource Sharing) is the proper approach to perform a call to an external domain. If the remote API is CORS-compliant, you can use a regular XMLHttpRequest  JavaScript object to perform the API call. In practice the browser will first perform an HTTP OPTIONS request to the remote API to check which caller domains are allowed and if it is authorized to execute the requested URL.\nFor example here is a CORS request issued by a browser. The most important lines are the last two headers… that specify which permissions are checked. In this case, the method is POST and the three specific HTTP headers that are requested.\n\nThe server reply will be similar to this one:\n\nThis answer indicates that this POST method can be called from any domain (Access-Control-Allow-Origin: * ) and with the requested headers.\nCORS has many advantages. First, it allows access to a real REST API with all HTTP verbs (mainly GET, POST, PUT, DELETE) and it also allows to better handle errors in an API (bad requests, object not found, ...). The major drawback is that it is only supported by modern browsers (Internet Explorer ≥ 10, Firefox ≥ 3.5, Chrome ≥ 3, Safari ≥ 4 &amp; Opera ≥ 12; Internet Explorer 8 &amp; 9 provides partial support via theXDomainRequest  object).\nOur initial conclusion\nBecause of the advantages of CORS in terms of error handling, we started with a CORS implementation of our API. We also added a specific support for Internet Explorer 8 &amp; 9 using the  XDomainRequest  JavaScript object (they do not support XMLHttpRequest). The main difference is that XDomainRequest  does not support HTTP headers so we added another way to specify user credentials in the body of the POST request (it was initially only supported via HTTP headers).\nWe were confident that we were supporting almost all browsers with this implementation, as only very old browsers could cause problems. But we were wrong!\nCORS problems\nThe reality is that CORS still causes problems, even with modern browsers. The biggest problem we have found was with some firewalls/proxies that refuse HTTP OPTIONS queries. We even found software on some computers that were blocking CORS requests, as the Cisco AnyConnect VPN client, which is widely used in the enterprise world. We have found this issue when a TechCrunch employee was not able to operate search on crunchbase.com because the AnyConnect VPN client was installed on his laptop.\nEven in 2014 with a large majority of browsers… supporting CORS, it is not possible to have perfect service quality with a CORS-enabled REST API!\nThe solution\nUsing JSONP is the only solution to ensure great compatibility with old browsers and handle problems with a misconfigured firewall/proxy. However, CORS offers the advantage of proper error-handling, so we do not want to limit ourselves to JSONP.\nIn the latest version of our JavaScript client, we decided to use CORS with a fallback on JSONP. At client initialization time, we check if the browser supports CORS and then perform an OPTIONS query to check that there is no firewall/proxy that blocks CORS requests. If there is any error we fallback on JSONP. All this logic is available in our JavaScript client without any API/code change for our customers.\nHaving CORS support with automatic fallback on JSONP is the best way we have found to ensure great service quality and to support all corner case scenarios. If you see any other way to do it, your feedback is very welcome."
  },
  {
    "id": "2279-0",
    "title": "Inside GrowthHackers.com’s Implementation of Algolia",
    "author": "Maxime",
    "text": "We interviewed Dylan La Com, Growth Product Manager at Qualaroo &amp; GrowthHackers.com, about their Algolia implementation experience.\n\nWhat role did search play at GrowthHackers before the Algolia implementation?\nWhen we launched our community site GrowthHackers.com in October 2013, search was admittedly an afterthought for us. GrowthHackers is a social-voting site where marketers, founders, and product-people can share and discuss growth-related content. At launch, it was unclear what role search would have on the site. GrowthHackers is built on WordPress, and with that comes WordPress’ standard search functionality. What WP search does is append an additional keyword or phrase parameter to its typical post query and load a new page with the results. WP search only indexed the outbound URLs of the articles our members submitted, and this made finding specific content difficult.\nWhy did you want to give search an update on GrowthHackers?\nWe started hearing about our lack of a solid search feature from some of our more active users. One of our members even put together a slide presentation to prove just how useless our search was [check it out here]. At the same time, GrowthHackers was becoming more than just a way to stay up-to-date on the best growth articles, it was becoming the place to get answers: an encyclopedia for growth-related information. Search volume at this time was peaking in the mid-hundreds per week. We needed a search feature that could support this evolving use-case.\nWhy did you choose Algolia?\nWe looked at several search solutions before trying Algolia, including Swiftype, WP Search (plugin), and Srch2. All are great solutions, but ultimately, we went with Algolia because they had the right mix of features: Their integration was simple, the documentation was thorough, and there were plenty of starter templates. I knew it was a good sign when, while looking their GitHub repository, I found they had a demo site built with search that worked… very similar to how we hoped ours would work, complete with real-time results, typo-tolerance, and filters. The Algolia team was incredibly helpful getting us set up and was there each step of the way through the integration process, providing resources and best practices for creating a truly top-notch search experience.\nTell me a little about how the new search works.\nOur primary use of Algolia is to store and index user submitted content, and provide real-time search in our growing database of growth-related articles, questions, videos and slides. The majority of what we index is article titles and URLs--strings which are generally small. Visitors to our site often come with specific growth-related questions and use our search to find answers quickly. For example, someone interested in learning best practices for running Twitter ads could type in “Twitter ad” and within milliseconds see dozens of articles and discussions related to maximizing ROI for Twitter ads. Using Algolia’s admin dashboard, we’re able to set ranking priorities based on the number of votes and comments of each article, and make sure the top results are the most relevant. So, the visitor who searches “Twitter ad” is shown articles with the highest mix of votes and comments. Algolia took the search ranking process and wrapped it in a clean and simple interface that allows anyone, regardless of their experience with search, to easily adjust and manipulate.\nOne of the challenges we faced during the integration process was understanding how to keep our main database synced and up to date with our Algolia index. User submitted content on GrowthHackers changes often as users interact with the content. Each post once submitted may receive upvotes and comments from members in the community. Each post also has a wiki-style summary field that can be edited by community members. Lastly, posts can have several states, including published, pending and trashed. In order to ensure our content on… Algolia mirrored the content in our database, we set up a job queue and a cron process to periodically push updates to our Algolia index. This has been working quite well for us.\nHow has the new search impacted engagement?\nWe released the new search mid-February, and since the release we’ve seen search volume increase 4-5X. Of course there are several factors at play here, including increased traffic volume and better search bar placement, but it is clear that Algolia’s search features have contributed to an impressive increase in search engagement. On average, visitors who utilize search view 2-3X more pages per session and spend 5-6X longer on the site than those who don’t search. Algolia’s analytics dashboard provides us with an incredible glimpse of visitor intent on our site by showing us the queries visitors are searching for, and trend lines to show popularity over time. With this data, we’re able to better understand how our visitors want to use our site, and make better decisions about how to organize the content.\nMoving forward, we’re hoping to implement Algolia’s search filters to provide even better ways to access content on our site. We’re excited to have such a powerful tool in our stack and hope to experiment with new ways to provide search functionality throughout GrowthHackers."
  },
  {
    "id": "2263-0",
    "title": "Dealing with OpenSSL Heartbleed Vulnerability",
    "author": "Julien Lemoine",
    "text": "Yesterday, the OpenSSL project released an update to fix a serious security issue. This vulnerability was disclosed in CVE-2014-0160 and is more widely known as the Heartbleed vulnerability. It allows an attacker to grab the content in memory on a server. Given the widespread use of OpenSSL and the versions affected, this vulnerability affects a large percentage of services on the internet.\nOnce the exploit was revealed, we responded immediately: All Algolia services were secured the same day, by 3pm PDT on Monday, April 7th. The fix was applied on all our API servers and our website. We then generated new SSL certificates with a new private key.\nOur website is also dependent on Amazon Elastic Load Balance, which was affected by this issue and updated later on Tuesday, April 8th. We then changed the website certificate.\nAll Algolia servers are no longer exposed to this vulnerability.\nYour credentials\nWe took the time to analyze the past activity on our servers and did not find any suspicious activity. We are confident that no credentials were leaked. However, given that this exploit existed in the wild for such a long time, it is possible that an attacker could have stolen API keys or passwords without our knowledge. As a result, we recommend that all Algolia users change the passwords on their accounts. We also recommend that you reset your Algolia administration API key, which you can do at the bottom of the \"Credential\" section in your dashboard. Be careful to update it everywhere you use it in your code (once you have patched your SSL library if you too are vulnerable).\nSecurity at Algolia\nThe safety and security of our customer data are our highest priorities. We are continuing to monitor the situation and will respond rapidly to any other potential threats that may be discovered.\nIf you have any questions or concerns, please email us directly at security@algolia.com"
  },
  {
    "id": "2235-0",
    "title": "Introducing Search Analytics: Know Your Users Better",
    "author": "gaetan",
    "text": "This week we have released a much requested feature by our customers: analytics.\nThe importance of analytics to search\nAt Algolia, our goal is to revolutionize the way people search and access content inside the Web and mobile services. Think about Spotify, LinkedIn, Amazon: Everyone wants to find the right songs, people and products in just a couple keystrokes. Our challenge is to provide fast and meaningful access to all of this content via a simple search box. In March, we answered more than 200 million user queries for our customers on every continent.\nProviding the right content through the right search and browsing experience is key. For our customers, understanding their users - what they like, what they want and when they want it -  is just as important, if not more. This is why we came up with this new analytics section, built on top of our API and available on our customers’ online dashboards when they log in to their Algolia account. So what exactly do we track for you?\nWe describe here some of the top features that are now available to all our users.\nMost popular queries\nIn this chart, we show which items were most queried. It would be useful, for example, to a procurement department for anticipating their  most frequently-searched products' inventory needs. And if you monetize your service through advertising, know what people are most interested in is especially valuable.\nA new analytics feature supports the most popular queries.\n \nQueries with no or a few results\nToday, most services are simply clueless when it comes to what is missing in their content base. How do you know that your catalogue of products fits your users’ expectations? Knowing whether or not you provide what your users need is critical for your business.\nAlgolia lets you determine which top queries have few or nonexistent results.\n \nHow does a query evolve over time?\nIs Chanel more popular than Louis Vuitton in the morning or at night? Are bikes more popular in June or… in December? With this new feature, you can now answer such questions for your own content by following the number of times a specific query is typed on an hourly basis.\nExample: Search analytics lets you track the evolution of the query \"louboutin\" over 24 hours.\n \nWhich categories do people search the most?\nWhen users type in a query, they often use categories to refine the results. We let you know which categories were the most frequently used for refinement. We even provide the most used combinations of categories (such as “dress“ + “blue” + “size M”). It should help you understand how your users browse your content and has broader implications if the ergonomics of your app is optimized.\nTrack which combinations of categories people search for the most.\nThese new analytics features are included in our existing plans at no extra cost. The number of days when our analytics tools are available vary based on the plan you choose. We hope you will like it, and we will be more than happy to read your feedback and feature requests!"
  },
  {
    "id": "2230-0",
    "title": "On HipChat's blog: Algolia extends HipChat to customer support",
    "author": "Sylvain Utard",
    "text": "As you may probably know, we're using HipChat to build our live-help chat. If you want to know more, go ahead and read our guest post on HipChat's blog.\nAlgolia uses HipChat to provide live customer service over chat."
  },
  {
    "id": "2120-0",
    "title": "Realtime Search: Security and our Javascript Client",
    "author": "Julien Lemoine",
    "text": "Edit: As suggested on Hacker News, SHA256 is not secure, as it allows a length extension attack. We have replaced it with HMAC-SHA256.\nInstant is in our DNA, so our first priority was to build a search backend that would be able to return relevant realtime search results in a few milliseconds. However, the backend is just one variable in our realtime equation. The response time perceived by the end user is the total lapse of time between their first keystroke and the final display of their results. Thus, with an extremely fast backend, solving this equation comes down to optimising network latency. This is an issue we solve in two steps: \n \n\nFirst, we have datacenters in three different locations, allowing us to answer queries in North America, Europe and Asia in less than 100ms (including search computation).\n\n\nSecond, to keep reducing this perceived latency, queries must be sent directly from the end users' browsers or mobile phones to our servers. To avoid intermediaries like your own servers, we offer a JavaScript client for websites and ObjC/Android/C# clients for mobile apps.\n\nThe security challenge of JavaScript\nUsing this client means that you need to include an API key in your JavaScript (or mobile app) code. The first security issue with this approach is that this key can be easily retrieved by anyone who simply looks at the code of the page. This gives that person the potential to modify the content behind the website/mobile application! To fix this problem, we provide search-only API keys which protect your indexes from unauthorized modifications.\nThis was a first step and we've quickly had to solve two other security issues:\n\nLimiting the ability to crawl your data: you may not want people to get all your data by continuous querying. The simple solution was to limit the number of API calls a user could perform in a given period of time. We implemented this by setting a rate limit per IP address. However, this approach is not acceptable if a… lot of users are behind a global firewall, thus sharing one IP address. This is very likely for our corporate users.\n\n\nSecuring access control:  you may need to restrict the queries of a user to specific content. For example, you may have power users who should get access to more content than \"regular\" users. The easy way to do it is by using filters. The problem here with simple filters in your JavaScript code is that people can figure out how to modify these filters and get access to content they are not be supposed to see.\n\nHow we solve it altogether\nToday, most websites and applications require people to create an account and log in to access a personalized experience (think of CRM applications, Facebook or even Netflix). We decided to use these user IDs to solve these two issues by creating signed API keys. Let's say you have an API key with search only permission and want to apply a filter on two groups of content (public OR power_users_only) for a specific user (id=42):\n\nYou can generate a secured API key in your backend that is defined by a hash (HMAC SHA 256) of three elements:\n\nFor example, if you are using rails, the code in your backend would be:\n\nYou can then initialize your JavaScript code with the secured API key and associated information:\n\nThe user identifier (defined by SetUserToken) is used instead of the IP address for the rate limit and the security filters (defined by SetSecurityTags) are automatically applied to the query.\nIn practice, if a user wants to overstep her rights, she will need to modify her security tags and figure out the new hash. Our backend checks if a query is legit by computing all the possible hashes using all your available API keys for the queried index, as well as the security tags defined in the query and the user identifier (if set).  If there is no match between the hash of the query and the ones we computed, we will return a permission denied (403). Don't worry, reverse-engineering the original API key using… brute-force would require years and thousands of core.\nYou may want to apply security filters without limiting the rate of queries, so if you don't need both of these features, you can use only one.\nWe launched this new feature a few weeks ago and we have received very good feedback so far. Our customers don't need to choose anymore between security and realtime search. If you see any way to improve this approach, we would love to hear your feedback!"
  },
  {
    "id": "2193-0",
    "title": "What Caused Today’s Search Performance Issues In Europe and Why It Will Not Happen Again",
    "author": "Julien Lemoine",
    "text": "During a few hours on March 17th you may have noticed longer response times for some of the queries sent by your users.\n\nAverage latency for one of our European clusters on March 17th\nAs you can see above, our slowest average response time (measured from the user’s browser to our servers and back to the user’s browser) on one of our European clusters peaked at 858ms. On a normal day, this peak is usually no higher than 55ms.\nThis was clearly not a normal behavior for our API, so we investigated.\nHow indexing and search calls share the resource\nEach cluster handles two kinds of calls on our REST API: the ones to build and modify the indexes (Writes) and the ones to answer users’ queries (Search). The resources of each cluster are shared between these two uses. As Write operations are far more expensive than Search calls, we designed our API so that indexing should never use more than 10% of these resources.\nUp until now, we used to set a limitation on the rate of Writes per HTTP connection. There was no such limit for queries (Search); We simply limited Write calls to keep search quality. To avoid reaching the Write rate limit too quickly, we recommended users to Write by batching up to 1GB of operations per call, rather than sending them one by one. (A batch, for example, could be adding 1M products to an index on a single network call.) A loophole in this recommendation was the origin of yesterday’s issues.\nWhat happened yesterday is that on one of our European clusters, one customer pushed so many unbatched indexing calls from different HTTP connections that they massively outnumbered the search calls of the other users on the cluster.\nThis eventually slowed down the average response time for the queries on this cluster, impacting our usual search performance.\nThe Solution\nAs of today, we now set the rate limit of Writes per account and not per HTTP connection. It prevents anyone from using multiple connections to bypass this Write rate limit. This… also implies that customers who want to push a lot of operations in a short time simply need to send their calls in batches.\nHow would you batch your calls? The explanation is in our documentation. See here for an example with our Ruby client: https://github.com/algolia/algoliasearch-client-ruby#batch-writes"
  },
  {
    "id": "2145-0",
    "title": "Algolia Heroku add-on enters general availability",
    "author": "Julien Lemoine",
    "text": "We launched the first beta of our Heroku add-on in October 2013 and are now happy to announce its general availability!\nDuring the beta period we received excellent feedback (and some bug reports!) that helped us improve our integration. We are now fully ready to serve production on both Heroku datacenters. If you were part of our beta program, we will contact you shortly to invite you to migrate to a standard plan.\nYou can directly install it from our Heroku add-on page and as ever, please let us know if you have any feedback!"
  },
  {
    "id": "2088-0",
    "title": "Algolia Now Provides Realtime Search in Asia!",
    "author": "Julien Lemoine",
    "text": "One of the terrific advantages of building a SaaS company is that your clients can be anywhere in the world. We now have customers in more than 15 different countries distributed across South America, Europe, Africa, and, of course, North America. We feel incredibly lucky to have so many international customers trusting us with their search.\nLanguage support is one of the key factors that enabled us to enter these markets. Since the beginning, we wanted to support every language used on the Internet. To back our vision with action, we developed a very good support of Asian languages over time. As an example, we are able to automatically retrieve results in Traditional Chinese when the query is in Simplified Chinese (or vice-versa). You simply need to add objects in Chinese, Japanese or Korean, and we handle the language processing for you.\nDespite the fact that we could process Asian languages well, we didn't plan to open an Asian datacenter so early, mainly because we thought the API as a service market was less mature in Asia than in the US or Europe. But we were surprised when an article on 36kr.com gave us dozen of signups from China. We got more signups from China in the past month than from Canada!\nOne of our core values is the speed of our search engine. To provide a realtime search experience, we want the response times to be lower than 100ms, including the round trip to search servers. In this context a low latency is essential. Up to now we have been able to cover North America and Europe in less than 100ms (search computation included) but our latency with Asia was between 200ms and 300ms.\nThe first step of our on-boarding process is to select the datacenter where your search engine is hosted (we offer multi-datacenter distribution only for enterprise users). Interestingly, we discovered that we had no drop for European & US users but it became significant for others. It was a difficult choice for people outside of these two regions, or even… between the two datacenters. So we also now display the latency from your browser and pre-select the \"closest\" datacenter.\nTo propose better latency and to reduce friction in the on-boarding process, it was clear that we had to add a datacenter in Asia. We chose Singapore for its central location. Unfortunately, the hosting market is very different in Asia. It's much more expensive to rent servers, so we sadly had to add a premium on plan prices when choosing this datacenter.\nWe are very happy to open this new datacenter in Asia with a latency that reaches our quality standard. Now that Algolia provides realtime search in Asia, we are even happier to be able to help multinational websites and apps provide a great search experience to all their users across Europe, North America & Asia in less than 100ms with our multi-datacenter support!*\nMulti-datacenter support is currently only available for Enterprise accounts.\n "
  },
  {
    "id": "2033-0",
    "title": "Introducing Easier Onboarding and Activation with Connectors",
    "author": "Sylvain Utard",
    "text": "Most of our users are technical. They love writing code, and we love providing API clients in the major programming languages to them (we are currently supporting 10 platforms).\nThey are doers. They love prototyping. Just like us, they work for startups which need to move fast, and get things done, keeping in mind that done is better than perfect. It is very important that they don't want to waste time. In this post, I will explain how one would have used our API up to now, and how we introduced SQL and MongoDB connectors for easier onboarding, integration and testing.\nBefore: The first steps with our API\nUp until now, our onboarding process asked you to try the API by uploading your data. We emphasized our documentation, and we made sure our users would not need more than a few minutes to integrate our REST API. Nevertheless, exporting your application's data to a JSON or CSV file is often more complex than it appears, especially when you have millions of rows - and especially because developers are lazy 🙂 No worries, that's totally OK. It is something you may not be willing to do, especially just to try a service, so we decided to try something else.\n\nInitial import\n90% of our users are using a SQL or MongoDB database. Exporting a table or a collection to a JSON file can be easy if you're using a framework, for example Ruby on Rails:\n\n...or more annoying, for example when using PHP without any framework:\n\nAnyway, in both cases it gets harder if you want to export millions of rows without consuming hundreds GB of RAM. So you will need to use our API clients:\n\n\nIncremental updates\nOnce imported, you will need to go further and keep your DB and our indexes up-to-date. You can either:\n\nClear your index and re-import all your records hourly/daily with the previous methods:\n\nnon-intrusive,\nnot real-time,\nnot durable,\nneed to import your data to a temporary index + replace the original one atomically once imported if you want to keep your service running… while re-importing\n\n\n\nOr\n\nPatch your application/website code to replicate every add/delete/update operations to our API:\n\nreal-time,\nconsistent & durable,\na little intrusive to some people, even though it is only a few lines of code (see our documentation)\n\n\n\nAfter: Introducing connectors\nEven if we did recommend you to modify your application code to replicate all add/delete/update operations from your DB to our API, this should not be the only option, especially to test Algolia. Users want to be convinced before modifying anything in their production-ready application/website. This is why we are really proud to release 2 open-source connectors: a non-intrusive and efficient way to synchronize your current SQL or MongoDB database with our servers.\nSQL connector\n Github project: algolia/jdbc-java-connector (MIT license, we love pull-requests :))\nThe connector starts by enumerating the table and push all matching rows to our server. If you store the last modification date of a row in a field, you can use it in order to send all detected updates every 10 seconds. Every 5 minutes, the connector synchronizes your database with the index by adding the new rows and removing the deleted ones.\n\nIf you don't have an updated_at  field, you can use:\n\nThe full list of features is available on Github (remember, we ♥ feature and pull-requests)!\nMongoDB connector\n Github project: algolia/mongo-connector\nThis connector has been forked from 10gen-lab's official connector and is based on MongoDB's operation logs. This means you will need to start your mongod  server specifying a replica set. Basically, you need to start your server with: mongod --replSet REPLICA_SET_IDENTIFIER. Once started, the connector will replicate each addition/deletion/update to our server, sending a batch of operations every 10 seconds.\n\nThe full features list is available on Github (we ♥ feature and pull-requests).\nConclusion: Easier Onboarding, Larger Audience!\nHelping our users to… onboard and try Algolia without writing a single line of code is not only a way to attract more non-technical users; It is also a way to save the time of our technical but overbooked users, allowing them to be convinced without wasting their time before really implementing it.\nThose connectors are open-source and we will continue to improve them based on your feedback. Your feature requests are welcome!"
  },
  {
    "id": "1949-0",
    "title": "Ranking Algorithm Unveiled: How Algolia Makes Search Better",
    "author": "Nicolas Dessaigne",
    "text": "Most search engines rank results based on a unique float value that is hard, if not impossible, to decipher. This is because their ranking algorithm has been designed for document search. They take into account the number of occurrences of query words in matching documents to determine their relevance, usually using a tf-idf based scoring.\nWe designed Algolia with database search as our main use-case. The foremost impact of this design decision is that we don't care about the number of occurrences of query words. Instead of assigning a global float score to each result, our ranking algorithm rates each matching record on several criteria (such as the number of typos or the geo-distance), to which we individually assign a integer value score.\nYou even have the option to assign a custom criterium, allowing you to consider additional factors for your ranking, instead of applying a superficial boost has nothing to do with the ranking and that can seriously muddy your results.\nHere is how it works:\n\nAll the matching records are sorted according to the first criterion.\nIf any records are tied, those records are then sorted according the second criterion.\nIf there are still records that are tied, those are then sorted according to the third criterion\nand so on, until each record in the search results has a distinct position.\n\nA record’s score on each criterium is explicitly listed in the search results (see _rankingInfo  below for the query \"the rains\"), so you can understand why one record ranked higher than another one. We will explain each of these criteria in this article.\n\nSearch-as-you-type\nBefore diving into our \"secret sauce\", you first need to understand that Algolia searches for matching prefixes, not matching whole-words. For example, if you are searching for “Joe B”, we would consider all the following records as matches:\n\nJoe Black\nJoe Benson\nJoe Bolick\n\nPrefix matching is what enables us to return relevant results even when a user has only typed a… single letter. When Google introduced instant search, they claimed that showing results before you finish typing can save 2-5 seconds per search.\nNote: By default, when the query contains multiple terms, Algolia only uses the last term as a prefix. This is because when searching, say, for a person by name, it's quite normal to type their entire first name but not their last (e.g George Cloo). Not so for the reverse (e.g. Geo Clooney). You can override this behavior by setting  queryType=prefixAll .\nRanking algorithm criteria\nBy default, Algolia ranks every matching record by using the following criteria, in the order listed below. The higher up the criterion on the list, the more importance it has on ranking. You can easily change this order if you want, but we have found that this default order is the best one in 90% of the use cases.\n\nTypos\nGeo-location (if applicable)\nProximity\nAttributes\nExact\nCustom\n\nLet's understand each one of these criteria by applying them to an example:\n\n1. Typos\nAre there words that start (that is, are prefixed) with a term typed by the user? And if so, do they match exactly the query?\n\n0 points means there are prefixes that exactly match all the terms in the query.\n1 point means there is a 1-character discrepancy between the matching prefixes and the query terms.\n2 points means there is a 2-character discrepancy, and so on.\n\nExample: for the query “joe black”, here is how each result would rank for typos only (joey is considered as a typo as only the last word of the query is searched as a prefix):\n\n\n\n\n\n\n\n\n\n\nRank\n\n\nRecord\n\n\nScore\n\n\nWhy\n\n\n\n\n1\n\n\nrecord 3\n\n\n0\n\n\njoe black\n\n\n\n\n1\n\n\nrecord 4\n\n\n0\n\n\njoe thompson black birds inc \n\n\n\n\n1\n\n\nrecord 5\n\n\n1\n\n\nthompson, joe_ & blackburn ltd\n\n\n\n\n2\n\n\nrecord 2\n\n\n1\n\n\njo_ t. black\n\n\n\n\n3\n\n\nrecord 1\n\n\n2\n\n\njo_ bla_k\n\n\n\n\nNote: By default, Algolia accepts 1 typo for words having at least 3 characters and 2 typos for words having at least 7 characters (this behavior can be configured… withminWordSizefor1Typo  andminWordSizefor2Typos  query parameters). This means that the query \"ab\" only matches words starting with \"ab\", while the query \"abc\" matches words starting by \"abc\" but also \"aba\", \"abb\", \"aac\", etc.  A typo is defined by an insertion, deletion, or substitution of a single character, or a transposition of two adjacent characters (Damerau–Levenshtein distance). As it is extremely unusual to mistype the first character of a word, a typo on the first character counts for 2 points instead of 1.\n2. Geo-location (if using)\nIs the record found within a certain radius of the specified location? And if so, how far from it? The geoDistance  score is expressed in meters, the shorter the better.\nHowever, you may want to consider results \"100m distant\" and \"102m distant\" equal for ranking consideration. To do so, you can use the aroundPrecision  query parameter. For example, with aroundPrecision=10 , two results up to 10 meters close will be considered equal.\nWe don't use geo-location in our example, but you can find a dedicated guide in our documentation.\n3. Proximity\nFor a query that contains two or more words, how physically near are those words in the matching record?\nAlgolia adds 1 point for each word in between query words, with a maximum of 8 points.\n\n0 points means no proximity: there was only one word in the query.\n1 point means the best possible match: the words are next to each other.\n2 points means there is one word between the matched query words.\nand so on.\n\nWhen words are in different attributes they get automatically the maximum of 8 points per new attribute. So if three query words are in three different attributes, the score is 16. If three words are in two different attributes, the score is 8.\n\nIn our example, we have a 3-way tie between records 1, 3 and 5 ('&' is considered as a separator and is not taken into account). Record 2 has a word in between the matched query words (Jo T. Black), while record 4 matches in… two different attributes:\n\n\n\n\n\n\n\n\n\nRank\n\n\nRecord\n\n\nScore\n\n\n\n\n1\n\n\nrecord 1\n\n\n1\n\n\n\n\n1\n\n\nrecord 3\n\n\n1\n\n\n\n\n1\n\n\nrecord 5\n\n\n1\n\n\n\n\n1\n\n\nrecord 2\n\n\n2\n\n\n\n\n2\n\n\nrecord 4\n\n\n8\n\n\n\n\n4. Attributes\nThis is the order of the attributes (fields) Algolia will follow to search inside a record. Records where there is a match in the 1st listed attribute rank higher (that is, gets fewer points) than records with a match in an attribute that’s lower on the list.\nDepending of the first matching attribute, results will get a score in a specific range:\n\n1st attribute: 0-999 points\n2nd attribute: 1000-1999 points\n3rd attribute: 2000-2999 points\nand so on.\n\nThe exact number of points are determined by the position of the first matching word in the attribute:\n\n1st word in attribute: 0 \n2nd word in attribute: 1\n3rd workd in attribute: 2\n\nIn our example, say we consider the name as more important than the company. We would then use the setting attributesToIndex:[\"name\", \"company\"] to indicate that we want to index, i.e. search in, the attributes \"name\" then \"company\", in this specific order of importance.\n\n\n\n\n\n\n\n\nRank\n\n\nRecord\n\n\nScore\n\n\n\n\n1\n\n\nrecord 1\n\n\n0\n\n\n\n\n1\n\n\nrecord 2\n\n\n0\n\n\n\n\n1\n\n\nrecord 3\n\n\n0\n\n\n\n\n1\n\n\nrecord 4\n\n\n0\n\n\n\n\n2\n\n\nrecord 5\n\n\n1001\n\n\n\n\nLastly, matching text at the beginning of a given attribute will be considered more important than matching text further in this attribute. You can disable this behavior if you add your attribute inside unordered(AttributeName). If we considered the position of the match not relevant for the attribute \"company\", we would use the setting attributesToIndex:[\"name\", \"unordered(company)\"] . In that case the \"attribute score\" of record 5 would be 1000 and not 1001.\n5. Exact\nRecords with words (not just prefixes) that exactly match the query terms rank higher. A record gets 1 point for every word that is exactly matched.\nHere is how our records would rank based on exact-matching alone for the query \"joe… black\":\n\n\n\n\n\n\n\n\n\n\nRank\n\n\nRecord\n\n\nScore\n\n\nWhy\n\n\n\n\n1\n\n\nrecord 3\n\n\n2\n\n\njoe black\n\n\n\n\n1\n\n\nrecord 4\n\n\n2\n\n\njoe tompson black bird inc\n\n\n\n\n2\n\n\nrecord 2\n\n\n1\n\n\njo t. black\n\n\n\n\n3\n\n\nrecord 1\n\n\n0\n\n\n\n\n\n3\n\n\nrecord 5\n\n\n0\n\n\n\n\n\n \n6. Custom / Business metrics\nAt this stage, the previous five criteria have ascertained a record's relevance for a user's search query. Now you can specify additional criteria.\nA common approach is to use one or several business metrics that express the popularity of a record. With other search engines, you have to choose between sorting the results according to their relevance to the user's query, or according to their popularity (number of visits, ratings, sales, etc). You just cannot do both. This means users may get results that are outrageously popular, but completely irrelevant to their search.\nWith Algolia, you can integrate popularity (or anything else, like population, or the last date of update) into the relevance calculation. To us, it is just an additional criterium so it will not outweigh classic relevance criteria. The ranking will just make additional sense.\nIn our example, we may consider people with whom we had many calls more popular than others. For people having the same number of calls, we can just order them by alphabetical order. We would then use the setting:  customRanking:[\"desc(nbCalls)\", \"asc(name)\"]\n\nFor this criterium alone, here's how our example records rank:\n\n\n\n\n\n\n\n\n\nRank\n\n\nRecord\n\n\nScore\n\n\nWhy\n\n\n\n\n1\n\n\nrecord 2\n\n\n4\n\n\nnbCalls=45\n\n\n\n\n2\n\n\nrecord 3\n\n\n3\n\n\nnbCalls=9, \"Joe B\"  "
  },
  {
    "id": "1991-0",
    "title": "Postmortem of today’s 8min indexing downtime",
    "author": "Julien Lemoine",
    "text": "Today (Jan 29) at 9:30pm UTC, our service experienced an 8 minute partial outage during which we have rejected many write operations sent to the indexing API (exactly 2841 calls). We call it “partial” as all search queries have been honored without any problem. For end-users, there was no visible problem.\nTransparency is in our DNA: this outage is visible on our status page (status.algolia.com) but we also wanted to share with you all the details of the outage and more importantly the details of our response.\nThe alert\nThis morning I fixed a rare bug in indexing complex hierarchical objects. This fix successfully passed all the tests after development. We have 6000+ unit tests and asserts, and 200+ non regression tests. So I felt confident when I entered the deploy password in our automatic deployment script.\nA few seconds after, I started to receive a lot of text messages on my cellphone.\nWe developed several embedded probes to detect all kinds of problems and alert us using Twilio and Hipchat APIs. They detect for example:\n\na process that restart\nan unusually long query\na write failure\na low memory warning\na low disk-free warning\netc.\n\nIn case embedded probes can't run, other external probes run once a minute from an independent datacenter (Google App Engine). These also automatically update our status page when a problem impacts the quality of service.\nOur indexing processes were crash looping. I immediately decided to rollback to the previous version.\nThe rollback\nUntil today, our standard rollback process was to revert the commit, launch the recompile and finally deploy. This is long, very long when your know that you have an outage in production. The rollback took about 5 minutes in total out of the 8 minutes.\nHow we will avoid this situation in the future\nEven if the outage was on a relatively small period of time, we still believe it was too long. To make sure this will not happen again:\n\nWe have added a very fast rollback process in the way of a simple… press button like the one we use to deploy. An automatic deploy is nice, but an automatic rollback is actually more critical when needed!\nStarting now, we will deploy new versions of the service on clusters hosting community projects such as Hacker News Search or Twitter handle search, before pushing the update on clusters hosting paying customers. Having real traffic is key to detect some types of errors. Unit-tests & non-regression tests cannot catch everything.\nAnd of course we added non-regression tests for this specific error.\n\nConclusion\nHaving all these probes in our infrastructure was key to detect today’s problem and react quickly. In real conditions, it proved not to be enough. In a few hours we have implemented a much better way to handle this kind of situation. The quality of our service is our top priority. Thank you for your support!"
  },
  {
    "id": "1903-0",
    "title": "Hacker News search: 6.5 million articles and comments at your fingertips",
    "author": "Sylvain Utard",
    "text": "We are Hacker News readers and probably just like you, there is not a day that goes by we don’t use it. It is a little like checking the weather app of the tech world. Long story short, Hacker News is awesome, and we wanted to add our two cents to make it even greater to use.\nIndeed, here is our problem: how do we instantly access the old posts we wish we had saved?\nPowering a new Hacker News search engine\nUp until now we’ve been using hnsearch.com, maintained for years by the great folks at Octopart. I hope we speak on behalf of the HN community here, we are all grateful for the work they put in hnsearch.com and they inspired us to pursue their effort.\nBack in September 2013, we created a “homemade Hacker News crawler” and built a search engine with the data we could get. It was not perfect but somehow, it did the job fine.\nNow part of the Ycombinator W14 batch, we have a direct access to the data and it has allowed us to provide instant search for the entire content of Hacker News, 1.2 million articles, 5.2 million comments as of today. See for yourself right here: hn.algolia.com\nHere is how we did it\n\n\nHacker News API access\n\nYC provides us a private API access to fetch batches of 1000 items (an item being a comment or a post). Every two minutes, we update our database with the latest 1000 items. Last 48,000 items are refreshed every hour to keep the number of votes and comments up to date.\n\n\n\n\n\n\nThumbnails generation\n\nWe use wkhtmltoimage to render the URLs and generate the associated thumbnails. Playing with connection timeouts and JavaScript infinite loops was a pleasure:\n\n\n\n\n\n\nThumbnails storage\n\nThumbnails are resized and stored on a S3 bucket.\n\n\n\n\n\n\nThumbnails distribution\n\nWe configured a CloudFront instance targeting the S3 bucket to distribute thumbnails with low latency and high data transfer speed. We followed Amazon's associated developer guide.\n\n\n\n\n\nIndexing\n\nWe used the “algoliasearch-rails” gem and a standard (Ruby on Rails)… MySQL-backed ActiveRecord setup. Indexing is performed automatically as soon as new items are added to the database, providing a near-realtime experience.\n\n\n\n\n\nConfiguration\n\n\n\n\n\nSearch\n\nQueries are sent directly to our API via the javascript client, the javascript code uses a public API-Key that can only perform queries.\n\n\n\nSeeking feedback from the community\nThere is still room for improvement and we would love to know how you are searching for news on HN. What is important for you? Are you searching by date, by upvote, by comment or by user? All together maybe?\nWe would love to have your feedback! Don't hesitate to checkout the code: We open-sourced it.\nSpecial thanks to the Octopart and YC teams for making this experience possible!\nGive it a try now: hn.algolia.com"
  },
  {
    "id": "1834-0",
    "title": "Search Grader by Algolia: How does your search engine perform?",
    "author": "Maxime",
    "text": "Search is important\nAn effective search engine should be a seamless and natural extension of the user experience. With improved relevance, your users should be able to find what they are looking for in no time.\nUnfortunately, developers often consider search as a second-tier priority. This is a mistake. Every day, consumers use Google, Amazon, and Youtube to find what they want on the web quickly and easily. Users of web applications and eCommerce websites will feel the gap in search experience. As their expectations are not met, your conversion rate will plummet, your bounce rate will skyrocket, and the damage to your brand may be irredeemable.\nSearch is tricky\nThe reason why many web applications and e-commerce websites suffer from bad search is because finding a good solution is not easy. Few current search technologies combine relevancy and business metrics in a way that sorts search results optimally.\nIn most cases, they fail on the following items:\n\nlong response times,\nno handling of mistakes,\nno search field auto-completion,\nunexplainable or even nonexistent results.\n\nTo improve your search experience, you first need to understand which areas are problematic. That’s exactly why we built Search Grader by Algolia.\nIntroducing Search Grader by Algolia\nSearch Grader by Algolia is a tool to help you quickly find out what your search engine may be missing. We divided the search user experience in 3 categories in order to get a maximum score of 100:\n\nUser Experience: 30 points\nSpeed: 20 points\nRelevance: 50 points\n\n\nUser Experience: 30/100\nUser experience is not just design, it's the key of a good user satisfaction. If your users cannot find what they're searching for, they will just leave.\n\nSearchbox visibility (3 pts): It is easier for your users to find something if your search bar is clearly visible!\nDescriptive placeholder (2 pts) : A hint in your search bar is a good way to let your users know what kind of data they can dig into.\nSearchbox… auto-completion (6 pts): Auto-completion guides your users more efficiently towards what they are looking for.\nSuggestions after the first keystroke (5 pts): Delight your users by providing relevant suggestions immediately after the first keystroke.\nFaceting (4 pts): Faceting enables users to browse results by filtering them on specific categories (e.g., author, tags, price).\nHighlight (6 pts): You need to explain why the displayed results are chosen, especially when you tolerate typos or misspelled queries.\nPagination (2 pts): Providing relevant results on the first page is great. But to keep your users engaged, you need to give them an easy way to access other results.\nPicture (2 pts): Sometime images are the fastest way to display information. Users will go through results and find the right hits much faster if you show them images.\n\nSpeed: 20/100\nIf results show up in more than 200ms, you will lose part of your users. Time is money, real-time is gold. Because your location is important to the speed of the search we graded speed 3 times based on the location of the user:\n\nResponse time from US East coast\nResponse time from US West coast\nResponse time from Europe\n\nRelevance: 50/100\nRelevance is when you give your users what they want in the top results. Although it's not very fancy, it's probably the more critical aspect of a good search engine.\n\nTypo-tolerance (10 pts): People make a lot of typos, especially on mobile devices. Tolerating misspelled queries provides a great value to both your users and the products you promote.\nAuto-completion shows results, not queries (10 pts): Suggesting queries is good. Suggesting results directly is a lot better as you spare your users one click and a lot of time.\nRanking uses business metrics (10 pts): Considering customized criteria such as sales numbers or the popularities in the way you rank results makes a key difference. It is THE way to give relevant results with one single keystroke.\nOverall ranking (20 pts):… Search must always return relevant results. We perform multiple queries to detect if your search is performing well.\n\nGet Google, Amazon-like search for your website\nThese criteria were defined by our team of experts with over 30+ years of experience in search.\nWe tested out some of the biggest names in tech:\n\nAs you could expect, Amazon and LinkedIn received an excellent score of 90/100. That's the kind of quality Algolia can help you achieve in your application or e-commerce website, for as low as $19/month.\nNow, how about your search? How is it performing? To find out, use Search Grader by Algolia.\nIf you want to share your ideas with us, please leave your comments!\n "
  },
  {
    "id": "1781-0",
    "title": "Improving Search for Twitter Handles",
    "author": "Sylvain Utard",
    "text": "Hello Twitter,\nI have been using your service for awhile, and I love it!\nAt first, I was skeptical about what you could offer: Broadcasting to all my friends that I was eating a pizza, or taking a walk, is not really my cup of tea. But 3 years ago I figured out what Twitter was really meant for and how it could help me in a totally different way from what I first thought:\n\nsharing interesting articles,\nchecking if /replace by the service provider you want/ is down,\nor catching up on HackerNews.\n\nMore recently, I discovered you had a feature that could help me even more: I can now ask for support by tweeting. Tweeting is often faster and more productive than sending an email. You taught me to include the recipient's Handle in my tweets, and your current Handle auto-completion implementation works pretty well: but what if you could provide a better typo-tolerance and ranking? (I'm NOT speaking about your official OSX/iOS native clients and its totally unusable auto-completion feature... btw, could you explain me why it is different from the one on your website?).\nI have been leading a search-engine development team over the last 5 years and I'm now VP of engineering at Algolia. I am aware that considering my job, I have kind of an \"expert\" point of view about search. But search has become so essential that I am convinced it must be irreproachable. Did you know that 1.7M+ people are currently following @twittersearch: I'm not the only one expecting great things from your search-engine, Twitter 🙂 Here is how I would improve search for Twitter handles:\nFor example, it would be nice if I could find President @barackobama with his last name:\n\nSame for Justin:\n\nTypo-tolerance is now a must-have, especially because we're all using smartphones and tablets:\n\n&nbsp;\nMore and more handles are now prefixed/suffixed by \"official\", which makes finding @OfficialAdele just impossible:\n\n&nbsp;\nFor sure we can improve it, let's code!\nFirst of all Twitter, I need your… Handles database 🙂\n\nI used your Streaming API to crawl about 20M+ accounts in ~2 weeks: it's not blazing fast but I must admit it does the job (and it's free). That's about 5 lines of Ruby with TweetStream, good job guys!\nand Daemonize to create a bin/crawler executable.\n\n\nFor each new tweet you send to me, I store the author (name + screen_name + description + followers_count) and all his/her user mentions.\n\nAnd every minute, I re-index the last-updated accounts with a batch request using algoliasearch-rails, scheduled by Whenever:\n\nThe result order is based on several criteria:\n\nthe number of typos,\nthe matching attributes: the name/handle is more important than the description,\nthe proximity between matched words,\nand the followers count (I also use the \"mentions count\" if my crawler didn't get the followers count yet).\n\nI could have improved the results by using the user's list of followers/following but I was limited by your Rate Limits. Instead, I chose to emphasize your top-users (accounts having 10M+ followers).\nHere is the configuration I used\n\nThe user query is composed by 2 backend queries:\n\nthe first one retrieves all matching top-users (could be replaced by a query targeting your followers/following only)\nthe second one the others.\n\nTry it for yourself, and enjoy relevant and highlighted results after the first keystroke: Twitter Handles Search."
  },
  {
    "id": "1535-0",
    "title": "Full Text Search in your Database: Algolia vs Elasticsearch",
    "author": "Julien Lemoine",
    "text": "Most applications and websites are built on top of a database. It can be a traditional relational database like MySQL or a NoSQL database like MongoDB. The problem is that none of these databases offer a satisfying full text search feature. Although they often have similar features (using LIKE operand in MySQL, using text index in MongoDB), these are poor alternatives, as all developers have experienced.\nAlgolia was built to answer the shortcomings of database full-text search. It is a SaaS API dedicated to solving application and website developers' struggles in providing end users with a fast, reliable, and relevant search feature.\nUntil now, Elasticsearch has been the fall-back solution for developers. Although a beautiful product for big data analysis or document search, it hasn’t been designed for object searches. Algolia has. The purpose of this blog post is to answer a question we’re frequently asked: If Algolia brings a specific answer when Elasticsearch offers a broad set of tools, how do they compare for database search?\nWe decided to put both to the test. Using the IMDB database of 400k actors and 2M movies/TV series, we decided to build and measure the performance of both search services keeping everything else constant. We didn't limit our test to crude keyword search but aimed at building a first-class user experience, returning instant results after each keystroke, factoring popularity in the ranking, and tolerating user mistakes gracefully.\n\nWhat we learned\nIn this first section, we sum up our findings. Technical details about the test are described in the second part of this article.\nRaw performance\nIn the benchmark below, you can see Algolia consistently performing between 12 and 200 times faster than Elasticsearch – for every search query we performed.\n\n\n\nQuery\nAlgolia 1 Shard\nAlgolia 5 Shards\nES 1 Shard\nES 5 Shards\n\n\n\n\ngeo\n< 1ms\n2ms\n101ms\n36ms\n\n\ngeorge clo\n2ms\n3ms\n121ms\n51ms\n\n\nb\n< 1ms\n2ms\n147ms\n60ms\n\n\nbatman\n2ms\n2ms\n94ms\n29ms\n\n\nworld… w\n5ms\n2ms\n134ms\n68ms\n\n\ne\n< 1ms\n2ms\n202ms\n81ms\n\n\nemilia\n< 1ms\n2ms\n102ms\n31ms\n\n\nalexandre b\n18ms\n9ms\n243ms\n109ms\n\n\n\nWe understand however that it’s not all about performance, and here’s why: do we want to give the user a search result in the quickest possible time or do we want to give the user what they are looking for in the quickest possible time? We decided for the latter.\nUser Experience\nThere are a lot of things that should happen “behind the scenes” that result in the user quickly finding what it is they are looking for:\n\nInstant Search. Users have been used to search engines auto-completing queries, instead of suggesting actual results. Here we want to go a step further: index-wide results are presented and updated every time a key is entered, in real time. Type a letter and get the best result immediately.\nBalance Relevance and Popularity. In the IMDB example, if we search for “geor”, then we want all results where the actor’s name is ‘George’ (i.e. relevance) and we want ‘George Clooney’ to be top of that list because he is the most popular (defined by how many times users have visited his page/looked him up on IMDB). While straightforward with Algolia, mixing relevance and popularity is nothing short of impossible in Elasticsearch. Either you sort by relevance or by using a popularity attribute, you cannot mix both.\nHandle Typos Intelligently. Users often mistype when entering search queries. A good user experience would be to find ‘George Clooney’ when searching for ‘gorge cloney’ for example. Algolia provides out-of-the-box typo-tolerance that works on both words and prefixes, and intelligently highlights the results. This allows end-users to understand search results even with typos. Unfortunately, Elasticsearch fuzzy matching does not work out of the box, is complex to customize, and does not provide the ability to highlight prefixes.\n\nPerformance or User Experience? Both.\nBy doing a terrific job on database full-text… search, having intuitive configuration of both relevance and matching and by having robust typo tolerance that works out of the box, Algolia helps the user get what they are looking for in the quickest time possible. On desktop but on mobile in particular, this makes a huge difference on time spent executing a search.\nElasticsearch is a great toolbox that can be used to build both intranet search and big data analytics. However, addressing such diverse use cases leads to a difficult integration. One size doesn’t fit all. Algolia’s focus on object search makes it much better for this purpose: faster integration, better performance, superior user experience. And don’t forget you also need to host your Elasticsearch somewhere.\nUnder the hood\nThat's all good but the most technical of you may want to know more about the actual implementation of this benchmark. Here we go.\nIn order to have a point of reference, we used the same hardware: a Xeon E3 1245v2 (Quad-core 3.4Ghz) with 32GB of RAM and 240GB of SSD (2 SSD in raid 0). We did a test with 1 shard and another with 5 shards, and used Elasticsearch version 0.90.2 which is based on Lucene 4.3.1. Note that the number of shards is not exposed in the Algolia API since it is automatically determined in the back-end for scalability and is not triggered for small datasets.\nData Set\nAt the end of July 2013, we extracted from IMDB a list of 400k actors/actresses and 2M movies/TV series. In order to factor popularity in the relevance, we computed an integer rank for each object with the following approaches:\n\nActors: We used the weekly ranking of IMDB at the end of July for actors and actresses. The result was a rank of 1 for the best male or female actor and then an increasing value according to the IMDB weekly rank.\nMovies: We sorted movies according to the following formula: log(nb_voters) * rating. We then chose to use the position of movie in the sorted vector as a rank (1 for the best movie).\n\nAt this step, we had a… set of 2.4M entries from IMDB containing actors and movies with a indication of relevance. Check out the data set with all objects and ranks in JSON.\nHere is an example of a movie:\n\nAn example of a TV Series:\n\nAnd an example of an actor:\n\nTo have a relevant search, we wanted to search first in the 'name', then in the 'year' and finally in the 'episode' attribute. We also wanted queries to match across the three attributes to be able to answer searches like 'Shawshank Redemption 1994' or 'Game of Thrones rains of castamere'\nIndexing\nAlgolia and Elasticsearch are schema less and directly support indexing of our objects. To do the indexing on Algolia we used its ruby client with the following code:\n\nFor Elasticsearch, we converted our objects in the bulk indexing format. We wrote a small ruby script to split data in several files and then imported them with CURL.\n\nTo assess performance, we directly imported data locally on the indexing host. Here were the indexing times:\n\n\n\n\n1 Shard\n5 Shards\n\n\n\n\nAlgolia\n82s\n38s\n\n\nElasticSearch\n148s\n68s\n\n\n\nSidenote: Why we didn't use a suggest plugin for Elasticsearch\nA shortcut to providing instant search in Elasticsearch is to use the autocomplete feature or suggest plugin. These modules, much faster than standard search, are used in several sites such as SoundCloud. They however don't support multi attributes search and thus cannot be used in our situation. What's more, they may hinder the user experience. In SoundCloud for example, you can get a suggestion for \"Pink Floy The Dark Side of the moon\" if you enter the text in that exact order, but if you enter \"Dark side of the moon Pink Floyd\", you won't get any result. Annoying.\nFirst Query\nIndexing is actually very easy with the two solutions!\nWith Algolia, a single line of ruby was enough to perform our first search:\n\nFor ElasticSearch, we selected the tire ruby client and our query translated in:\n\nSearchable content\nNow that we had indexed all our data, we wanted to… specify in which attribute to search. As explained before, we wanted to search in these three attributes: 'name', then 'year' and finally 'episode'. Other attributes should not be used for search.\nWith Algolia, we can specify the list of fields in the index settings via the « attributesToIndex » parameters, they are sorted by decreasing order of importance so you do not need any to set any boost. A single line of code was enough to change them:\n\nWith Elasticsearch, we can specify fields directly in the query and use boosts for each of them. The choice of the boost value is important as it directly impacts the \"_score\" value. It is, however, an opaque value that requires a trial and error process to get right.\n\nCustomize ranking\nHere, the first difficulties started. As a reminder, we wanted to take into account the popularity of actors and movies, so that the query ‘geo’ would return ‘George Clooney’ as first result since it is the most famous actor starting with ‘geo’. To do so, we used the rank value that we computed.\nIn Algolia, we simply changed the customRanking setting:\n\nTo add a sort criteria in the query of ElasticSearch, we modified the query directly:\n\nThis sorting configuration might seem pretty explicit, but it is in fact quite dangerous as it conflicts with the boost on fields. To better understand the problem, let's look at the query 'the rains':\n\nIt might be counterintuitive that the result that matches in the \"episode\" attribute is found before results in the \"name\" attribute without respecting the boost. The boost has an impact on the \"_score\" float value that is smaller when it matches the episode attribute and not the name. The problem here is that merging the user defined \"rank\" with the floating value \"_score\" is complex:\n\nIf  \"_score\" is before \"rank\" in the sort criteria, \"rank\" is not used since each hit has a different float value for \"_score\".\nIf \"rank\" is before \"_score\" in the sort criteria, attribute order is not taken… into account.\n\nIn Algolia the ranking is handled differently. Instead of having a unique float value for ranking, we compute a set of integer values that are explicit and easy to understand. You can see them on the first three results of the query 'the rains':\n\nThe ordering of attributes is strictly respected. As we didn't specify otherwise, results are ranked with the default ordering that performs:\n\nFirst a sort by number of typos (defined by nbTypos value, in increasing order)\nThen, a sort by geo-distance. It is only used when the query is done inside a given geographic area and is applicable in this case.\nThen, a sort by proximity between matched query words in objects (defined by proximityDistance, in increasing order). If query words are next to each other, the value is 1, is there is one word between two query words, the value is 2, etc.\nThen, a sort according to the matched attribute and the position of the word in the attribute (defined by firstMatchedWord, in increasing order). This is the criteria that respect the order of attributes defined in \"attributesToIndex\" setting.\nThen, a sort on the number of exact words that matched (defined by nbExactWords, by decreassing order). It is often relevant since last query word is interpreted as prefixe by default.\nFinally, a sort by user provided custom ranking (defined by customScore attribute, by decreasing order).\n\nThis explicit way of ranking results gives you a full understanding of how results are ranked, by opposition to having a float value that is very difficult to understand. For example in the two first results, all integers are identical except for the \"userScore\", which indicates that the custom ranking was used to make the difference between these two objects.\nThe most important thing about Algolia ranking is that you are able to easily customize it without giving up other criteria. In this case, we introduced a popularity rank on movies/actors while still considering matching attribute as more… important in the overall ranking rules. The ordering of criteria can of course be changed if we had wanted to consider the custom ranking score more important than the matching attribute.\nInstant Search\nGoing further,  we wanted to provide an Instant Search, also called search-as-you-type. In order to do that, we needed to interpret the last query word as a prefix.\nIn ElasticSearch, you can enable wildcards and add one at the end of the last query word, here is an example with the query 'word w':\n\nWildcards in ElasticSearch are not exact and are performed in an approximative way (there is a limit in the number of words a prefix query can be expended to).\nIn Algolia prefix queries are exact, no approximation is performed. By default last query word is considered as a prefix but you can easily change this behavior to have all words interpreted as a prefix in the index settings:\n\nTypo-tolerance\nThe final step was to enable fuzzy search in order to tolerate user mistakes.\nIn both products, typo tolerance (or fuzzy search) is defined by a Levenshtein-distance between query words and hit words. For Asian languages (Chinese, Japanese, Korean), Levenshtein-distance is known to be inefficient and Algolia applies a different strategy that for example considers transformation in simplified Chinese as a typo of Traditional Chinese and vice versa.\nOn our data-set, levenshtein-distance is a good measure of typos.\nIn Elasticsearch, you can use the Lucene fuzzy operator '~' on each query word to apply a Levenshtein-distance with the number of typos you want to tolerate (for example 'george~1 clooney~1' means that you can tolerate one typo on 'george' and 1 typo on 'clooney').\nUnfortunately, you cannot combine the fuzzy operator with a wildcard operator, so you cannot apply a fuzzy search on a prefix and thus have typo-tolerance on the last query word. In order to keep the Instant-Search feature, we did a wildcard search on last query word and applied fuzzy search on other… query words.\nIn Algolia, typo tolerance works out-of-the-box and you can define the size of the word needed to allow one or two typos with these two index settings:\n\nminWordSizefor1Typo: minimum number of letters in a word/prefix to tolerate one typo (defaults is 3)\nminWordSizefor2Typos: minimum number of letters in a word/prefix to tolerate two typos (default is 7)\n\nHere is for example the first result of the query 'alexandre~1 b*' in ElasticSearch:\n\nAnd the first result for the query 'alexandre b' in Algolia:\n\nWe can see that Elasticsearch misses results with 'Alexandre' because it does not sort results by number of typos. This is a huge problem to get a good ranking when accepting user mistakes.\nBut Algolia goes even further and allows typos on prefixes. Here are the first 3 results for the query 'gorge clon' in Algolia. The \"nbTypos\" element indicates the number of typos corrected. As you can see, corrected parts of the words are highlighted, which is essential for the end-user to understand why such hits are retrieved. For the first object, 'gorge' matched 'george' (count for one typo since the Levenshtein-Distance between the two words is 1) and 'clon' matched the prefix 'Cloon' (also count for one typo since the Levenshtein-Distance between the two words is 1).\n\nAlgolia provides out-of-the-box typo-tolerance that works on both words and prefixes, and does a smart highlighting of results according to typo tolerance. This allows end-users to understand search results even with typos. Unfortunately Elasticsearch fuzzy matching is more complex to customize and does not provide the ability to highlight prefixes.\nSearch Performance\nWe did several search queries to evaluate performance on the IMDB data set. In Algolia we used all features including typo-tolerance on prefix. In Elasticsearch we used Instant-Search queries with typo-tolerance except for the last query word (we cannot apply typo tolerance on the last word since it is a prefix).\n\n\n\nQuery\nAlgolia 1… Shard\nAlgolia 5 Shards\nES 1 Shard\nES 5 Shards\n\n\n\n\ngeo\n< 1ms\n2ms\n101ms\n36ms\n\n\ngeorge clo\n2ms\n3ms\n121ms\n51ms\n\n\nb\n< 1ms\n2ms\n147ms\n60ms\n\n\nbatman\n2ms\n2ms\n94ms\n29ms\n\n\nworld w\n5ms\n2ms\n134ms\n68ms\n\n\ne\n< 1ms\n2ms\n202ms\n81ms\n\n\nemilia\n< 1ms\n2ms\n102ms\n31ms\n\n\nalexandre b\n18ms\n9ms\n243ms\n109ms\n\n\n\n \nConclusion\nElasticsearch is a wonderful tool for Big Data analytics, but it is very difficult to reach a good relevance with it on database search. You can try to add some logic on top of Elasticsearch or try to reorder manually results for some queries, but it’s tedious work that continuously needs to be tuned. Algolia on the other hand focuses on getting a very good relevance with minimal configuration. While not optimal for all use cases, it makes it particularly appropriate for database search."
  },
  {
    "id": "1495-0",
    "title": "Our Search-as-a-Service offer has now 10 API Clients!",
    "author": "Julien Lemoine",
    "text": "We recently reached a new milestone towards the release of our Search as a Service offer. We're now proud to offer 10 API clients, covering all major languages.\nEase of use was a major focus during development. We began by offering a complete and easy-to-integrate REST API. Providing API clients was a logical way to improve ease of use. You can now quick start and test the engine with your data in a couple of minutes, with no prior configuration whatsoever. Each API Client is released under the MIT License and comes with a quick start and complete documentation:\n\nJavascript client\nRuby client\nPython client\nPHP client\niOS and OS X client\nAndroid client\nJava client\nC# client for Windows, Windows Phone and Silverlight\nNode.js client\nCommand line client for linux/mac\n\nThis variety of languages and platforms reveals the diversity of our beta testers:\n\nCustomer size: from a small startup developing their MVP, to a big social network searching in their 130M+ users.\nVolume: from a few queries to tens of millions per day. \nTechnical environments: mobile, desktop, and web apps.\n\nInterested in trying it out yourself? Ask for an invite!"
  },
  {
    "id": "1471-0",
    "title": "Asian Language support in our Offline Search SDK 2.2",
    "author": "Julien Lemoine",
    "text": "Like most search engines, version 2.1 did not include any specific processing for Asian Languages. Version 2.2 significantly improves Asian language support (Chinese, Japanese, Korean) by including specific processing like the automatic conversion between Simplified Chinese and Traditional Chinese using the Unicode UniHan Database. This advanced processing was only possible because we built our own Unicode library. Many thanks to Stephen for his help!\nThis release also contains other improvements we released first for our SaaS version:\n\nThe out-of-the-box ranking was greatly improved when queries contained more than two words,\nIndexing speed was greatly improved on mobile (2 times more efficient),\nSearch speed was improved by about 20%.\n\nWe hope you’ll like these new features, and as ever, we welcome your feedback!"
  },
  {
    "id": "1449-0",
    "title": "New iOS and OS X API clients for our Search-as-a-Service offer",
    "author": "Julien Lemoine",
    "text": "One week after releasing our Java & Android clients, we are happy to release our iOS and OS X API clients for our search-as-a-service offer.\nIn order to ease the setup, we support Cocoapods. Installation of the client just requires one line in your Podfile:\n\nAnd don’t forget we also provide developers with an offline SDK that they can use to search directly on iOS devices with no connection to the network. Developers now  have the perfect tools to build a great search experience both online and offline.\nWith this new client, we now have API Clients for the most popular languages and platforms. They are all released under the MIT license and available on our Github account:\n\niOS and OS X client\nJava client\nAndroid client\nRuby client\nPython client\nPHP client\nJavascript client\nNode.js client\nCommand line client for linux/mac\n\nEase of integration just improved again! Your feedback (and pull requests) is most welcome.\n "
  },
  {
    "id": "1434-0",
    "title": "New Java & Android Search-as-a-Service API Clients at DroidCon Paris!",
    "author": "Julien Lemoine",
    "text": "Our Search-as-a-Service offer is progressing toward its official release. We launched our Java and Android search API Clients at DroidCon Paris today! Come to see us if you're attending!\nAnd don't forget we also provide developers with an offline SDK they can use to search directly on Android devices with no connection to the network. Developers now have the perfect tools to build a great search experience both online and offline.\nThe Android API Client is based on the Java client and adds support of asynchronous API calls. You can thus easily trigger a search query from the UI thread and get the result in a listener without any additional line of code. You have just to implement the IndexListener interface.\nWith these two new clients, we now have eight API Clients released under the MIT license to simplify integration of Algolia Search as a Service:\n\nJava Client\nAndroid Client\nRuby client\nPython client\nPHP client\nJavascript client\nNode.js client\nCommand line client for linux/mac\n\nEase of integration just improved again! Your feedback (and pull requests) is most welcome."
  },
  {
    "id": "1417-0",
    "title": "Discover our 6 new Search-as-a-Service API Clients!",
    "author": "Julien Lemoine",
    "text": "We launched the private beta of our Search-as-a-Service offer two months ago. In that time we received very positive feedback from our beta testers and we couldn't thank them enough for the help they provided. To date, they have sent over 1M indexation jobs and over 5M queries, and the trend is increasing.\nToday we are happy to continue simplifying the experience with the release of six API Clients under the MIT license:\n\nCommand line client for linux/mac\nRuby client\nPython client\nPHP client\nJavascript client\nNode.js client\n\nEase of integration just improved again! Your feedback (and pull requests) is most welcome."
  },
  {
    "id": "1388-0",
    "title": "Algolia Search Offline 2.1 is out!",
    "author": "Julien Lemoine",
    "text": "We are pleased to introduce a new version of Algolia Search Offline for iOS, Android and OS X (Windows versions will come soon).\nVersion 2.1 significantly improves the out-of-the box relevance of Algolia Search. We are now confident we have the best relevance on the market. We will discuss our ranking approach compared to traditional methods in another post. For now, the two main improvements offered in this version are:\n\n\nWhen the query string has one word that is the entire content of a field, we will display it first. You can try it with the \"arm\" query on our Crunchbase demonstration to get the idea: http://bit.ly/searchcrunchbase (it uses the same relevance approach).\n\n\n\nMore importance is given to proximity than to the order of attributes. For example, the query \"managing p\" will now match first \"Managing Partner\" in the \"Title\" attribute instead of \"P\" in the \"Name\" attribute followed by \"Managing\" in the \"Title\". This is the case even if the order of the attributes is [\"Name\", \"Title\", ...].\n\n&nbsp;\nWhile you can control ranking with the setRankingOrder method, you will benefit from these improvements by default.\nThis new release also introduces some new features:\n\nA way to efficiently serialize latitude/longitude and float values in your custom objects (reduce the size of serialized objects by up to 80%).\nA method to compile an index in an old version format. This is useful when indexes are created server side and then pushed to applications that support old versions of Algolia.\nAll characters in tag filters are now supported.\nIt is now possible to do a logic OR between tags in filters. For example, you can search all contacts that match the query \"paul\" and have the tag \"friend\" OR the tag \"classmate\".\n\nWe hope you'll like these new features, and as ever, we welcome your feedback!"
  },
  {
    "id": "1286-0",
    "title": "Algolia Search Offline SDK now supports Cocoapods",
    "author": "Julien Lemoine",
    "text": "We have great news for our iOS and OS X users: our Offline SDK is now available as a CocoaPods dependency.\nCocoapods is a popular dependency management tool that lets you specify the libraries (dependencies) you want to use for your project in an easy-to-edit text file (Podfile). It then fetches all the required libraries and sets up your Xcode workspace.\nYou can now set up Algolia Search Offline in your iOS project with this line in your Podfile:\n\nYou can also set up Algolia Search Offline in your OS X project with this line:\n\nOnce you're done, simply use the \"pod install\" command to set up Algolia Search Offline in your project. Now it's easy to manage library dependencies for iOS and OS X projects!"
  },
  {
    "id": "1214-0",
    "title": "Introducing a RubyMotion search gem by Algolia!",
    "author": "Nicolas Dessaigne",
    "text": "If you are a Ruby developer and have an iPhone, chances are you already now about the cool project that is RubyMotion! I quote:\n\"It lets you quickly develop and test native iOS applications for iPhone or iPad, all using the awesome Ruby language you know and love.\"\nAnd it rocks! It's actually used by the Ruby on Rails sponsor 37signals.\nWhat if you could use your favorite search engine along with your favorite language to create iOS apps? That's exactly what we propose with Algolia's RubyMotion search gem that seamlessly integrates in your Ruby project. You know the trick:\n\nThe gem code is open source. You can fork it from github.\nSpecial thanks to the RubyMotion team and especially to Joffrey for this integration!"
  },
  {
    "id": "1199-0",
    "title": "V2: Search by Geolocation in our Offline Search SDK",
    "author": "Nicolas Dessaigne",
    "text": "While our latest news focused on the Algolia Search cloud offer (you can still join the beta), we're pleased to introduce a major new version of Algolia Search offline: V2! It is available today for iOS, Android and OS X. Windows Phone and Windows versions will be released as soon as they are ready. A few months in the making, these new features were built on early customer feedback and will simplify integration.\nAlgolia becomes the easiest way to search by geolocation!\nThe ease of integration is a constant concern for us and that's why we carefully consider every new feature. Two important features made it in this version:\n\nGeo-search means the ability to search around a location or inside a bounding box. Results can be sorted by distance and of course geo-queries can be combined with textual ones. We added a dedicated tutorial in the doc to get up to speed with this new feature in no time (for iOS and Android).\nTag filters enable restriction of results to specific tags. We received this demand a number of times in order to avoid the creation of too many specialized indexes.\n\nThese new features are also available in the beta of our cloud version!\nImproved performance and ranking\nWith some hard work... and a lot of profiling, we have been able to get a 10% gain in performance on every query.\nIn V1, name matches were always considered more important than other attributes, but we didn't consider differences between other attributes. This changed in V2: ranking priority now respects the order in which you indicate attributes in the textToIndex method. It's more powerful while actually being more consistent with no specific processing of the name field.\nBut this improvement comes at the cost of a slightly bigger index and longer computation. If index size is important or if you need to earn a few nanoseconds more, you can optimize it away with the increaseCompression method. You'll get a 10 to 30% reduction in index size and an additional 20% boost in performance… (that's 30% total compared to V1!).\nEasy just got easier\nIntegrating search in an app has never been so easy. For V2 we took into account all the excellent feedback we received, and wherever it was possible we simplified the API:\n\nNo distinction between suggest and search methods. We wanted to match the expected use-cases of the SDK but it was causing more confusion than anything else. So there is now only one way to send queries to an index: the search method.\nWith the addition of geo-search, the index class was becoming crowded. We simplified this by decoupling the search approach and query definition. A small set of search methods enable the developer to choose if the search will be synchronous, use a callback, or batch several queries. And a simple SearchQuery class defines the nature of the queries themselves: geolocation, use of prefixes, tag filters, etc.\nOut of simple strings for which we provide a helper, every indexable object now has a UID. Our use of a \"name\" for this role led to a few difficulties when collisions were possible (persons for example). There are no longer any privileged attributes.\nLicense key initialization is now done using a static method. It is a best practice that was actually necessary to build a RubyMotion gem.\n\nSpecific to Android, we also added an AbstractIndexable abstract class. Instead of implementing the Indexable interface, you now have the option of directly extending AbstractIndexable that takes care of optional methods for you.\nSpecific to iOS, you can now directly index core data entities with the setCoreDataEntityDescription selector. No need to create a wrapper.\nStill able to read V1 indexes\nIf for any reason you cannot replace or reindex your data, V2 is still able to search in a V1 index. However, as the name attribute was removed you do need to implement the IndexableLegacy interface. If you then publish changes, the new index will be in the V2 format.\nWe're really sorry to make our Windows Phone and Windows… customers wait. Feel free to torment us with your needs, it's great motivation to finish more quickly 😉\nIf you're still reading, I guess it's time for you to test this new version of the Algolia Search Offline SDK. Get started!"
  },
  {
    "id": "1181-0",
    "title": "We are NEXT Berlin Finalists!",
    "author": "Nicolas Dessaigne",
    "text": "The NEXT Berlin Conference will take place on April 23rd & 24th in one of the most active entrepreneurial ecosystems in Europe!  More than 100 experts will share their knowledge with an expected audience of 2000 attendees.\nIf you follow our newsletter or social presence, chances are you saw our call for help to participate in the NEXT Berlin startup competition. We sincerely thank you for your votes! They placed us in the 30 top startups to be considered for the final. The judges then selected us as one of the 12 finalists to pitch during the conference. We are thrilled to be able to present Algolia to Berlin!\nIf you happen to be attending NEXT Berlin, ping me if you’d like to meet! (@dessaigne on twitter, or nicolas at algolia dot com)."
  },
  {
    "id": "1112-0",
    "title": "Search on OS X: Search Kit vs Algolia",
    "author": "Nicolas Dessaigne",
    "text": "I recently read Matt Thompson's post about Apple Search Kit and he's right about it being rather unknown; At least I had never heard of it before. So my very first thought was \"How does it compare to our own SDK for OS X?\" I immediately asked Julien if he could perform a few tests. Here are our findings.\nScope of use\nBefore examining the technical differences, it is important to stress the different intents behind the creation of both tools.\nSearchKit has been designed as a document search engine, in what I call the \"traditional approach\". It is able to index documents directly from disk (including conversion for a few common formats like pdf), and at the same time enables tokenization or stop words to be customized.\nAlgolia was designed differently. Even if it can be used to index 100-page documents, we targeted small texts, focusing on ease-of-use and performance. Tweaking a search engine requires deep expertise so we chose to hide that complexity by making sensible choices that work 99% of the time. You may see it as the \"we know what's best for you\" approach taken by Apple for many of their products! On the other hand, we provide a Search SDK and only that. If you need to index PDF files, you need to manage the conversion with other libs.\nTest setup and scenario\nTo get up to speed quickly, we performed our test with an existing dump of city data we had already grabbed from geonames for a tutorial (available here: http://www.algolia.com/doc/GeonamesCities.json). It contains 150k cities with their name, country, known population, and geoposition.\nSearch Kit doesn't support multi-fields (a document is a single collection of terms) so we chose to only index the city names in this test. (Teaser: upcoming Algolia SDK V2 will support geosearch).\nResults\nHere are the raw results on a MacBook Pro Retina:\n\n\n\n\nSearch Kit\nAlgolia\n\n\nIndexation time\n19498ms\n4240ms\n\n\nIndex size\n17MB without terms\n(24MB with terms for summary)\n8MB\n\n\nSearch time for prefix queries\n(max 1k… hits)\n\n\n\n\n    l*\n255ms\n14ms\n\n\n    lo*\n255ms\n16ms\n\n\n    los*\n51ms\n20ms\n\n\n    los a*\n40ms\n12ms\n\n\n\nThe actual syntax of Algolia queries does not need the '*' character, as we use prefix queries by default.\nOur observations\nWe discovered a few weak points of Search Kit during the test:\n\nIt is a low level API and it doesn't provide an Objective-C wrapper. It may explain why almost no one has heard of it.\nIt doesn't provide any help to highlight matching characters or terms.\nIt doesn't support approximations, nor stemming. That means you need to query the exact words that are in the documents you index (prefix queries can also help in some cases).\nThere is no notion of multiple fields. A document is a simple set of terms. This is particularly problematic when you want to tune ranking, for example by boosting the title.\nYou must use a URI as identifier, even if you are indexing a simple string or if you are indexing an object that already has its own unique identifier. No big deal, but it complicates the code. For our test, we had to write something like the following:\n\n\nHowever, there are a few features in Search Kit that Algolia does not provide:\n\nIt enables customization of tokenization and of stop words. Make sure you know what you're doing however, as it can have unexpected impact. For example, if you add '-' as a kSKTermChars, you won't be able to find \"Saint-Etienne\" with the \"etienne\" query.\nIt is able to convert some common file types (PDF, HTML, RTF, DOC).\nIt supports more advanced boolean queries.\n\nConclusion\nThere is no one-size-fits-all search engine out there, especially on OS X. I would say that if you're building a search-as-you-type feature like Spotlight, you should first give our SDK a try. It is much faster to integrate, has better response times, and handles typos. And don't forget it is also available for your iOS apps.\nIf, on the other hand, you want to build a more traditional document search, you may need the document conversion… and customization of Search Kit.\nWe are not Search Kit gurus and we're happy for any errors in this post to be corrected.\n\n \nHere is the code we used for indexing and searching on Search Kit & Algolia.\nIndexation with Search Kit\n\nIndexation with Algolia Search\n\nSearch with Search Kit\n\nSearch with Algolia Search"
  },
  {
    "id": "1054-0",
    "title": "Online Search in the Cloud: Version is in Beta!",
    "author": "Nicolas Dessaigne",
    "text": "Imagine all the power and simplicity of Algolia Search, but available online, from any connected device. Welcome to our Cloud!\nUp to now, our instant, typo-friendly search was only available locally on your device. You could only index data stored directly in your app. While that was great for offline apps, it was not so fantastic for rapidly changing data server-side. All that has just changed! You can now use our search engine online, enabling you to change your data at any frequency and return consistantly up-to-date search results in your apps. Interested? Request your invitation to the Beta from the features page.\nAnd don't forget you can combine it with our offline SDK, for great search performance whatever the connectivity!"
  },
  {
    "id": "1026-0",
    "title": "FREE Search SDK: Algolia is Now Available for Free!",
    "author": "Nicolas Dessaigne",
    "text": "Yes, you read that right! We're not talking about a free trial, but of a completely FREE version of our SDK. It is often difficult to implement a good search experience, so we want to democratize access to easy-to-integrate and first class search features! All you have to do is to display non-intrusive Algolia branding.\nThis branded offer is already available on our website. Don't delay, register today! Check out our use-cases section to discover possible implementations and see how Sharypic, Offline dictionaries, and Sush.io have integrated Algolia in their apps.\nHelp us spread the word. We want every app developer to be able to take advantage of this offer!"
  },
  {
    "id": "993-0",
    "title": "Meet us at the Mobile World Congress! - The Algolia Blog",
    "author": "Nicolas Dessaigne",
    "text": "We'll be in Barcelona next week for the Mobile World Congress! Last year, more than 67,000 attendees participated in the event.\nWe will use this opportunity to launch our new website along with a brand new offer... Algolia for FREE! Stay tuned!\nIf you happen to be attending MWC too, ping me if you'd like to discuss over a coffee 🙂 (@dessaigne on twitter, or nicolas at algolia dot com)."
  },
  {
    "id": "1000-0",
    "title": "Round table at Microsoft TechDays 2013",
    "author": "Nicolas Dessaigne",
    "text": "I had the pleasure to be invited by David Catuhe to participate in a round table about Windows8 development during Microsoft TechDays 2013!\nI was joined by Christopher Maneu from Deezer and Guillaume Leborgne from MCNext, both deeply involved in Windows development. David (cropped from the photo) led the discussion with Jean Ferré, who leads the developers, platform and ecosystem division at Microsoft France.\nThe discussion was very interesting and openly addressed the late start of Microsoft on mobile. They seem to have spared no effort to ease the work of developers, for example by opening the platform to development in HTML5/Javascript. I confess I initially thought it was a strange choice for native apps but it seems to have attracted quite a few web developers.\nThis round table was a great opportunity to meet smart people and gain insight into the Microsoft platform! Thanks also to the journalists in attendance who covered Algolia in the IT press."
  },
  {
    "id": "963-0",
    "title": "We ranked Second at Start In Paris!",
    "author": "Nicolas Dessaigne",
    "text": "Start In Paris is a monthly event where 5 startups have the opportunity to pitch their service to the Paris startup community. After a first selection and then a public vote, we were selected as one of 5 finalists of the #22 edition that took place yesterday, January the 28th!\n\nAlgolia has greatly evolved over the last few months, so it was an excellent occasion for us to test a new pitch! And the response has truly exceeded our expectations. After a 5 minute pitch we received a rush of questions, displaying interest and insight from the 400-strong audience!\nWhile we were the only tech startup to pitch to an audience including only a few developers, we ranked second in votes, just behind Kitchen Trotter, our fellow Seedcamp finalist from last December. Congrat to them, they were truly excellent!\n[Edit 03-Feb-2013] Check out Alexis Niki great feedback about the event on Rude Baguette [/Edit]"
  },
  {
    "id": "900-0",
    "title": "Sharypic benefits from Algolia Search!",
    "author": "Nicolas Dessaigne",
    "text": "Demonstrating how Algolia Search functions in the wild, in this post we will discuss a recent integration with sharypic, another Paris-based startup focussed on photo sharing at events.\nSharypic is a web and mobile app dedicated to collaborative photo sharing, focused on enabling users to easily gather photos from attendees during and after an event. The platform allows users to collect and share photos from all devices, including mobiles (via Twitter and Instagram), cameras, and computers, and from already existing albums on Facebook, Picassa, and Flickr. One of their killer features is the ability to stream photos to a live PhotoWall at the venue and to an embeddable slideshow widget. This increases engagement both with event attendees and with an online audience. On sharypic one of the primary ways that users discover events is via a search bar, in addition to pages highlighting recent and popular events.\n\nThe existing mobile search function relied on users accurately typing an event's name into the search field, which limited the results (especially on smartphones where typos are common). By integrating Algolia Search into the mobile app, sharypic users can now type just a few letters of a search term, or enter it incorrectly ('pqris' instead of 'Paris'), and the results will display the corrected term within event names, locations, descriptions, or hashtags.\nMartin Fourcade, one of sharypic's co-founders, said \"For our users, it's exactly what we needed. They can show the best photos of their events to friends without bugging their smartphone and whining about the internet connection. I'm lazy when it comes to typing on my smartphone, impatient when it comes to waiting for server responses... now everything is done with a few keystrokes!\"\nSharypic's other co-founder François-Joseph Grimault hopes that this new intelligent search will enable users to find specific content more easily, possibly leading to increased exploration on the platform. Time will… tell how the new search feature affects user behaviour, but reducing user frustration through quick and efficient search is a step in the right direction.\nDownload the latest version of the sharypic app, including integrated Algolia Search, and have fun with photo sharing at your next event!"
  },
  {
    "id": "817-0",
    "title": "Why Android APK Format is a Mistake",
    "author": "Julien Lemoine",
    "text": "When I started to develop for Android it appeared to me that an APK file was just an archive, a simple approach that you can find in many systems today. Files are extracted from the archive at installation and you can access them via the file-system.\nThis seemed even more reasonable since Android uses Linux which is very good in respect to POSIX standards.\nBut I was completely wrong! An APK is not a mere archive: the application starts from and uses the APK at runtime! This is a horrible decision that will probably hurt Android for a long time...\n[Edit 28-Jan-2013] The goal of this post was to express my point of view about the bad properties of using directly the APK file at runtime versus relying on the file system. I used memory-mapped file to illustrate this but the post is incorrect on that topic. There is in fact a way to memory-map a file directly from the APK: you can use an extension for which files are stored uncompressed inside the APK (mp3, jpg, ...) and use the  or  to have offset/length inside the APK file.\nAll my thanks to Jay Freeman for his excellent feedback. His comments helped me to understand my mistake and to improve our Android integration!\n[/Edit]\n\nWhat is the Problem with the APK format?\nLet's look at our own example. At Algolia, we have designed an efficient binary data-structure that is able to answer instant-search queries in just a few milliseconds, even with a very big data set (for example with all the world cities' names from Geonames: 3M+ entries). This data-structure is designed to be used directly on disk without being copied in memory. To obtain optimal performance, we use a memory-mapped file which is standard on all platforms, especially on Linux.\nWe have been able to use memory-mapped files on all platforms, except on Android!  In fact you can only retrieve an InputStream from a file packaged in an APK. So the only solution to use a memory-mapped file is to copy the resource from the APK on disk and then to use the… file-system. This seems like re-implementing an installer in each application.\nIs the APK so bad? Why did they design it this way?\nI imagine that Android developers chose this approach to solve some pitfalls of file-systems. I can think for example about solving performance problems when you have a lot of small files in one folder, or reducing the size of applications on the device (resources are compressed in the APK and decompressed only when the application uses them, which actually contributes to the sluggish image of Android).\nI may of course be wrong, there may be other more important reasons for this approach. But if not, Android should have thought more about the consequences of their choice: in the long term, the APK constraints are more serious than those small pitfalls that could have been solved in other ways.\nBut wait... Android applications can contain dynamic libraries (.so files) via NDK. Isn't it the principle of dynamic libraries to be memory-mapped? In fact I am pretty sure they discovered this problem when working on NDK since dynamic libraries are automatically extracted from APK file at installation and stored in an application directory in '/data/data'. I am wondering why they decided to implement this hack instead of fixing the problem...\nConclusion\nDeveloping an API, a SDK or worse, a whole platform, is extremely difficult. Let's face it, it's unavoidable to ship some badly designed components or inconsistent APIs. We definitely need to listen to developers' feedback even when it hurts. Actually, the real difficulty comes when it's time to put things right without alienating existing users!\nBy the way, if you know more about APK design choices, I'm interested to hear from you!"
  },
  {
    "id": "732-0",
    "title": "Seedcamp: Tips and Advice From a Finalist - The Algolia Blog",
    "author": "Nicolas Dessaigne",
    "text": "About one month ago, we were given the opportunity to participate in Seedcamp Paris 2012. In this post I'll try to think back to that time and provide feedback, along with some advice if you're considering going in that direction. In short, I can already say that this experience has revolutionized the way we drive the company!\n\nThe Application\nEverything starts with a good application... or a rushed one in our case! At that time (end of October) we were fully focused on product development and finishing everything needed for our launch (documentation, video, etc.). While we knew about Seedcamp coming to Paris in early December, we initially saw it as a potential distraction from more important stuff... we could not have been more wrong!\nOur opinion changed after a lunch with Philippe Laval, former Seedcamp alumni with his company Kwaga (now WriteThat.name), whom I had the pleasure to meet a few times in my previous jobs. In a couple of hours, Philippe opened our eyes to the broad requirements for successfully launching a startup. Product is clearly one, but there are so many others we could not ignore. His description of how his Seedcamp participation helped him a few years back was enough to convince us we had to give it a shot.\nEach step of the process brought its own rewards, starting with the application! It is a simple form but it contains excellent questions about positioning, monetization, competition and more. The best thing about it was that we were forced to write down our vision for the company and this simple process made us think harder than we had ever done before. We ended up convincing ourselves more fully of what we were doing and why, with facts and arguments to defend our vision. Of course, we would present the company completely differently today! We continued to learn at an accelerated pace in the following weeks. Despite some imperfections, participating in a Seedcamp final is definitely a must.\nThe Almighty Pitch Preparation\nAbout a couple… of weeks before the event,  we received the good news: we were in! The promise to keep our success confidential was a bit frustrating, but honestly we were overjoyed! However, there was no time to celebrate as preparations for the event began immediately.\nWe already knew about the format and what to expect from the Seedcamp team from what we gathered on the web. But there is a great leap from knowing to actually preparing to participate! And with all the regular day-(to-night-)to-day activities that our startup entailed, time was short.\nWe started by focusing on the pitch: 3 minutes, in English, to say a lot. We rapidly drafted a first version and began to iterate, adding essential information and removing anything not worth mentioning. It may seem straightforward, but including everything from the problem, solution, vision, market, competition, monetization and team in a three minute deck is no easy task. It is, however,  an excellent way to learn how to better explain your startup! We learned to be concise - every word counts, and I began to memorize each and every one of them. I had given speeches of various kinds on many occasions in the past but I had never learned a text by heart. If you've never done it before, it is more difficult than you may think. For a long time, I was so concerned to get the wording right that any mistake would send me fumbling for my next words. I had to practice dozens of times to feel comfortable enough to pay attention to simple things like intonation!\nSome advice: start working on your pitch as early as possible and get as much feedback as you can. We were able to organize some pitch training sessions with startup personalities and I can't stress how useful this proved to be! (Our greatest thanks to Wessel, Liam, Philippe and Fred). But wait, isn't there a training session on the eve of the event? Yes, and it is incredibly useful. But if you don't like the idea of changing everything overnight, work on your draft and seek… feedback beforehand! The preparation session will be all the more focused for it. Also, if you can, do test your presentation with the actual computer and projector that will be used the next day. While in our case the conference room was really nice, the projector was very low-res and our embedded video would not render at all. Fortunately we had the time to record a new one with all fonts in bold...\nPrepare your Mentoring Session too!\nOut of the 20 teams participating, only two or three get an investment, so the odds are tough. You may be lucky and seduce the Seedcamp team, but if not don't waste your opportunity to get the most out of the mentoring sessions! This is an incredible opportunity: about 80 mentors, many of them expert in some field key to the success of your company, and all there ready to help. Believe me, you don't want to miss that! You would have a hard time to get their attention at any other time.\nSo be prepared! Think hard about the important questions (tip: usually the ones that hurt) and look at the mentors' bios, articles, and anything else you can dig up. If there's a fit, that's a win! Don't take it lightly, this is probably harder than preparing your pitch. You're probably thinking you only have one chance to make a good first impression with your pitch, while you can't shame yourself too badly by being underprepared for the mentoring sessions. You're right... but I'm convinced it is much more fruitful to concentrate on getting answers and advice that could mean success over failure for your company (a passable pitch may not be enough to get a Seedcamp investment but good advice might reap untold rewards). Convinced? Have a look at Andreas Klinger excellent post to get on track!\nIn our case, Julien did most of the work preparing for mentoring while I concentrated on the pitch. Could we have done more? Probably. Should we have done more? Yes. One month after the event, I think we put a good level of effort into the pitch preparation, but… we should have taken some more time on the mentoring preparation. Honestly, I think we were good, with a clear list of questions and we were ready to shut up and listen. But we could have researched the mentors more thoroughly, especially the ones not on our schedule.\nD Day\nYour pitch is ready, you've got all mentors covered, you slept 8 6 4 as many hours as you could... let's do this! First, get up early. Arrive at least 30 minutes before the start of the event. Then you have time to relax, get friendly with other teams and actually take the opportunity to speak with a few mentors before the event.\nAfter a relatively short presentation by the Seedcamp team, it will be time for the pitches! 20 of them, 3 minutes each. Attention tends to drift, which is why your pitch has to be captivating. Now is your chance to shine and put all your practice to the test! We were lucky to go first as we were sorted in alphabetical order. Other participants thought that was unlucky but I was thankful! I had an opportunity to grab the audience's attention and there was no time for stress to build up in anticipation before our turn... and everything went all right! It was actually one of my best performances, much better than during training the previous day.\nOne thing we could have improved was leveraging the pitch for communication. There are two things you can easily do - take some photos or a video, and tweet about your experience! Also, always have your twitter handle visible on the slides. You may think it is enough to put it on the first and last slides but what if someone watching wants to tweet during your pitch? Make it easy for them!\nTime for Mentoring\nOK, so you are prepared for the mentoring and have researched all the mentors? Some good ones are certainly not in your schedule so go and meet them during the breaks! And either ask them to participate in your open sessions (mentors are free to pick the teams they want for the last two sessions) or directly ask them the… questions that trouble you. Don't expect them to remember everything you said during your pitch (they just saw 20 of them!) so get a short intro ready. It is also better to have a quick demo instead of too many words.\nYou're now up for 5 or 6 sessions of  intense discussion with mentors. Andreas provides excellent advice on the pitfalls and potential benefits of these sessions. Don't forget, mentors are there to help you, they won't attack you. Don't try to convice them you're right, just listen what they have to say. Don't lose time explaining every aspect of your startup, instead ask specific questions about their experience. And don't be shy to ask for introductions. Either they will or they won't, but it doesn't hurt to ask!\nOf course nothing is ever perfect. All mentors are not equal and some of them may not even show up to your scheduled session (don't blame them, they usually have a hectic schedule and your startup may not have an activity they relate to). Anyway, you don't need to speak with all of them. It is much better to enjoy profound and fruitful discussions with a few.\nFollowups\nThe day concludes with a party - in our case at a nice wine bar. As you can guess, every team was quite exhausted and didn't stay late. A handful would be invited back the next day to discuss potential participation in the program. The selection process is a little obscure, with no clue provided as to their criteria. This is a bit frustrating as I would have appreciated to know why they didn't select us.\nKeep in mind that mentors will have been approached by many people and may forget the intro they promised you. Connect with them, thank them for their precious advice, and kindly remind them of the introductions they promised. And if you had a really good discussion with some of them, see if they would be happy to continue a more regular mentorship role.\nTakeaways\nTo get the best out of your Seedcamp final, you should invest a lot of time in preparation. Don't bet… everything on being accepted, the pitch day and mentoring alone is definitely worth your effort. The preparation itself helped us to focus on key questions and strategic decisions. We then had a priceless opportunity to test our ideas with experts in many domains and to ask them focused questions. That was like 6 months worth of mentorship in one day and it profoundly affected our strategy for 2013! The event also boosted our network - we contacted many people directly after the event and a few have even become regular mentors.\nThe Seedcamp team is amazing but there is some room for improvement. We got excellent feedback on our pitch during the training,  but unfortunately, it was lacking after the actual event. We were forwarded a few very nice comments from mentors, but that was it. While we had the opportunity to discuss directly with mentors, I would have loved to have some feedback from the Seedcamp team. I understand they do not want to encourage debate over their selection decisions, but understanding their reasoning would certainly be useful.\nFrom listening to other participants about their experience, be aware that if you are selected for investment, the paperwork is time-consuming! Do not plan too many developments for your startup or you may be disappointed by the poor progress you'll make during that time. And don't forget that the Seedcamp US trip is early the following year and that is something to prepare for too! Another important thing we overlooked when applying is the need to (re)incorporate your company in the UK. This is not such a big deal in the first year as you will have help from Seedcamp but what about afterwards, especially if you operate from your home country? Each country's legislation is highly specific and you may have some headaches in store. Don't forget to prepare for this before the end of the Seedcamp program.\nIn conclusion, Seedcamp Paris lived up to its reputation. Asked if we would do it again, we would certainly answer… yes! And I sincerely encourage other startups to give it a go!"
  },
  {
    "id": "794-0",
    "title": "Android NDK: How to Reduce Binaries Size - The Algolia Blog",
    "author": "Julien Lemoine",
    "text": "When we started Algolia Development for Android, binary size optimization was not one of our main concerns. In fact we even started to develop in JAVA before switching to C/C++ for reasons of performance.\nWe were reminded of the importance of binary size by Cyril Mottier who informed us that it would be difficult to integrate our lib in AVelov Android Application because its size. AVelov is 638KB and Algolia was 850KB, which would mean that AVelov would more than double in size with Algolia Search embedded.\nTo address this problem we managed to reduce Algolia binary size from 850KB to 307KB. In this post we share how we did it.\n\nDo not use Exceptions and RTTI\nWe actually do not use exceptions in our native lib, but for the sake of completeness, I'll cover this point too.\nC++ exceptions and RTTI are disabled by default but you can enable them via APP_CPPFLAGS in your Application.mk file and use a compatible STL, for example:\n\nWhilst using exceptions and RTTI can help you to use existing code, it will obviously increase your binary size. If you have a way to remove them, go for it! Actually, there's another reason to avoid using C++ exceptions: their support is still far from perfect. For example if was impossible for us to catch a C++ exception and launch a Java exception in JNI. The following code results in a crash (will probably be fixed in a future release of the Android NDK toolchain):\n\nDo not use iostream\nWhen starting to investigate our library size following Cyril's feedback, we discovered that Algolia binaries had vastly increased in size since our last release (from 850KB to 1.35MB)! We first suspected the Android NDK toolchain since we upgraded it and tested different toolchains, but we only observed minor changes.\nBy dichotomy search in our commits, we discovered that a single line of code was responsible for the inflation:\n\nAs incredible as it may sound, using iostream increases a lot the binary size. Our tests shown that it adds a least 300KB per… architecture! You must be very careful with iostream and prefer to use __android_log_print method:\n\nMake sure you also link against the logging library, in your Android.mk file:\n\nUse -fvisibility=hidden\nAn efficient way to reduce binary size is to use the visibility feature of gcc. This feature lets you control which functions will be exported in the symbols table. Hopefully, JNI comes with a JNIEXPORT macro that flags JNI functions as public. You just have to check that all functions used by JNI are prefixed by JNIEXPORT, like this one:\n\nThen you have just to add -fvisibility=hidden for C and C++ files in Android.mk file:\n\nIn our case the binaries were down to 809KB (-5%) but remember the gains may be very different for your project. Make your own measures!\nDiscard Unused Functions with gc-sections\nAnother interesting approach is to remove unused code in the binary. It can drastically reduce its size if for example part of your code is only used for tests.\nTo enable this feature, you just have to change the C and C++ compilation flags and the linker flags in Android.mk:\n\n\n\nOf course you can combine this feature with the visibility one:\n\n\nThis optim only got us a 1% gain, but once combined with the previous visibility one, we were down to 691KB (-18.7%).\nRemove Duplicated Code\n\nYou can remove duplicated code with the --icf=safe option of the linker. Be careful, this option will probably remove your code inlining, you must check that this flag does not impact performance.\nThis option is not yet available on the mips architecture so you need to add an architecture check in Android.mk:\n\nAnd if you want to combine this option with gc-sections:\n\nWe actually only obtained a 0.8% gain in size with this one. All previous optimizations combined, we were now at 687KB (-19.2%).\nChange the Default Flags of the Toolchain\nIf you want to go even further, you can change the default compilation flags of the toolchain. Flags are not identical accross architectures, for… example:\n\ninline-limit is set to 64 for arm and set to 300 for x86 and mips\nOptimization flag is set to -Os (optimize for size) for arm and set to -O2 (optimize for performance) for x86 and mips\n\nAs arm is used by the large majority of devices, we have applied arm settings for other architectures. Here is the patch we applied on the toolchain (version r8d):\n\nWe were good for a 8.5% gain with these new flags. Once combined with previous optimizations, we were now at 613KB (-27.9%).\nLimit the Number of Architectures\nOur final suggestion is to limit the number of architectures. Supporting armeabi-v7a is mandory for performance if you have a lot of floating point computation, but armeabi will provide a similar result if you do not need a FPU. As for mips processors... well they just are not in use on the market today.\nAnd if binary size is really important to you, you can just limit your support to armeabi and x86 architectures in Application.mk:\n\nObviously, this optim was the killer one. Dropping two out of four architectures halved the binaries size. Overall we obtained a size of 307KB, a 64% gain from the initial 850KB (not counting the bump at 1.35MB due to iostream).\nConclusion\nI hope this post will help you to reduce the size of your native libraries on Android since default flags are far from optimal. Don't expect to obtain the same size reductions, they will highly depend on your specific usage. And if you know other methods to reduce binary size, please share in the comments!\n&nbsp;"
  },
  {
    "id": "776-0",
    "title": "Two new Platforms in beta: Mac OS X and Windows Phone 8",
    "author": "Nicolas Dessaigne",
    "text": "After the launch of our Windows 8 beta (desktop and tablets), I'm pleased to start the year with even more new platform support! And this time it's directly two new platforms that we launch: Mac OSX and Windows Phone 8.\nDesktop Continued Support\nFollowing the mobile trend, \"app\" stores are becoming more and more popular on desktops. OS X started first with its App Store in January 2011 (already 2 years ago) while Windows joined the trend only recently.\nWith the release of this beta, we now support both environments! Get ready to build fantastic search in your apps!\nWindows Phone 8, the third major Mobile Player\nWhile the competition is harsh, Microsoft has committed all its energy to get a sizable marketshare. I honestly don't know if they will succeed but they are now definitely an actor to take into consideration. Major applications now need to have their Windows Phone 8 version alongside their iOS and Android ones.\nWith this new support, app developers can benefit from all Algolia Search features and provide a consistent experience across devices!\nCheck out the new guides and register for your OS X and Windows Phone 8 versions now!\n "
  },
  {
    "id": "511-0",
    "title": "Simplicity is the most Complex Feature!",
    "author": "Julien Lemoine",
    "text": "I've been convinced for a long time that simplicity is the most important property of a product. Long-gone are the 90s when a product was admired for its complexity. But I am also convinced that this is the most complex property to achieve and maintain as time passes by.\nA good example of an over-complex product is Atlassian JIRA, a bug tracker that also do scrum management and plenty of other things via dozens of plugins. It's basically a toolbox to create the bug tracker adapted to your company.\nIn my previous job, I faced an uncomfortable situation with JIRA because of its complexity. We used it for bug tracking and scrum management and I tried to upgrade our old version to the latest one. After some long hours to upgrade our setup on a test server, I finally got the latest version working but most of our installed plugins were not available anymore because the authors did not port their plugins to the new plugin API. Of course each plugin was there for a reason and I was in a tricky situation: keep the old version with security issues or upgrade to a new version without our plugins.\nBut it was far more vicious: There were about 10 versions between our old version and the latest one, and I didn't find any of these versions working with our set of plugins! In the end, we were forced to keep our old version.\nAtlassian forgot the most important lesson, even with a toolbox: simplicity! This is probably more expensive for them to keep plugin backward compatibility, but I would prefer for them to not have any plugin rather than breaking compatibility at each release. The final system is too complex to be maintainable and our final decision was to stop paying for JIRA support since we were blocked with an old release. It is even bad for their business.\nYou should be focused on simplicity for your users even it this results in more complexity for you (like maintaining backward compatibility)! As this post is strongly related to backward compatibility of API, I encourage… you to reread this famous post of Jeol Spolsky: How microsoft lost the API war.\n&nbsp;"
  },
  {
    "id": "489-0",
    "title": "Never, Ever, Hinder the Use of your Products!",
    "author": "Julien Lemoine",
    "text": "One of the worst user experiences I have ever had with software was with the Sony PS3. I kind of liked this product; I found the user interface very nice and well organized... but they were much too aggressive about upgrades! They simply blocked features until the upgrade was done!\nA few weeks ago I wanted to watch a VOD movie with my wife. I launched the Playstation Store that asked me to upgrade the OS to the latest version. That's 45 minutes before being able to access the Playstation again! But wait! Once the new OS was installed, I tried to launch  the Playstation Store again... This time, it was the Playstation Application that was not up to date !\nIn total it took me over 1 hour to do upgrades and guess what, at the end it was just too late to watch the movie!\nGenerally, frequent upgrades are good for your users, and I am sure there are plenty of bug fixes/improvments in the lastest version. But Sony has just made the wrong choice in blocking features until the upgrade is done. This is just plain frustrating for users! On the contrary, Android and iOS propose an upgrade that you can apply when you want. Best of all, they download in the background.\nIt may sound evident, but it is very important to ensure your users will always be able able to keep control over their products. You should never force them to do something they do not want to, like Sony did with the PS3."
  },
  {
    "id": "684-0",
    "title": "Algolia Search is now in Beta for Microsoft Windows 8 Desktops and Tablets - The Algolia Blog",
    "author": "Nicolas Dessaigne",
    "text": "You may have been expecting Windows Phone 8 as our next platform of choice, but we preferred to go Windows 8 first! With more than 40M sold in a month, let's say it's a slightly more interesting market for now.\nWindows 8: Our First Move to the Desktop\nYes, desktop apps are not as dead as many people think, especially now that both Mac OSX and Windows come with their app stores. Sure, Windows is now running on tablets - and we also support them - but it will take time before they surpass desktops.\nWhat's more, the new Windows 8 Metro Modern UI includes a charm bar with search and autocompletion as the number one feature. Microsoft actually provides an interface to ease application integration in the charm search bar... but no library to help you implement it. No problem, here comes Algolia Search for Windows 8!\nAwesome Features for Your Apps\nThe core technology is exactly the same as the one in our iOS and Android versions. That means you'll get exactly the same exceptional performance... well probably even better depending of the hardware you're running on. You'll also get instant visual feedback, typo-tolerance and multiple attributes support! See all the features on our website.\nEasy to Integrate in your Language of Choice\nCheck out the dedicated tutorials! The Algolia Search library has been designed to work directly on the platform. That means you can use it in whatever language you prefer, be it JS or one of the .Net choices (C#, VB, C++). The tutorials are available in both JS and in C#.\nMoreover it is fully compatible with Microsoft Search Contracts. It just became child's play to integrate Search in your Windows app!\nHey Microsoft, There are a Couple of Features You Could Improve\nWindows 8 is clearly a step in the right direction, but we encountered some problems we didn't expect!\nFirst is the multi-arch support. While it's just straightforward with iOS or Android (out of the JNI part...), using a native lib forces the developer to chose an architecture on… Windows! Fortunately, Visual Studio Package Generator handles that correctly and proposes you to select all architectures for your final export. But well... we'd have preferred it plain and simple.\nOur second deception is a bit more problematic as it prevents one of our very nice features: typo-tolerant highlighting during autocompletion. It starts with best of intentions from Microsoft. To remove the burden of implementing highlight for autocompletion on developers, Windows 8 handles it directly. The only problem is that it cannot be replaced by our own 🙁 So you can obtain hits even if your query contains typos, but hits containing text different from the query won't be highlighted.\nWe hope to be able to change this behaviour in the future. In the meantime, we're impatient to get your feedback on this beta! Register for your Windows 8 version of Algolia Search now!"
  },
  {
    "id": "634-0",
    "title": "Smart Contacts Demo Hits the App Store!",
    "author": "Nicolas Dessaigne",
    "text": "After going back and forth a few times with Apple, we are really happy to announce the availibility of our Smart Contacts application on the App Store! Even if we built it to demonstrate the amazing features of our search technology SDK, it can be used as an in-place replacement of the traditional iPhone Contacts app. Check it up!\n\nOutstanding Search Features\nIt plugs directly into your iPhone contacts and classically allows you to update them or create new ones. What's more interesting is the Search integration! It features:\n\nInstant search with autocompletion and visual feedback as you type\nSearch on all contacts attributes (name, company, notes, etc.)\nTypo tolerance to account for smartphone small keyboards\nAnd even search by initials!\n\niOS API Limitations\nThere are a few of features we would have loved to integrate that were not possible due to limitations in iOS APIs: group management, in place replacement of contacts in the phone app (that's maybe a lot to ask!), and ranking based on contact popularity.\nFree your Imagination!\nIf you ever wanted to see Algolia Search in action, now is the time. Check it up with your own data and imagine what Algolia can do for the application you develop!\nAnd don't forget your 5 stars ranking 😉"
  },
  {
    "id": "570-0",
    "title": "We are SeedCamp Finalists!",
    "author": "Nicolas Dessaigne",
    "text": "Hey, we are thrilled to announce our nomination among the Seedcamp Paris finalists! The event will take place next week on Monday, just before LeWeb 2012 (by the way we'll be there, feel free to ping us if you'd like to meet).\n\nFrom what we learned from all over the place, the day's mentorship alone is worth it. We're now fully committed to the event preparation! I'll try to write a followup post with our impressions.\n[Edit 15-Jan-2013] Here it is with some tips and advice [/Edit]"
  },
  {
    "id": "569-0",
    "title": "Why Autocomplete in Twitter Mobile App Sucks",
    "author": "Julien Lemoine",
    "text": "Autocomplete is so intuitive,that it seems like it would be easy to implement. However, most mobile apps that offer it provide a pretty poor user experience. Let's look at the Twitter mobile app as an example.\nTwitter proposes autocompletion when you create a new tweet. The idea is to make suggestions after the '#' and '@' characters. It's actually very nice to gain time, especially when you're tweeting with a small virtual keyboard... but it sucks!\nAvoid Roundtrips to Server for Autocompletion\nThe first reason is that when you're on the go, latency is often too high on mobile, leading to unusable autocomplete -  except if you're very slow to type. Twitter developers chose to develop this functionality server-side, probably with lucene, and to expose it via APIs to their mobile app. That's good for reusability but not so much for usability...\nBeware of the Suggestions Ranking\nThe second reason is the ranking is just obscure. Yesterday I sent a tweet to @cocoanetics and the screenshot on the left shows the suggestions I got when typing \"@c\". I would greatly prefer to see Twitter handles before names and it would never come to my mind to look for \"Marie Cecile\" with \"@c\"!\nExplain the Matches\nLast but not least there is no visual feedback to show me why the app proposes a given user. So ok let me think... the 'c' was reffering to \"Cécile\" in \"Marie-Cécile\"! A bit far fetched!\nNow let's imagine the Twitter mobile app with instant autocompletion even offline, intuitive ranking, and visual feedback... Appealing, isn't it? Twitter if you listen, check it up, I'm sure you'll love Algolia Search!"
  },
  {
    "id": "544-0",
    "title": "Algolia Search is Out!",
    "author": "Nicolas Dessaigne",
    "text": "After 4 betas and much priceless feedback, we are really excited to announce the release of Algolia Search! We launched it Monday during the 60th edition of Mobile Monday Paris! If you don't know what Algolia Search can do for your app, take a look at its amazing features or just watch the video!\nThis release also means that you can now integrate it directly into your apps and publish them! We wanted to have a simple and clear pricing plan: You can have all the benefits of Algolia Search in one Android or iOS App for only $590! Think about it, that's less than the day-rate of a mobile developer in many countries. Compare it to the many days you would need to integrate SQL Lite FTS for poor features.\nTry Algolia Search for free for 30-days!\nWe are also very excited to introduce our new website alongside this launch. You'll be able to access up-to-date documentation easily, to try and download Algolia Search in a few clicks and, of course, to order the product! Check it out and let us know what you think!\nThanks again to our beloved beta testers. Stay tuned for more news!"
  },
  {
    "id": "487-0",
    "title": "The Ordeal of Obtaining an Apple Developer Professional Account",
    "author": "Julien Lemoine",
    "text": "I recently had a pretty bad user experience when I upgraded my personal Apple developer account into a professional one.\nTo sum things up, we officially created Algolia in early October and I decided to convert my personal Apple Developer Account in order to have our applications published under the \"Algolia\" name. This process is not available online, but after a quick call, people at Apple sent me the link for the process. It is actually pretty simple: They just need you to fill some information about your company and to accept their EULA.\nWell, actually some of this information was pretty obscure. They needed our D-U-N-S® number... Some time ago, Apple was doing the job of checking that your company is a real one, but they decided to delegate this job to an external company (D&B). It looks like a good idea, doesn't it? After all this is not their core competency and D&B may be doing a really good job for this task...\nThe short answer is no, it wasn't a good idea at all! I started to check how to get this famous D-U-N-S® and after some time to understand the antiquated website of D&B, I finally found the contact address. One week later I finally received our D-U-N-S®. But that's not all, Apple then asked us to wait for the synchronization between D&B and Apple... which can take up to 14days!\nHey guys, I don't know if you realize how this is ridiculous to wait 14 days for a simple database synchronization! You have just no idea what 14 days means for a young startup 🙂\nAs you can imagine, I was already quite frustrated... but this was just the beginning. After the 14 days, Apple recognized the DUNS number... but a field was missing. They didn't have the legal type of our company. They then asked me to contact D&B and a new nightmare started at this level: D&B was saying that the entry was correct while Apple was asking me to contact D&B to correct the entry! There's nothing worst than to stand be between two big companies who pass the buck to each… other.\nHopefully Apple was far smarter than D&B and they finally accepted to bypass the missing field if I sent them directly our legal documents.\nIt is crucial to pay attention to all your users' problems and solve them as soon as possible. They may sometimes look like details to you, but that's what your customers will remember about your company. Of course, we try to apply this lesson to ourselves. Feel free to tell us if something's going wrong!"
  },
  {
    "id": "453-0",
    "title": "Painless integration, crystal clear documentation, please welcome Algolia Search beta 4!",
    "author": "Nicolas Dessaigne",
    "text": "Last month has been truly electrifying! We joined our friends at Yakaz in their office space, we participated in many events... and most exciting of all, we spent days and nights reworking the product! Today we are really proud to present you the result of this time well spent!\nI know I told some of you beta3 would be the last, but we could not ignore your excellent feedback. So here comes Algolia Search beta4, a true revolution (I hope this is not trademarked!) in mobile search!\nHere come the major improvements:\n\nA completely reworked API that we just love to use everywhere. The time to fully understand the library has been reduced to nearly nothing thanks to all your feedback. We are proud to have the easiest to use search library ever made!\nA completely rewritten documentation with detailed API and step-by-step tutorials. You should be able to make your first queries in a matter of minutes!\nEasy highlight for multiple fields queries. A bit cryptic? Stay with me... just imagine a Contact application where you can search for your contact by any field, i.e., name, company, address or even notes. Algolia Search now provides easy-to-use highlighting of any matching words. It is even able to generate a snippet when the text is too long. Check out the tutorials to learn more! Highlighting relevant results just became child's play!\nA greatly improved out-of-the-box relevance. Our mission is to simplify search: we want the best possible relevance by default so you can forget these long hours of tuning 🙂\n\nBut that's not all, many smaller improvements were also included in this release:\n\n\nSupport of advanced queries. Take again our tutorial contact application, wouldn't it be great to be able to search by initials? You can now implement this cool feature in a couple of minutes without any headache on relevance tuning.\nSupport for ARC and no ARC. In the previous beta we added an iOS version for people that do not use Automatic Reference Counting (ARC). We now have… only one version that supports projects with and without ARC. If you do not use ARC, all objects received from Algolia Search are autoreleased.\nSupport for user objects backward compatibility. As the index is also an objects store, you can modify your object members and still read older indexes! It is very easy to implement, just check the tutorials.\nThe release also fixes a few bugs that we discovered during your and our intensive testing.\n\n\nIt would not have been possible without the help of our many beta testers, thank you all! Special thanks to Kris, Hoa and Thomas whose guidance has been priceless.\nSo... what's next? Many things! This time I really believe this is the last beta. The final release is just around the corner. Of course, we appreciate your feedback nonetheless and always will! You can also expect a new website and a few apps in the app store! Who said a contact app?\nStay tuned!\n "
  },
  {
    "id": "415-0",
    "title": "A few thoughts after Apps World",
    "author": "Nicolas Dessaigne",
    "text": "Last week provided me with an occasion to feel the tempo of the mobile ecosystem at Apps World in London. Here are a few thoughts about what I saw.\nThere were many many mobile agencies exhibiting... and local UK agencies were dwarfed in number by offshore ones, hailing mainly from India. Eastern European countries also had an important presence, especially Poland. It seems that the golden days of apps development are behind us. Most big companies now outsource their apps offshore. It also means that it's becoming very important for local agencies to differentiate themselves. I actually pitched many of them about Algolia Search and differences in reactions were interesting. Most Indian based agencies didn't have a second look; They preferred to wait for \"requirements\" from their customers. On the other hands European ones were on average much more interested in what they could do with such a lib. The most geniune interest always came from technical guys when they were present. By the way, if I could give advice to any agency participating in such a event, please come with at least a developer, and at best your CTO. You would gain much credibility and differentiation!\nOut of all of the agencies, a few other disciplines were well represented... actually I may say \"too much\" represented as it often indicates an over-crowded field and a battle for survival!\n\nTesting and QA services. Offshore Indian firms are also very active in this space.\nAd platforms and payment tools. I didn't know there were so many options to choose from! Competition seemed harsh to get the attention of the few apps developers attending.\nAnd most of all cross-platform HTML5 frameworks! I'm not a big fan of PhoneGap and consorts, even if I admit it's a good choice for some apps, especially \"enterprise\" ones. The space is so crowded with offers now, that many may not survive the next year! By the way if you want to offer a framework and want to differentiate yourself with a cutting-edge search… functionality, you know how to contact me 😉\n\n\nTwo companies had a particularly important presence at the event compared to what we could have expected:\n\nRIM, with probably the nicest booth of all. They proposed BlackBerry porting classes, offered developer guidance and gave several workshop speeches. After their recent commitment to paying BlackBerry 10 developers a minimum of $10K, they continue to do all they can to attract developers. But I'm afraid I agree with Charlie Kindel that paying developers is a bad idea!\nTwilio, actively promoting their voice and SMS APIs. Their immense success reminds us that there are still billions of feature phones out there!\n\nThe other less surprising major players included Samsung and Microsoft, but no Google and no Apple (not unexpected!). Of course, they were present in many conversations! I had for example a very interesting chat with Adam Hościło about the many opportunities provided by the new iOS 6 Passbook. It's opening a golden area for many in the next few months!\n "
  },
  {
    "id": "387-0",
    "title": "Upcoming Mobile App Conferences: Apps World, Mobility for Business, Appdays",
    "author": "Nicolas Dessaigne",
    "text": "With the summer behind us, the period is pretty active. It's really difficult to attend all the great events, big and small, organized out there. After Mobile Monday, Failcon and the first meeting of Appsterdam Paris this week, here are the major mobile app conferences we're attending in the next few weeks!\n\n\nAppsworld, the 2nd and 3rd of October in London. It's a major event attended by lot of mobile industry professionals. There are expecting up to 5000 participants!\nMobility for business, the 11th and 12th of October in Paris. A big event too, they are expecting about 3000 attendees!\nAppdays, the 9th of November in Paris. A more human sized event (200 participants) and definitely a place to be!\n\nIf you happen to participate to one of these, get in touch for some passionate discussion around mobile apps development!\n "
  },
  {
    "id": "357-0",
    "title": "Algolia Search Beta 3 is out!",
    "author": "Julien Lemoine",
    "text": "We are pleased to announce Algolia Search Beta 3, our third release with a strong focus on performance.\nAs for the previous release, we would like to sincerely thank all of our beta testers for their excellent feedbacks!\nHere what's new in this beta3.\n\niOS & Android changes:\n\n\n\nUltra fast loading: indexes are now loaded in a few milliseconds (always less\nthan 10ms!)  With the Beta 2, an index of 500MB could take up to 20 seconds to load.\nUltra fast search on big indexes: Beta 2 was able to search up to 100k entries\nin real time. Beta3 can search in 5M entries in real time (and probably\nmore!). Our main use case was to search in all titles of the English version of\nWikipedia on a IPhone 3GS. The speedup is also very nice for small datasets,\nthe near-zero CPU usage increases battery life compared to Beta 2.\nHighlight is now always done on longest match. In previous version a query 'anq' could highlight \"Angeles\" in\ntwo different ways: \"Angeles\" or \"Angeles\", with this version you will always obtain \"Angeles\" which is easier to\nunderstand for end-users.\nImproved proximity scoring when a query contains multiple words.\nFixed two memory leaks that could lead to problems with very heavy usage.\n\niOS specific changes:\n\nAdded a version without ARC that allows to target iOS >= 3.0"
  },
  {
    "id": "327-0",
    "title": "Algolia Search Beta 2 is out!",
    "author": "Julien Lemoine",
    "text": "We are pleased to announce the launch of Algolia Search Beta 2, our second release!\nWe would like to sincerely thank all of our beta testers for the great feedback. You really helped us to make Algolia Search a first-class product guys! Please continue your feedbacks!\nAnd here what's new in this beta2.\n\niOS & Android changes:\n\n\nImproved performance for big data sets (up to three times faster). In our tests, we successfully used a 3 millions entries data set on a old iPhone 3GS. The index was 250MB large!\n\nDocumentation changes:\n\nReworked the overview\nFixed a lot of typos and small errors\n\niOS specific changes:\n\nAdded an AlgoliaSearch.h header that includes all public headers\nPrefixed all public classes by AS\nChanged addEntry selector in ASIndexWriter to be more similar to NSMutableDictionary API\nRemoved internal objects from public headers\nChanged ASAsyncIndexSearcher API to implement the delegate pattern"
  },
  {
    "id": "277-0",
    "title": "Results of the Evernote DevCup",
    "author": "Nicolas Dessaigne",
    "text": "Time flies! I just realized we didn't offer any feedback about our participation in the Evernote DevCup back in July.\nFirst of all, thank you so much for your support! We ranked 14th out of 174 contestants in public voting! That actually exceeded our expectations as we aimed for the 20th position (so drinks are on Julien!)\nBut even with all of your support, we didn't make it to the finals. We would have loved to fly to San Francisco, but the six finalists all feature rich apps that merit their position. Congrats guys, and good luck! The conference is in a couple of days now, and I can't wait to find out who will win the cup!\nAs for us, it was a truly great experience! Remember our post announcing our participation? Well, outside the *slight* frustration of not going to SF, the couple of days we spent building the Search for Evernote app was really worth it!\n\nWe were able to correct a few corner case bugs which this new use case highlighted. It's always better to find them yourself than let app developers stumble upon them 🙂\nWe developed a new feature enabling prefix search on all words.\nOur participation led to an improved awareness of the company and helped our SEO.\nWe created a new demo of the search lib which is much more compelling than Cities Suggest in many situations. It is actually a bit more than a demo for some people - as of today, it has 317 active users on google play!\n\nThat got us thinking... we may do this kind of contest again. But this time we'll aim higher!"
  },
  {
    "id": "294-0",
    "title": "Algolia Search beta is out!",
    "author": "Nicolas Dessaigne",
    "text": "After a few months in the making, our first mobile library is finally ready to hit the shelves! Well, nearly! Before releasing it, we want all the feedback you can give us to ensure it's bug-free, easy to use and responsive to your needs.\nIf you subscribed to the beta, you should already have received your access info. If not, you can request it directly on www.algolia.com.\nYou'll be able to download both the iOS and the Android versions, along with all necessary documentation. Feel free to ask for any clarification or additional info. We'll do a few technical posts which explore the product internals in the coming months.\nWe're now eager to read your feedback! You just have one address to remember to report bugs, request features, or anything beta related: beta@algolia.com\nEnjoy!\n "
  },
  {
    "id": "193-0",
    "title": "Free marketing @ WWDC 2012!",
    "author": "Nicolas Dessaigne",
    "text": "Let's rewind time a bit. Back in June was the famous Apple Developer Conference, aka WWDC 2012. Algolia didn't even have a website at that time, but what we did have is a friend who would attend: Thomas! That was an occasion we could not pass!\nThomas agreed to speak about us a bit and to invite people to visit our website. That simple action defined our deadline to have a website up and running. But wait! That would be better if people could also find the site when Googling our company name. That meant a few days to allow for indexing... and a tight deadline!\nAbout two weeks of brainstorming and implementation went into the web site. Our goal was to have a clear presentation of our mobile search lib... and to be found on Google! And you know what? It's damn difficult to do so in so short a time! Our site was up and running two days before the conf and we immediately submitted it to all search engines. Unfortunately we were not in first position when Googling for Algolia 🙁\nFortunately, things improved without delay. Two actions were particularly useful in helping our search rankings: our participation to the Evernote DevCup, and our blog opening! We quickly got the pole position for our brand and started to rank well for some key queries like mobile instant suggest 🙂\nWe also thought about creating some posters that explain our technology, but eventually we were so concentrated on the web site that we passed.\nResult: Some awareness, one subscription to beta and more importantly, a running website! Next marketing related action would be to have a demo ready for LeWeb in London, but that's another story...\nAnd here is what you could see on WWDC whiteboards:\n\nThank you again for your help Thomas!"
  },
  {
    "id": "212-0",
    "title": "C/C++ is still the only way to have high performance on Mobile",
    "author": "Julien Lemoine",
    "text": "When it comes to programming languages and performance, you can read all and its opposite on the web. It's definitely a very controversial topic!\n[Edit 15-Nov-2012] I had questions on reddit about the data-structures and algorithms we used. We develop an embedded search engine for mobile, and tests were done on our own data-structure that is far more efficient than SQLite or other platform options for this use-case. [/Edit]\nFor Algolia the story started when researching an instant suggest algorithm. I used Java for two reasons:\n\n\nMain reason: our first client was using Java on Google App Engine\nSecondary: at that stage, I was doing a lots of refactoring and Eclipse is very efficient for these tasks\n\n\nOnce our algorithm was designed, I started to optimize performance on a desktop computer (core I7 950). For this, I indexed all titles of the English version of Wikipedia (4 millions titles). I optimized the Java code mainly by reducing the number of allocations. All instant suggest queries were then faster than 10ms.\n\nAs we planned from the begining to support iOS and Android, I needed to optimize for high performance on mobile. I then ported the Java code to Android and ended up with a few modifications (we needed to support old Android versions which have not a full support of Java SDK).\nIn order to test performance, I created a small \"demo\" app: CitiesSuggest, a mobile application based on Geonames database to look for a city name. I filled the index with all places with a known population. In the end, the database contained 270 000 city names.\nAs could be expected, the resulting application was quite sluggish on my Galaxy S2. The worst queries could take more than one second of CPU.\nI then applied all possible methods described in the Anroid documentation and was able to reduce the response time to 700ms for the longest queries. This is better but still gives end-users an impression of sluggishness! Fortunately, common queries worked well enough to present our… very first demos at LeWeb'12 London. I was subsequently able to improve the user experience a lot by adding asynchronous support. At that point, we decided to start implementing the objective-C version for iOS.\nI started a new implementation fully done in objective-C from scratch without any premature optimization. I then developed the same CitiesSuggest application for iOS. I first got stuck with some basic UI stuff. For example I needed to reimplement an UILabel that supports highlighting! The standard UILabel does no support this, and other implementations like Three20 just have too many dependencies (you can download AlgoliaUILabel on github, I released  the code under Apache licence). Once the UI was ready, I could see the actual performance was catastrophic! Instant suggest queries were between 200ms and 10 seconds on an iPhone 4S!\nAfter profilling, I discovered that 95% of the time was spent in Objective-C messaging and ARC. Actually, I have millions of calls to very small methods with 1 or 2 lines of code, and I found a very good explanation in the internal of objc_msgSend (mainly on mikeask.com). There is in fact a hash table lookup behind objc_msgSend! This explains most of my performance problems.\nTo fix these problems, I started to replace all this low level objective-C implementation by a C++ implementation with inlined methods. Eventually, I finished with Objective-C code for high level classes while all the core was C++.\nThis change has dramatically improved performance with instant-search queries between 2ms and 90ms on a iPhone 4S. I struggled to find complex enough queries to reach 90ms! This resulted in a very nice user experience with a remarkably slick demo 🙂\nAfter this succes, I decided to use the same C++ code on Android with Android NDK. With this approach I reduced our maximum query time from 700ms to 80ms on a Galaxy S2. But I was really disappointed by the Android display, as I did not reach the same level of interactive… experience that I had with iOS. The display of results stays slower on Android, probably because I did not spend enough time to optimize this part.\nIn the end, I lost a lot of time with Java and Objective-C trying to optimize code when the real solution was to use C/C++. I am fully convinced that it is just not possible to reach the same speed with Objective-C only or Java only.\nAnd there is also another good property with C++ code: Blackberry supports C++, and there is large chance that Windows Phone 8 SDK will support C++ when released in a few weeks. Actually, I do not see any other alternative than C/C++ when you are looking for performance on mobile devices, which is our case 🙂\nI hope this article will prevent you from spending as much time on Java/Objective-C optimizations as I did."
  },
  {
    "id": "79-0",
    "title": "iOS: When Automatic Reference Counting is a Bad Idea",
    "author": "Julien Lemoine",
    "text": "I started developing for iOS in 2009 by learning about the Objective-C language. At that time ARC (Automatic Reference Counting) was not yet available and developers were responsible for alloc/release/autorelease. I found it pretty straightforward as it was very similar to C++ and the resulting code was very self-explanatory.\nWhen ARC was released at the end of 2011 it made a great impression on me and looked like the perfect feature for any programmer coming from the Java world who was not familiar with memory management. I started using ARC and discovered a major flaw that completely changed my mind. Here is an example :\n[Edit 28-Jan-2013] This post describes a bug in ARC that was fixed in Xcode 4.4.[/Edit]\nLet's start with an example in C++, the sample contains a constructor that allocates two vectors, a destructor that destroys them and a compute method that just performs a swap between the two vectors. This code is perfectly valid and a swap is the perfect operation when you have two sets of data to maintain because this is very efficient (just two pointers copy, no data copy).\n\nBefore ARC the objective-C code was very similar and perfectly valid:\n\nThe semantics are very clear, it is exactly the same as in C++.\nSo when you start using ARC you tend to do exactly the same thing with less code, and you just delete the dealloc method:\n\nThe error does not come from a missing strong attribute, because instance variables are strong by default. The problem is that ARC does not generate code in the variable assignation. You should explicitely add properties and use \"self.variableName\" notation like in the next example. I would encourage ARC designers to read this excellent article by Joel Spolsky \"Making Wrong Code Look Wrong\". This ARC usage is a perfect example of wrong code that looks perfect but leads me to the conclusion that ARC is not trustworthy.\nHere is the correct version:\n\nI am surprised Apple hasn't corrected that flaw yet. This is a major issue as ARC… does not generate code for variable affectation like it does for properties (if one of you reads this post and has a complete explanation, please tell me!):\n\nIs it because of a performance issue? I would prefer to have no ARC at all than to see such a behavior.\nIs this case too complex to be handled? The problem is it undermines ARC's utility and might get stuck at the prototype stage.\n\nTo me, this is a perfect example of a technology driven product. The engineers know their product so well that they forget to step back and look at it with a fresh eye to analyse the full complexity of their system.\nAnd that leaves me with two important lessons we'll try to apply to our products:\n\nAlways organize user tests, even when your users are developers themselves!\nAlways keep a fresh eye when trying to simplify a product (this one may prove particularly difficult!)\n\nI hope you'll tell us when we will (inevitably) break these rules!"
  },
  {
    "id": "127-0",
    "title": "Search for Evernote: We're in the Evernote DevCup!",
    "author": "Nicolas Dessaigne",
    "text": "You mint already be familiar with Evernote. It's a great company that delivers an impressive product that I use often, both professionally and personally. When we heard about them organizing their second developer competition, we immediately thought about the fantastic opportunity it could be for Algolia!\nLet's sum up:\n\nIt's an excellent use case for our first lib: search in Evernote mobile apps is quite awful and we could really bring a better user experience!\nIt's a good incentive to create a second demo (after Cities Suggest) that's more convincing, especially for Evernote's users.\nIt's an opportunity to get some media coverage 🙂\nAnd most of all it's an excellent occasion to pitch our lib to the Evernote team. We would love to have them as an happy customer!\n\nAs you see, even if we don't make it to the finals, the decision was a no-brainer! But, wait... we'd love to go to the finals! And you can help us! Part of the competition is to get the maximum public support. You want to hep us? Just go to our submission page and vote once a day! Tell your friends! Tell your grandma! You can even log in via facebook 😉\nWant to know more about our app \"Search for Evernote\"? Here comes the video. You can also download the app directly from http://www.algolia.com/evernote.html\nhttpvh://youtu.be/Te7gtKl_vIU"
  },
  {
    "id": "65-0",
    "title": "Why develop our own Unicode Library? - The Algolia Blog",
    "author": "Julien Lemoine",
    "text": "At one time or another, most developers come across bugs or problems with Unicode (about 3,720,000 results on google for the request unicode bug developer at the time of this writing). Let me tell you about my experience in the last decade and why we have now implemented our own unicode Library to produce exactly the same result across devices/languages.\nI first started to use Unicode in 2004 when I was developing a Text Mining software specialized on information extraction. This software was fully implemented in C++ and I used IBM ICU library to be Unicode compliant (all strings were stored in UTF16). I also used some normalization functions of ICU based on decomposition, but I did not notice any major problem at that time. I started to understand the dark side of Unicode later when I used it in other languages like Java, Python, and later in Objective-C. My first surprise was when I understood that a simple isAlpha(unicodechar c) method can return different results!\n\nI started to look in details at the standard and downloaded UnicodeData.txt (the file that contains most of the information about the standard, you can grab the latest version here).\nThis file contains descriptions of all Unicode characters. Third column represents “General Category” and is documented as:\n\n&nbsp;\n\n\n\n\nGeneral Categories\nThe values in this field are abbreviations for the following. Some of the values are normative, and some are informative. For more information, see the Unicode Standard.\nNormative Categories\n\n\nLu: Letter, Uppercase\nLl: Letter, Lowercase\nLt: Letter, Titlecase\nMn: Mark, Non-Spacing\nMc: Mark, Spacing Combining\nMe: Mark, Enclosing\nNd: Number, Decimal Digit\nNl: Number, Letter\nNo: Number, Other\nZs: Separator, Space\nZl: Separator, Line\nZp: Separator, Paragraph\nCc: Other, Control\nCf: Other, Format\nCs: Other, Surrogate\nCo: Other, Private Use\nCn: Other, Not Assigned (no characters in the file have this property)\n\n\nInformative Categories\n\n\nLm: Letter, Modifier\nLo: Letter,… Other\nPc: Punctuation, Connector\nPd: Punctuation, Dash\nPs: Punctuation, Open\nPe: Punctuation, Close\nPi: Punctuation, Initial quote (may behave like Ps or Pe depending on usage)\nPf: Punctuation, Final quote (may behave like Ps or Pe depending on usage)\nPo: Punctuation, Other\nSm: Symbol, Math\nSc: Symbol, Currency\nSk: Symbol, Modifier\nSo: Symbol, Other\n\n\n\n\n\n\nAs you can see there is quite a lot of categories, some of them are very easy to understand like \"Lu\" (Letter, uppercase) and \"Ll\" (Letter, lowercase) but some of them are more complex like \"Lo\" (Letter, other)  and \"No\" (Number, other), and this is exactly where the first problem begins.\nLet’s take the unicode character U+00BD(½) as an example. It is quite common to describe spare parts and is defined as \"No\"... except that some unicode libraries consider that this is not a number and return false to isNumber(unicodeChar) method (e.g., Objective-C).\nIn fact the two most used methods, isAlpha(unicodeChar) and isNumber(unicodeChar), are not directly defined by the Unicode standard and are subject to interpretation.\nThe consequence is that results are not the same across devices/languages! In our case this is a problem because our compiled index is portable, and we want to have exactly the same results on different devices/languages.\nHowever, this is not the only problem! Unicode normalization is also a tricky topic. The Unicode standard defines a way to decompose characters (Characters decomposition mapping), for example U+00E0(à) which is decomposed as U+0061(a) + U+0300( ̀). But most of the time you do not want a decomposition but a normalization: get the most basic form of a string (lowercase without accents, marks, …). This is key to be able to search and compare words. For example, the normalization of the French word “Hétérogénéité” will be normalized as “heterogeneite”.\nTo compute this normalized form, most people compute the lowercase form of a word (well defined by the Unicode… standard), then compute the decomposed form and finally remove all the diacritics. However, this is not enough. Normalization can not always be reduced to just a matter of removing marks. For example the standard German letter ß is widely used and replaced/understood as \"ss\" (you can enter ß in your favorite web search engine and you will discover that it also search for \"ss\"). The problem is that there is no decomposition for \"ß\" in the Unicode standard because this letter is not a letter with marks.\nTo solve that problem, we need to look in the Character Fallback Substitution table that is not part of most of Unicode library implementations. This substitution table defines that “ß” can be replaced by “ss,\". There are plenty of other examples; For instance, 0153(œ) and 00E6(æ), letters of the French language, can be replaced by “oe” and “ae”.\nAt the end, this led us to implement our own Unicode library to ensure that our isAlpha(unicodechar) and isNumber(unicodechar) methods have a unique behavior on all devices/languages and to implement a normalize(unicodestring) method that contains character fallback substitution table. By the way our implementation of normalization is far more efficient because we implemented it in one step instead of three (lowercase + decomposition + diacritics removal).\nI hope you found this post useful and gained a better understanding of the Unicode standard and the limits of standard Unicode libraries. Feel free to contribute comments or ask for precisions."
  },
  {
    "id": "42-0",
    "title": "Great discussions at LeWeb'12 London",
    "author": "Nicolas Dessaigne",
    "text": "On June 19 & 20th, I had the chance to participate to LeWeb 2012 London edition. This year theme was \"Faster than Real Time\" and we had an impressive list of speakers! But the true value of LeWeb is elsewhere: It's in the 1283 people from 52 countries who were present and with whom you could network!\nThey chose Presdo Match to help people meet and honestly... this tool would benefit from some improvements, especially a mobile version! Still, I was able to find no fewer than 100 participants having the \"mobile\" keyword in their profile and, from there, organize a handful of meetings. Thanks to all of you who accepted to meet me or whom I met unplanned, with a special thanks to Paul Ardeleanu, Gora Sudindranath, Lindsey C. Holmes, Marius Rostad, Kevin McDonagh and Alexandre Delivet for their precious feedbacks about Algolia.\nCities Suggest demo @ LeWeb\nI had the opportunity to do the very first demonstrations of our instant suggest lib, and that was both exhilarating and frustrating! We chose to develop a small proof of concept suggesting city names from anywhere in the world. Here's what I learned:\n\nA demo is better than many words! Even if most people knew what I meant by \"google instant suggest\", the demo was key in clarifying our offering.\nEven if we chose cities because it was easy to demonstrate (thanks to the geonames database), it can be interesting in itself!\n100ms seemed a pretty fast response time in our initial testing, but it's actually way too slow to have a smooth user experience.\n\nOver all that was a very good experience, and I came back with a few improvements to implement (most coming from my own frustration showing the demo while the feedback was actually very positive!)\nThe most important piece of feedback was about the perceived sluggishness of the app. We decided to implement an asynchronous version of the lib. Beware, it actually comes with a drawback for our users; It's significantly more difficult to integrate. But it did not take… long for us to decide it was the way to go, since the perception of speed is so natural that the benefit far outweighs the longer integration code. We'll now work on simplifying it!\nWe'll soon do a post about this demo. In the meantime, stay tuned!\n \n "
  },
  {
    "id": "16-0",
    "title": "Let's start the adventure",
    "author": "Nicolas Dessaigne",
    "text": "Welcome to The Algolia Blog! It's always difficult to write the first post of a blog! What should I talk about? The company, the founders, the business, the culture? And all that knowing that virtually nobody will read except diggers in a few years (hopefully)!\nLet's concentrate instead on what we'll be blogging about. Company news obviously, but not only. I expect we'll write quite a few posts about technology, algorithms, entrepreneurship, marketing, and whatever else we'll want to share with you 🙂\nAnd most important, feel free to participate in comments or by contacting us directly. We appreciate your feedback!\nWelcome to the Algolia blog!"
  }
]
